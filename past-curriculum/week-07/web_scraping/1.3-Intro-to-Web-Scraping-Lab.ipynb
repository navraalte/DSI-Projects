{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px;\">\n",
    "\n",
    "# 1.3 - Intro to Web Scraping Lab\n",
    "\n",
    "---\n",
    "\n",
    "Week 7- 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*In this lab you will learn how to*\n",
    "- How to read and understand the code in a Scrapy Spider class\n",
    "- Write a spider to scrape job post from Indeed.com\n",
    "\n",
    "The following class IndeedSpider is incomplete. It is missing code for two functions: **fill_url_list** and **build_indeed_url**. These functions build and populate a list with every unqiue combination of job search URLs based on the job query key terms provided below. \n",
    "\n",
    "### Direction\n",
    "1. Build those functions and scrape Indeed.com \n",
    "2. Annotate every line of code with a short description on how what that line of code is doing\n",
    "\n",
    "## NOTE: \n",
    "\n",
    "Once you're finished proto-type these two functions in this notebook, copy the code over to the scrapy files but **DO NOT SCRAPE ALL 9 Choose 3 PAGES**. If everyone does this, GA will be blocked from visiting Indeed.com by the site's servers. Instead, only run 2 or 3 URLS to test and verify that your code works. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scrapy\n",
    "from webscraper.items import  WebscraperItem\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "\n",
    "class IndeedSpider(scrapy.Spider):\n",
    "    name = \"indeed_spider\"\n",
    "    allowed_domains = [\"indeed.com\"]\n",
    "\n",
    "\n",
    "    # job query key terms \n",
    "    job_sites = [\"https://www.indeed.com/jobs\"]\n",
    "    job_titles = [\"Data Scientist\", 'Machine Learning Engineer', \"Data Analyst\"]\n",
    "    citites_stats = [(\"New York\", 'NY'), (\"Los Angeles\", \"CA\"), (\"San Diego\", 'CA')]\n",
    "\n",
    "    # increment page in order to scrap additonal posting in same city\n",
    "    page_increment_list = ['0','1','2']\n",
    "\n",
    "    # list populated with unique combinations of job query key terms\n",
    "    #these combinations form unique urls to scrap \n",
    "    start_urls = self.build_indeed_url(job_sites, job_titles, cities, states)\n",
    "    \n",
    "\n",
    "#     start_urls = (\n",
    "#         'https://www.indeed.com/jobs?q=Data+Scientist&l=San+Bernardino%2C+CA&start=2',\n",
    "#     )\n",
    "\n",
    "    def fill_url_list(self, job_sites, job_titles, cities, states):\n",
    "        '''Return a list with every combination of urls for job searchs. \n",
    "        Each combination mush as job_site, job_titles, and cities_states in this format:\n",
    "\n",
    "        https://www.indeed.com/jobs?q=Data+Scientist&l=San+Bernardino%2C+CA'''\n",
    "\n",
    "        # you will need to call build_url inside this function\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def build_indeed_url(self, site, job_title, city, state_initial, incremental_page = None):\n",
    "        '''Build site url template with specifc arguments.\n",
    "           Return a correct url address that can be used to request an Indeed page. \n",
    "           The url should be able to work on the chrome browser'''\n",
    "\n",
    "        pass \n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "        titles = response.xpath('//h2/a/@title').extract()\n",
    "        links = response.xpath('//h2/a/@href').extract()\n",
    "        companies = response.xpath('//span[@class=\"company\"]/span/text()').extract()\n",
    "\n",
    "        for title, link, comp in zip(titles, links, companies):\n",
    "            item = WebscraperItem()\n",
    "            item['title'] = title\n",
    "            abs_url = response.urljoin(link)\n",
    "            item['url'] = abs_url\n",
    "            item['company'] = comp\n",
    "\n",
    "            request = scrapy.Request(abs_url, callback=self.parse_job)\n",
    "            request.meta['item'] = item\n",
    "            yield request\n",
    "\n",
    "\n",
    "    def parse_job(self, response):\n",
    "        item = response.meta['item']\n",
    "        keys = ['sql', 'python', 'ml', 'spark', 'hadoop', 'dl',\n",
    "                'stats', 'ts', 'de', 'dp', \n",
    "                'nlp', 'bd', 'pca', 'pred_modeling', \n",
    "                'anomaly_detection', 'data_analysis', \n",
    "                'exp_design', 'cnn', 'bayes_opt', 'pipeline']\n",
    "\n",
    "        skills = ['sql', 'python', 'machine learning',\n",
    "                  'spark', 'hadoop', 'deep learning', \n",
    "                  'statistics', 'time series', 'data engineering', \n",
    "                  'data products', 'nlp', 'big data', \n",
    "                  'pca', 'predictive modeling', 'anomaly detection', \n",
    "                  'data analysis', 'experimental design', \n",
    "                  'deep convolutional networks','bayesian optimization',\n",
    "                  'pipeline']\n",
    "\n",
    "        for key, skill in zip(keys, skills):\n",
    "            item[key] = int(skill in response.text.lower())\n",
    "\n",
    "        yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and build your functions below\n",
    "\n",
    "Since there are 9 choose 3 possible combinations for URL based on the job query key terms provided, you know that you're functions works when: \n",
    "\n",
    "1. The URLs that your functions output work on your browser's search bar. \n",
    "2. There are 84 URLs outputed by the fill_url_list function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# job query key terms \n",
    "job_sites = [\"https://www.indeed.com/jobs\"]\n",
    "job_titles = [\"Data Scientist\", 'Machine Learning Engineer', \"Data Analyst\"]\n",
    "citites_stats = [(\"New York\", 'NY'), (\"Los Angeles\", \"CA\"), (\"San Diego\", 'CA')]\n",
    "\n",
    "# increment page in order to scrap additonal posting in same city\n",
    "page_increment_list = ['0','1','2']\n",
    "\n",
    "# list populated with unique combinations of job query key terms\n",
    "#these combinations form unique urls to scrap \n",
    "#start_urls = self.build_indeed_url(job_sites, job_titles, cities, states)\n",
    "\n",
    "\n",
    "start_urls = ('https://www.indeed.com/jobs?q=Data+Scientist&l=San+Bernardino%2C+CA&start=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_indeed_url(self, site, job_title, city, state_initial, incremental_page = None):\n",
    "    '''Build site url template with specifc arguments.\n",
    "       Return a correct url address that can be used to request an Indeed page. \n",
    "       The url should be able to work on the chrome browser'''\n",
    "\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_url_list(self, job_sites, job_titles, cities, states):\n",
    "    '''Return a list with every combination of urls for job searchs. \n",
    "    Each combination mush as job_site, job_titles, and cities_states in the following format:\n",
    "    \n",
    "    EXAMPLE: https://www.indeed.com/jobs?q=Data+Scientist&l=San+Bernardino%2C+CA&start=2''\n",
    "    \n",
    "    # you will need to call build_url inside this function '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_urls = self.build_indeed_url(job_sites, job_titles, cities, states)\n",
    "start_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
