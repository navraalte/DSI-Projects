{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px;\"> \n",
    "# Long, Wide, Pivoting, and Melting Tables in Pandas\n",
    "\n",
    "---\n",
    "Week 2 | Lesson 6.2\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe a wide and long table\n",
    "- Describe and use the pivot_table method\n",
    "- Describe and data imputing\n",
    "- Describe and using merging\n",
    "\n",
    "\n",
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Understand how to load data into a dataframe\n",
    "- Understand how numpy arrays work\n",
    "\n",
    "\n",
    "![](http://dataconomy.com/wp-content/uploads/2015/03/Python-Pandas-Features-Tutorial-Data-Mining-e1427131108858.jpg)\n",
    "\n",
    "\n",
    "# Long format, wide format, pivot tables, and melting\n",
    "\n",
    "This lesson is all about data transformation in pandas. Data transformation is in essense reorganizing the rows and columns of your dataset to be a different shape and format. \n",
    "\n",
    "The benefits to transforming your data are primarily for easier access and manipulation of data, whether it be through easier masking/conditional statements or because you would prefer to operate across columns or down rows. \n",
    "\n",
    "Over time you will get a feel for which data formats are better for different tasks. This lesson, however, is focused in large part on the _functional application_ of data transformation (i.e. how do you do **this** to a dataset?\n",
    "\n",
    "\n",
    "### Need Help with Pandas?\n",
    "\n",
    "The [Pandas Documention](http://pandas.pydata.org/pandas-docs/stable/api.html) tells you what methods do and what argumments they accept, as well as provide examples. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up with Series\n",
    "\n",
    "A **Series** is a single vector of data (like a NumPy array) with an index that labels each element in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series = pd.Series([100,200,300,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# like a numpy array but with added capabilities \n",
    "series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the series to its Numpy-array representation\n",
    "arr = series.as_matrix()\n",
    "type(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the series to a list\n",
    "arr2 = series.tolist()\n",
    "type(arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. \"Wide\" format data\n",
    "\n",
    "**Wide** format data is the more common format of data for .csv type files. You are already familiar with wide format data: I believe all of the datasets we have been using thus far have been in wide format.\n",
    "\n",
    "Wide format data is formatted with criteria:\n",
    "\n",
    "- There are multiple ID _and_ value columns. In other words, there is a column for every \"variable\" with its own unique values.\n",
    "- The format has both the conceptual simplicity of a single column of values per variable and a more compact matrix.\n",
    "- Is not useful for SQL-style operations: it can make it much harder or even impossible to join tables together on a value.\n",
    "- Can be more useful in pandas when you need to preform operations on variables **across columns**. For example, multiplying columns together.\n",
    "- It is the most commonly the format that you will put the data in when you are ready to perform modeling (with some exceptions). When we get into modeling next week I will explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load  \"Nerdy Personality Attributes\" dataset\n",
    "\n",
    "This is a parsed and modified version of the full \"Nerdy Personality Attributes\" survey that asked subjects to self-rate on questions related to \"nerdiness\" as well as more general personality traits such as openness and extraversion. Demographic information on the subjects was also collected.\n",
    "\n",
    "In this modified version, for the sake of example, some of the subjects have only data for the survey and not the demographic variables. Because there are missing values and the data in general is \"messy\", this is also in part a data cleaning problem.\n",
    "\n",
    "We will load the data in wide format first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data into dataframe\n",
    "nerdy_wide_f = '~/DSI-SF-5/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_wide_missing.csv'\n",
    "nerdy_wide = pd.read_csv(nerdy_wide_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the shape method to find out the dimentions \n",
    "nerdy_wide.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in the familiar (rows, columns) format where each column is a variable, each row contains the observation for that variable for (in this case) that distinct subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see how many null values there are per column with the convenient chained function pattern below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore api for isnull method in class\n",
    "nerdy_wide.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values and Imputing Data\n",
    "\n",
    "\n",
    "If we were to just drop all the rows that have any null values at this point, we would lose 970 rows due to the commonly missing variable `major`.\n",
    "\n",
    "### Imputing \n",
    "\n",
    "**Imputation** is the process of replacing missing data with substituted values.\n",
    "\n",
    "Sometimes it is not feasible to simply delete rows with missing data. For instance, if we were to delelet all 970 rows with missing data, we would be throwing away more than half of our data set! So instead we try to impute data whenever possible. \n",
    "\n",
    "\n",
    "#### Imputing Techniques \n",
    "\n",
    "Imputing techniques range from simple to more sophisticated. \n",
    "\n",
    "- Replacing missing numerical values with the mean or median of the column \n",
    "- Replaceing a missing categorical value with \"unknown\"\n",
    "- Using statistical infer what the mising values should be\n",
    "- Using machine learning models to predict what the values should be \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# break down this code in class - explore the pandas api for .loc and .isnull method \n",
    "nerdy_wide.loc[nerdy_wide.major.isnull(), 'major'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide.major.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \"Long\" format\n",
    "\n",
    "Now we can load the same data in but in what's commonly referred to as \"long format\". \n",
    "\n",
    "Long data is formatted with criteria:\n",
    "\n",
    "- Potentially multiple \"id\" (identification) columns.\n",
    "- Variable:value column pairs that match a variable key to a value (in the simple case, a single variable column and a single value column).\n",
    "- The \"variable\" column corresponds to the multiple variable columns in your wide format data. Now, instead of a column for each variable, you have a row for each variable:value pair, per id. \n",
    "- This is a standard format in SQL databases because it is appropriate for joining different tables together by keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load long data\n",
    "nerdy_long_f = '~/DSI-SF-5/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_long_missing.csv'\n",
    "nerdy_long = pd.read_csv(nerdy_long_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use shape to print out data size\n",
    "nerdy_long.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the long data has way more rows, but only three columns.\n",
    "\n",
    "Below you see the three columns: `subject_id`, `variable`, and `value`.\n",
    "\n",
    "**`subject_id:`**\n",
    "- This is the primary \"key\" or \"id\" column. Each subject id will have corresponding entries in the variable column, one for each row.\n",
    "\n",
    "**`variable:`**\n",
    "- This column indicates which variable the item in the value column corresponds to.\n",
    "\n",
    "**`value:`**\n",
    "\n",
    "- This contains all the values for all of the variables for all ids. Essentially, every cell in the wide dataset except the subject_id is listed in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the unique values in the variable column correspond to the column headers in the wide format data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_long.variable.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(nerdy_long.subject_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again replace the `major` variables with 'unknown', but in a way that works with long format data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nerdy_long.loc[nerdy_long.variable == 'major', 'value'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_long.loc[nerdy_long.variable == 'major', :].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas `pivot_table()`: long to wide format\n",
    "\n",
    "The `pd.pivot_table()` function is a very powerful tool to both transform data from long to wide format and also to conveniently summarize data into a matrix with arbitrary functions.\n",
    "\n",
    "First we'll look at how we transform this long format data back into the wide format data.\n",
    "\n",
    "**Parameters to note in the function:**\n",
    "\n",
    "    nerdy_long: the pivot_table() function takes a dataframe to pivot as its first argument\n",
    "    \n",
    "- **`columns`**: this is the list of columns in the wide format data to transform back to columns in wide format, with each unique value in the long format column becoming a header for the wide format   \n",
    "- **`values`**: a single column indicating the values to use when pivoting and filling in the new wide format columns\n",
    "- **`index`**: columns in the long format data that are index variables – this means that these will be left as single columns, not spread out across columns by unique value such as in the columns parameter \n",
    "- **`aggfunc`**: often pivot_table() is used to perform a summary of the data. aggfunc stands for \"aggregation function\". It is required and defaults to np.mean. You can put your own function in, which I do below.\n",
    "- **`fill_value`**: if a cell is missing for the wide format data, the value to fill in\n",
    "    \n",
    "I am putting in my own function, `select_item_or_nan()` to the `aggfunc` keyword argument. Because my `subject_id` column has a single variable value for each id, I just want the single element in the long format value cell. My data is messy and so I have to write a function to check for some places it can break. \n",
    "\n",
    "Note: `x` passed into my function is a series object (weirdly). I pull out the first element of that with the `.iloc` indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total number of values in the subject_id column\n",
    "nerdy_long.subject_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total number of unique values in the subject_id column\n",
    "len(nerdy_long.subject_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_long.subject_id.count()/float(len(nerdy_long.subject_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_item_or_nan(x):\n",
    "    x = x.iloc[0]\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide = pd.pivot_table(nerdy_long, \n",
    "                            columns=['variable'], \n",
    "                            values='value',\n",
    "                            index=['subject_id'], \n",
    "                            aggfunc=select_item_or_nan,\n",
    "                            fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiindex/Hierarchical indexing pt. 1\n",
    "\n",
    "Below in the header you can see that the format of the wide data is not the same as our original loaded wide format. Pandas implements something called **Multiindexing** or **Hierarchical indexing** which allows for \"tiered\" row and column labels.\n",
    "\n",
    "Right now it is not that bad, but this can get very complicated and annoying which we will see further down in the lesson.\n",
    "\n",
    "The main difference here is that we have a `variable` name in the top left corner, which is \"labeling\" our columns (and corresponds to the name of our original column in the long format data). The row indexer has become our single key/id variable `subject_id`. The columns are what we would expect here, each one a variable like in the original wide data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the null values from our recreated wide data.\n",
    "\n",
    "Remember our `subject_id` is now the **index**, and so we can access it with the `.index` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop all rows with na in them\n",
    "nerdy_wide.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nerdy_wide.shape\n",
    "print len(nerdy_wide.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dataframe function `.reset_index()` to move `subject_id` into a column and create a new index. Now we have the dataframe in the format we got when we loaded the original wide data in before. The only exception is that we still have that \"variable\" column label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide_flat = nerdy_wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide_flat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide_flat.columns.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove the column label (which I personally find confusing) by setting the `.columns.name` attribute to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide_flat.columns.name = None\n",
    "nerdy_wide_flat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pivot_table` for summarization\n",
    "\n",
    "For those of you who are experienced with Excel, the pandas pivot table does the same thing as the pivot table in Excel. It's more powerful, but obviously harder to use than the user-friendly spreadsheet version.\n",
    "\n",
    "Next we'll use pivot table to generate some summary statistics for `anxious`, `bookish`, and `calm` by `major`. \n",
    "\n",
    "We can do it two ways. First let's subset the data just to those columns and subject id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_subset = nerdy_wide_flat[['subject_id','major','anxious','bookish','calm']]\n",
    "nerdy_subset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going from wide to long with `.melt()`\n",
    "\n",
    "**`.melt()`** is a function that essentially performs the inverse operation of `pivot_table` on dataframes.\n",
    "\n",
    "Melt takes a dataframe as its first argument. Additional arguments typically used in the melt function are:\n",
    "\n",
    "- **`id_vars`**: the column or columns that will be id variables. id variables contain datapoints specified by the variable and value columns\n",
    "- **`value_vars`**: a list that specifies which columns should be converted into a single value column and variable column.\n",
    "- **`var_name`**: the header name of the variable column (default='variable')\n",
    "- **`value_name`**: the header name of the value column (default='value')\n",
    "\n",
    "Below I only specify the `id_vars` as subject_id and major. The variable and value columns are inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_sub_long = pd.melt(nerdy_subset, id_vars=['subject_id','major'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nerdy_subset.shape, nerdy_sub_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_sub_long.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same thing as above without having to subset the dataframe first by simply specifying the value_vars to lengthen. The output dataframe will then not have information on the columns left out of the `id_vars` and `value_vars` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nerdy_sub_long = pd.melt(nerdy_wide_flat, \n",
    "                         id_vars=['subject_id','major'], \n",
    "                         value_vars=['anxious','bookish','calm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nerdy_wide_flat.shape, nerdy_sub_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_sub_long.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value column is still a string, so we can convert it to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_sub_long.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nerdy_sub_long.value = nerdy_sub_long.value.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing with aggregate functions\n",
    "\n",
    "Pivot table can take in the long format variable, value, and an index to group by and apply aggregate functions as well for summarizing data easily. Note that your index variable should not be pulling out unique rows (for example, subject_id by variable would only have one value to send into the aggregate functions).\n",
    "\n",
    "The output dataframe gives you a \"hierarchical\" column index – the three variable for each aggregate function. The row index is the majors you divided the data up by.\n",
    "\n",
    "If you apply more index variables to split by, the row indices will also become hierarchical! It can get complicated fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_major_summary = pd.pivot_table(nerdy_sub_long, \n",
    "                                     columns=['variable'], \n",
    "                                     values='value',\n",
    "                                     index=['major'], \n",
    "                                     aggfunc=[np.mean, np.median, len],\n",
    "                                     fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_major_summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.names` attribute on the index and the columns will show you the hierarchy of labels. The row index is \"major\", and the two column indices are None and 'variable' (the aggregate functions get no label from pivot table in this case). \n",
    "\n",
    "If you print out the columns, you can see it has become a pandas `MultiIndex` object that has levels, labels, and names. I won't go into too much detail on this – reading the pandas documentation on MultiIndexes has a lot more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nerdy_major_summary.index.names\n",
    "print nerdy_major_summary.columns.names\n",
    "print nerdy_major_summary.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing along the hierarchical column headers can be done with chained bracket keys, with the top level column label in the first bracket down to the bottom level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_major_summary['mean'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_major_summary['mean']['anxious'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_major_summary['mean'][['anxious','bookish']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you can just split them up by comma within the brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_major_summary['mean', 'bookish'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preface to merging/joining: long and wide data\n",
    "\n",
    "Joining tables is a concept that has its roots in SQL, so we won't dive too deeply into it here. But it is good \n",
    "\n",
    "Load in the data we've been using above, but now split up with just the demographic variables in one dataset and the survey question answers in another. These datasets are in wide format, and they both contain `subject_id` to identify who the questions are for. \n",
    "\n",
    "As you may recall, the demographic responses have fewer observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_demos_file = '~/DSI-SF-5/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_demo_sample.csv'\n",
    "n_survey_file = '~/DSI-SF-5/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_survey.csv'\n",
    "\n",
    "demos_subset = pd.read_csv(n_demos_file)\n",
    "survey = pd.read_csv(n_survey_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print demos_subset.shape, survey.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demos_subset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survey.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print demos_subset.columns\n",
    "print survey.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas `.merge()` function\n",
    "\n",
    "The merge function is a built-in function in a DataFrame. The first argument is another DataFrame that you want to merge it with, and the `on` keyword argument is the key or keys that you want the DataFrames to be \"matched\" on.\n",
    "\n",
    "We are specifying `how='inner'` here, which essentially means that the subject_id has to be present in both dataframes to merge them together and return them. Because the demographics dataset has fewer subject_ids, it will only merge the subject_id rows from the survey dataset that are present in the demographics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demos_survey = demos_subset.merge(survey, on=['subject_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print demos_survey.shape\n",
    "demos_survey.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lesson we learned: \n",
    "\n",
    "- Wide tables have all unique categories as features \n",
    "- Long tables have multi-categorical values within features\n",
    "- How to use the pivot_table method\n",
    "- About Data imputing\n",
    "- How to merge tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "Checkout these resources for some extra help. \n",
    "\n",
    "[Pandas API](http://pandas.pydata.org/pandas-docs/stable/api.html) Official documentation for the Pandas package. An online \"textbook\" that explains how every method works, what parameters that it accepts, and provide examples. \n",
    "\n",
    "[Jupyter Notebook Tutorial](http://nbviewer.jupyter.org/github/fonnesbeck/Bios8366/blob/master/notebooks/Section2_1-Introduction-to-Pandas.ipynb) A tutorial for beginners. \n",
    "\n",
    "[Data Wrangling with Pandas](http://nbviewer.jupyter.org/github/fonnesbeck/Bios8366/blob/master/notebooks/Section2_2-Data-Wrangling-with-Pandas.ipynb) A jupyter notebook tutorial on how to clean and structure data using Pandas.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
