{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 \n",
    "\n",
    "Gaussian Processes is a well-known machine learning algorithm (<a href=https://en.wikipedia.org/wiki/Gaussian_process> Wiki Page </a>). They are heavily related to linear algebra. For this exercise, you won't need to know about Gaussian Processes but you will apply concepts from linear algebra to solve these exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "Covariance kernels encode similarity between the data points. <a href=http://www.gaussianprocess.org/gpml/chapters/RW4.pdf> Read More about it here. </a> One such  type of kernel is defined as: $K(x, z) = min(x, z) - xz$. \n",
    "\n",
    "Generate N = 30, random numbers $x_1, ... , x_n$ in the domain (0, 1). Create a matrix $A$ whose $i, j$ entry is $A_{i, j} = K(x_i, x_j)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "After creating this covariance matrix , create the vector whose the $i$th entry is $y_i = x_i(1 - x_i)$. Then compute, $c=A^{-1}y$. This is a common computation for doing Gaussian Processs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "\n",
    "Another example of a similarity function across data points is $B_{ij} = e^{-(x_i - x_j)^2}$. Redo Exercise 1.1, 1.2 using this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "\n",
    "Kernels can often produce ill-conditioned covariance matrices. <a href=https://en.wikipedia.org/wiki/Condition_number>Read about Condition Number here</a>. Compute the singular value decomposition of the matrix. $B = UΣV$ and determine what its numerical rank is, and thus whether it suffers from ill-conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "\n",
    "One way to combat ill-conditioning is to perform Regularization. Regularly will come regularly in machine learning. Read more about it <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)> here </a>.In practice, this consists of adding small noise to the diagonal of the matrix. Compute the perturbed matrix: $B + μI$ , where $I$ is the\n",
    "\n",
    "appropriately sized identity matrix and $μ = 10^{-10}$ . Then compute a vector $d = (B + μI)^{-1} y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "\n",
    "Define $U', V'$, as the first 8 columns of and as the upper left 8x8 block of $U, V$, and $Σ'$ for $Σ$.Determine the Frobenius norm of the difference between $B' = U'Σ'V'^{T}$ and B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6 \n",
    "\n",
    "Now, we want to efficiently approximate $d$ using this approximation $B'$ using the Sherman-Morrison-Woodbury formula. Compute $d' = (U' Σ' V'^{T} + \\mu I)^{-1}y$ using that formula and determine the 2-norm of $d − d'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Singular Value Decomposition\n",
    "\n",
    "Read this <a href=http://www.frankcleary.com/svdimage/> article </a> on image compression with SVD and use numpy apply it to images from the MNIST Datasets. To get the MNIST Datset, use the following comands. Note that the mnist data has shape (70000, 784). You will need to convert it to 28x28 images [Hint: np.reshape].\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original', data_home=custom_data_home)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: KNN\n",
    "\n",
    "k-nearest neighbors algorithm is one of the most famous machine learning algorithms. Read about it <a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm> here </a>. In this exercise, you will build k-nn from scratch and apply it on the IRIS data set. This will test your knowledge of numpy and linear algebra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from __future__ import division\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "    def __init__(self, n_neighbors):\n",
    "        self.n_neighbors = n_neighbors\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X_tr = X \n",
    "        self.y_tr = y\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
