{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "### Introduction to SVM\n",
    "Week **5** | Lesson **6 **\n",
    "\n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "*After this lesson, you will be able to:*\n",
    "- **Demenstrate** an understanding of how SVM separates data with a plane with written responses and graphics \n",
    "- **Analyze** and understand how the hyperparameter $C$ affects SVM's soft margins \n",
    "- **Explain** how the hinge loss error function works \n",
    "- **Describe** what the kernal trick is and how it works in terms of linear algebra and graphics\n",
    "\n",
    "---\n",
    "### Student Pre-Work \n",
    "\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Explian what a linear transformation is in terms of linear algebra\n",
    "\n",
    "----\n",
    "\n",
    "## Lesson\n",
    "\n",
    "The Support Vector Machine (SVM) algorithm is a different approach to classification.\n",
    "\n",
    "These fit a decision boundary similarly to a logistic regression, but uses a different loss function called the \"hinge loss\" (as opposed to the log loss in logistic regression).\n",
    "\n",
    "SVMs are notorious for being less intuitive than other classifiers like kNN and Logistic regression, but are quite powerful\n",
    "\n",
    "[For a really great resource check out these slides (some of which are cannabalized in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "\n",
    "[This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does the SVM classify?\n",
    "\n",
    "---\n",
    "\n",
    "It's important to start with the intuition for the special _**linearly separable**_ classification case.\n",
    "\n",
    "\n",
    "If classification of observations is \"linearly separable\", SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM](./images/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition behind the SVM decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "SVM's criterion for a decision surface is one that is _maximally far away from any data point between classes_. The distance from the decision boundary to the closest data point determines the \"margin\" of the classifier.\n",
    "\n",
    "The points SVM uses to fit the decision boundary are called \"support vectors\". This term comes from linear algebra: in a vector space, points can be defined as a vector between the origin and that point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./images/Margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why maximize the margin?\n",
    "\n",
    "---\n",
    "\n",
    "**SVM solves for a decision boundary that hypothetically minimizes generalization error.** \n",
    "\n",
    "Observations that are near the decision boundary between the classes are the most ambiguous observations. They are the observations that are approaching equal chance to be one class or the other.\n",
    "\n",
    "SVM, instead of considering all the observations \"equally\" in the loss function it minimizes, defines it's fit using the most ambiguous points. It's decision boundary is _safe_ in that errors in new measured observations are not likely to cause the SVM to mis-classify.\n",
    "\n",
    "The SVM is concerned with generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Perceptron Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "The perceptron algorithm is a linear function of weights $w$ and predictors $X$ that assigns points into binary classes. If the function returns a value greater than 0 then the observation is classified as 1, otherwise it is classified as zero.\n",
    "\n",
    "$f(X)$ below is the perceptron function that would determine the classes. $b$ here is known as the \"bias\" and is analagous to the intercept in a regression.\n",
    "\n",
    "### $$ f(X) = \\begin{cases}1 & \\text{if }w \\cdot x + b > 0\\\\0 & \\text{otherwise}\\end{cases} $$\n",
    "\n",
    "If the points are linearly seperable, solving the perceptron is guaranteed to converge on a solution, but that solution is not necessarily optimal for future observations. This led to the invention of the SVM, which finds the optimal discriminator: the maximum margin hyperplane.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## The maximum margin hyperplane\n",
    "\n",
    "---\n",
    "\n",
    "We can impose a \"normalizing constant\" $c$ on the equation for the line or hyperplane that separates the points:\n",
    "\n",
    "### $$ c(w^T x + b) = 0 $$\n",
    "\n",
    "And then choose this normalizing constant such that the distance from the plane to the closest points of either class will be 1.\n",
    "\n",
    "### $$ w^T x_{+} + b = 1 \\\\ w^T x_{-} + b = -1 $$\n",
    "\n",
    "For the distance to the closest positive and negative class points, respectively (known as \"support vectors\").\n",
    "\n",
    "If the normalizer for the weights is $||w||$, the size of the margin is then:\n",
    "\n",
    "### $$ \\text{margin} = \\frac{2}{||w||} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./images/linear_sep_support_vecs_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the maximum margin\n",
    "\n",
    "---\n",
    "\n",
    "We want to find a distance between these points that is the biggest possible. Therefore we are looking to maximize the value $\\frac{2}{||w||}$.\n",
    "\n",
    "So, maximize the weights with constraints on what the function can output. When the target is 1, the function needs to be 1 or greater. When the target is 0 (or -1), the function needs to output -1 or lower.\n",
    "\n",
    "### $$ \\underset{w}{\\text{max}} \\frac{2}{||w||} \\quad \\text{subject to} \\begin{cases}\\text{if } y_i = 1 & w^T x_i + b \\ge 1 \\\\ \\text{if } y_i = -1 & w^T x_i + b \\le -1 \\end{cases} \\text{for } i \\text{ in } N$$\n",
    "\n",
    "Note here that $y$ is specified as either 1 or -1, as opposed to the 0, 1 that we are used to. This is convenient for converting this to be a minimization. To make this a minimization, we can change the equation to be:\n",
    "\n",
    "### $$ \\underset{w}{\\text{min }} ||w||^2 \\quad \\text{subject to} \\quad y(w^T x_i + b) \\ge 1 $$\n",
    "\n",
    "Which works out because when $y = -1$ the values become positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./images/linear_sep_support_vecs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linearly separable cases and the hinge loss function\n",
    "\n",
    "---\n",
    "\n",
    "In cases where there is no line or plane that can separate all of the points perfectly, we need to introduce the capability for error into our equation. With the constraint above that $y(w^T X + b) \\ge 1$, we can introduce the **hinge loss function**:\n",
    "\n",
    "### $$ \\text{hinge loss} = \\sum_{i=1}^n \\max\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "Where now, if the point is on the correct side (correctly classified), the value of the maximization will be 0. If the point is not $\\ge 1$, this function will be greater than zero.\n",
    "\n",
    "Using $f(x_i) = (w^T x_i + b)$, \n",
    "\n",
    "### $$\\text{hinge loss} = \\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right)$$ \n",
    "\n",
    "If $f(x_i) > 1$, the point lies _outside_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) = 1$ the point is _on_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) < 1$ the point lies _inside_ the margin and **does** contribute to the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Hinge loss](./images/hinge_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hinge loss and \"slack\"\n",
    "\n",
    "---\n",
    "\n",
    "When we have a scenario where it is not possible to perfectly separate, we use the hinge loss with a regularization constant $C$:\n",
    "\n",
    "### $$ \\min ||w||^2 + C\\sum_{i=1}^N \\epsilon_i \\quad \\text{subject to} \\quad y(w^T x_i + b) \\ge 1 - \\epsilon_i $$\n",
    "\n",
    "Where the $\\epsilon$ are the errors of the classifier, and the $C$ is a regularization term that determines how much the classification errors matter (relative to the maximization of the margin).\n",
    "\n",
    "The function that the SVM minimizes to find the boundary is:\n",
    "\n",
    "### $$  \\underset{w}{\\text{min }} ||w||^2 + C\\sum_{i=1}^N max\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "A small $C$ creates a wider margin because errors will matter less. A large $C$ creates a tighter margin because errors matter more. An infinite $C$ parameter is a \"hard\" margin, which always minimizes error over the size of the boundary.\n",
    "\n",
    "We are trying to minimize the norm of the weights $||w||$ and thus maximize the margin, but we are _also_ trying to minimize our errors at the same time. We have a balance in our minimization between how wide the margin should be and how much error we should incur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The regularizing hyper-parameter $C$\n",
    "\n",
    "---\n",
    "\n",
    "The hyper-parameter $C$ controls the extent to which the SVM is tolerant to errors. It is sometimes called the \"soft-margin constant\". \n",
    "\n",
    "$C$ affects the strength of the \"penalty\" similar to the lambda terms in the Ridge and Lasso. \n",
    "\n",
    "By multiplying the sum of the errors, which are the distances from the margins to the points inside of the margin, it allows the SVM to classify non-linearly separable problems by allowing errors to occur. \n",
    "\n",
    "The lower the value of $C$, the more misclassified observations errors are allowed. These misclassified points are known as \"slack variables\". Reducing the effect of errors on the loss function puts the emphasis on widening the margin.\n",
    "\n",
    "For those interested in exporing the math more, [there is a good tutorial here.](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/#more-457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./images/slack_variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./images/soft_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuit Soft Margins \n",
    "[Interactive SVM Widget](Selective-Vector-Machines/svm-visualization.ipynb) **Check out this notebook** for an interactive viz on the effects of increasing and decreasing the value of hyperparameter $C$\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. What happens when you change the value of $C$ to 5? Is this an instance of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What happens when you change the value of $C$ to -4? Is this an instance of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The \"kernel trick\" for non-linearly seperable problems\n",
    "\n",
    "---\n",
    "\n",
    "The \"kernel trick\" allows an SVM to classify non-linearly separable problems. It is a big reason why SVMs are so popular.\n",
    "\n",
    "The idea behind the kernel trick is that you can arbitrarily transform your observations that _have no linear seperability_ by putting them into a different \"dimensional space\" where the DO have linear seperability, fit an SVM in that higher dimensional space, and then invert the transformation of the data and the model itself back into the original space.\n",
    "\n",
    "This is done by \"wrapping\" your predictors in a kernel function that transforms them into this higher dimensional space. \n",
    "\n",
    "[Check out these lecture slides for more detail.](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "The following pictures should give you a general intuition for what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./images/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anamation \n",
    "The animation shows how data from a 2 dimensional vector space can be transformed into 3 dimensional vector space. Once the data has been transformed into a higher dimensional space, a plane can freely pass between the blue and red data points and create a boundary. Once the plane is optimually fitted between the blue and red points, the data is transformed back into the lower \n",
    "2 dimensional vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://rvlasveld.github.io/images/oc-svm/visualization.gif)\n",
    "\n",
    "**Note:** The colors between the animation and the static pictures are reversed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![nonsep_transformed](images/Nonseparable_Transformed.png)\n",
    "\n",
    "We can transform the data visualized in the above figure from $[x,y]$ space to $[x,y,x^2+y^2]$ space and suddenly, a clear boundary appears!\n",
    "\n",
    "![twoD-to-threeD](images/TwoD_to_ThreeD_Boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Examples\n",
    "\n",
    "The following images show how different kenerls fit to the same data set. For this data set, we can see that both the polynomial and gaussian kernels do a better job at creating a boundary between the classes than the linear kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![polynomial kernel](./images/nonlinear-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![gaussian kernel](./images/nonlinear-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we covered: \n",
    "\n",
    "- How SVM separates data with a plane with written responses and graphics \n",
    "- How the hyperparameter $C$ affects SVM's soft margins \n",
    "- How the hinge loss error function works \n",
    "- What the kernal trick is and how it works in terms of linear algebra and graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Additional Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SVM Animation](https://www.youtube.com/watch?v=3liCbRZPrZA) A YouTube video showing how 2D data can be mapped into 3D with a gaussian kernal and how a plane can intersect the paraboloid to create a circular boundary between the two classes. \n",
    "\n",
    "[Optional: Deeper Dive into SVM Theory](./Optional-and-Supplemental-SVM-Lesson.ipynb) This jupyter notebook provides an exposer into the rigorous mathematics behind SVM. The lesson provided you just covered in class strikes a balance between math and concept in order to explain SVM as pain free as possible. The Deep Dive notebook will satisfy your curious about where the equations from today's lesson came from. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
