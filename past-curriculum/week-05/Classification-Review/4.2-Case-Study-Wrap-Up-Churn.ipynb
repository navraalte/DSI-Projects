{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "### Case Study Wrap Up: Churn\n",
    "\n",
    "Week **5** | Lesson **7 **\n",
    "\n",
    "---\n",
    "| TIMING  | TYPE  \n",
    "|:-:|---|---|\n",
    "| 25 min| [Review](#review) |\n",
    "| 10 min| [**Industry Example **](#hook) |\n",
    "| 45 min| [**Lesson **](#content) |\n",
    "| 20 min| [Conclusion](#conclusion) |\n",
    "| 5 min | [Additional Resources](#more)\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- **Plot, analyze, and explain** Random Forest's performance on learning curves \n",
    "- **Plot, analyze, and explain**  Gradient Boosted Trees'performance on learning curves\n",
    "- **Plot, analyze, and explain**  Random Forest's performance on ROC and Precision vs. Recall curves \n",
    "- **Plot, analyze, and explain**  Gradient Boosted Trees'performance on ROC and Precision vs. Recall curves \n",
    "\n",
    "---\n",
    "### Student Pre-Work \n",
    "\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Install ipyparallel by running: conda install ipyparallel\n",
    "- Plot, interpret, and explain what a Roc Curve is communicating about model performance\n",
    "- Plot, interpret, and explain what a  Precision vs. Recall Curve is  communicating about model performance\n",
    "- Explain and demonstrate how to use Sklearn's Gridsearch\n",
    "- Read, interpret and explain what learning curves are communicating about model performance\n",
    "- Read, interpret, and explain a Bias/Variance Error plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"review\"></a>\n",
    "## Review: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Monday we went through an extensive review on classification metrics and logistic regression. We also covered sampling methods and feature engineering in order to help bring up logistic regression's poor performance. However, we discovered that all the sampling and feature engineering we attempted could not compensate for the fact that LR is underfitting the data. The solution to this problem is a model with more complexity. \n",
    "\n",
    "\n",
    "![](http://www.kdnuggets.com/wp-content/uploads/bias-variance-total-error.jpg)\n",
    "\n",
    "Undertting models are heavily biased model's Logistic Regression's learning curves showed strong signs of bias. \n",
    "\n",
    "Now that we learned more complex and power models like Random Forest and Gradient Boosted Trees, we can test the affects of fitting a more complex model to the churn data set. \n",
    "\n",
    "### Data Science Work Flow \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# change path to where you have the DSI 5 repo stored (I can't do that for you.)\n",
    "Image(filename='/Users/Alexander/DSI-SF-5/ds_workflow.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"hook\"> </a>\n",
    "\n",
    "## Applications: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn rate is often used in industry to predict, maintain, and increase user retention. \n",
    "\n",
    "### Amazon\n",
    "\n",
    "[Amazon](https://www.fool.com/investing/2016/06/01/amazon-prime-improves-its-customer-retention-rate.aspx) retains more customers than Cosco through their Prime membership.\n",
    "![](http://www.rokuhelp.com/wp-content/uploads/2017/02/amazon-696x311.png) \n",
    "\n",
    "### Uber\n",
    "[Uber](http://www.sfchronicle.com/business/article/Uber-incentives-aim-to-lure-power-drivers-7089842.php) creates new incentive programs in order to help maintain driver retention. \n",
    "![](http://www.techproducts.com.ng/wp-content/uploads/2016/06/uber-serp-logo-f6e7549c89-660x330.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Do\n",
    "\n",
    "Read one of the above articles and describe in your own words how the tech company used insights from churn prediction to help retain users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Lesson**\n",
    "<a name=\"content\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from __future__ import division\n",
    "from cross_val_tool import cross_validation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score \n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# gridsearch\n",
    "import os\n",
    "from time import sleep\n",
    "from pprint import pprint\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import  ShuffleSplit\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from ipyparallel import Client\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean data \n",
    "\n",
    "Perform the same cleaning and scaling as we did in the first churn case study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data_path = \"~/DSI-SF-5/datasets/churn.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# drop irrelavent columns like zip code and user phone number\n",
    "relavent_cols = df.columns[4:]\n",
    "df = df[relavent_cols]\n",
    "\n",
    "# replace any positve categorical with 1 \n",
    "true = [\"yes\",\n",
    "        \"True\",\n",
    "        \"True.\"]\n",
    "# replace any negative categorical with 0\n",
    "false = [\"no\",\n",
    "         \"False\",\n",
    "         \"False.\"]\n",
    "\n",
    "df.replace(to_replace=true, value=1, inplace=True)\n",
    "df.replace(to_replace=false, value=0, inplace=True)\n",
    "\n",
    "# rename churn column\n",
    "df.rename(columns={'Churn?':'Churn'}, \n",
    "          inplace=True)\n",
    "\n",
    "# before scaling, exclude the categorical features\n",
    "numerical_cols = df.columns[2:-1]\n",
    "\n",
    "# columns won't rescale unless dtype = \"float\"\n",
    "df2 = df[numerical_cols].astype(float)\n",
    "\n",
    "\n",
    "# scale along the features\n",
    "df2[numerical_cols] = df2[numerical_cols].apply(lambda x: MinMaxScaler(feature_range=(-1,1)).fit_transform(x));\n",
    "\n",
    "# Move columns with boolean values back into dataframe\n",
    "df2[df.columns[0]] = df[df.columns[0]].values\n",
    "df2[df.columns[1]] = df[df.columns[1]].values\n",
    "df2[df.columns[-1]] = df[df.columns[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distribution of features - after rescaling\n",
    "df2.hist(color='r', alpha=0.5, bins=50, figsize=(18, 14));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Models using Gridsearch\n",
    "\n",
    "We'll be using a custom build gridserach to optimize our models. This version is faster than Sklearn's implementation because this version persist read-only files of the data into memory, while Sklearn's gridsearch has to read the data from hard disk every single time it cross validates a parameter combination. \n",
    "\n",
    "**Check out this supplemental notebook that dives deeper into** optimizing memory management for [Gridsearch](./Supplemental-Memory-Management.ipynb)\n",
    "\n",
    "You might find the [ipyparallel docs](http://ipyparallel.readthedocs.io/en/latest/intro.html) useful, you might also not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partition dataframe into a design matrix and response vector\n",
    "x_cols = df.columns[:-1]\n",
    "X = df2[x_cols].values\n",
    "Y = df2.Churn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = shuffle(X, Y)\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y, test_size = 0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Engineering functions used to greatly speed up model optimization \n",
    "def persist_cv_splits(X, y, n_cv_iter=3, name='data', suffix=\"_cv_%03d.pkl\", test_size=0.25, random_state=None):\n",
    "    \"\"\"Materialize randomized train test splits of a dataset.\"\"\"\n",
    "\n",
    "    cv = ShuffleSplit(X.shape[0], \n",
    "                      n_iter=n_cv_iter,\n",
    "                      test_size=test_size, \n",
    "                      random_state=random_state)\n",
    "    \n",
    "    cv_split_filenames = []\n",
    "\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        \n",
    "        cv_fold = (X[train], y[train], X[test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames\n",
    "\n",
    "def compute_evaluation(cv_split_filename, model, params):\n",
    "    \"\"\"Function executed by a worker to evaluate a model on a CV split\"\"\"\n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    X_train, y_train, X_validation, y_validation = joblib.load(\n",
    "        cv_split_filename, mmap_mode='c')\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_validation)\n",
    "    # accuracy\n",
    "    #validation_score = model.score(X_validation, y_validation)\n",
    "    \n",
    "    # f1_score\n",
    "    validation_score = f1_score(y_validation, y_pred, average = \"macro\")\n",
    "    \n",
    "    return validation_score\n",
    "\n",
    "def grid_search(lb_view, model, cv_split_filenames, param_grid):\n",
    "    \"\"\"Launch all grid search evaluation tasks.\"\"\"\n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, model, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks\n",
    "\n",
    "def progress(tasks):\n",
    "    return np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])\n",
    "\n",
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "        \n",
    "    \n",
    "    try: \n",
    "        mean_scores = np.array(mean_scores)\n",
    "        sorted_ind = np.argsort(mean_scores.T[0])[::-1]\n",
    "        return mean_scores[sorted_ind][:n_top]\n",
    "    \n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split and persist the training data into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_split_filenames = persist_cv_splits(X_train, Y_train, name='churn-splits', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the ipyparallel library to create a cluster on the laptop that will allow us to run multiple grid searches simultaneously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shut down any pre-existing local clusters\n",
    "!ipcluster stop\n",
    "sleep(3)\n",
    "# start a local cluster with 3 nodes\n",
    "!ipcluster start -n=3 --daemon\n",
    "sleep(8)\n",
    "# create a client to send commands to the cluster\n",
    "client = Client();\n",
    "sleep(8)\n",
    "print (\"Number of worker nodes in cluser {}\".format(len(client)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to optimize two machine learning models Random Forest and Gradient Boosted Trees. Both models have excellent features that we'll discuss shortly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instatiate model\n",
    "rfc = RandomForestClassifier()\n",
    "# fill a dictionary with different values for each hyper-parameter\n",
    "rfc_params = {'max_depth': [None, 4, 8, 12],\n",
    "              'min_samples_split': [2,6,10],\n",
    "             'max_leaf_nodes': [None, 4, 8, 12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_view = client.load_balanced_view()\n",
    "all_parameters, all_tasks = grid_search(lb_view, \n",
    "                                        rfc, \n",
    "                                        digits_split_filenames, \n",
    "                                        rfc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell every 3 seconds or so to get an update on the completion percentage of the grid serach. \n",
    "You will have to wait until this gridseach is complete before you run the next gridserach for GradientBoostingClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "print(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print out shows that the parameters in the top row lead to the hightest F1 score for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instatiate model\n",
    "gbt = GradientBoostingClassifier()\n",
    "# fill a dictionary with different values for each hyper-parameter\n",
    "gbt_params = {'max_depth': [None, 4, 8, 10],\n",
    "              'min_samples_split': [2,6,10],\n",
    "             'max_leaf_nodes': [None, 4, 8, 12],\n",
    "              \"learning_rate\" : [0.01, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_view = client.load_balanced_view()\n",
    "all_parameters, all_tasks = grid_search(lb_view, \n",
    "                                        gbt, \n",
    "                                        digits_split_filenames, \n",
    "                                        gbt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell every 3 seconds or so to get an update on the completion percentage of the grid serach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "print(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print out shows that the parameters in the top row lead to the hightest F1 score for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stop cluster\n",
    "!ipcluster stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You DO: Cross Valdiate Models \n",
    "\n",
    "Let's fit each model with the unbalanced data set to see how it performance. Analyze the learning curves and write your observations in the cell provided below the learning curve plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2')\n",
    "cv = cross_validation(lr, X_train, Y_train , n_splits=10,init_chunk_size = 100, chunk_spacings = 25, average = \"binary\")\n",
    "cv.train_for_learning_curve()\n",
    "cv.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(min_samples_split = 6, max_leaf_nodes = None, max_depth = None)\n",
    "cv_rfc = cross_validation(rfc, X_train, Y_train , n_splits=10,init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\n",
    "cv_rfc.train_for_learning_curve()\n",
    "cv_rfc.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(min_samples_split = 2, max_leaf_nodes = None, learning_rate = 0.1, max_depth = 8)\n",
    "cv_gbt = cross_validation(rfc, X_train, Y_train , n_splits=10, init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\n",
    "cv_gbt.train_for_learning_curve()\n",
    "cv_gbt.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Cruves\n",
    "\n",
    "Let's plot the roc curves of all three models on the same viz and compare their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2')\n",
    "lr.fit(X_train, Y_train)\n",
    "y_lr_prob = lr.predict_proba(X_test)\n",
    "\n",
    "rfc = RandomForestClassifier(min_samples_split = 6, max_leaf_nodes = None, max_depth = None)\n",
    "rfc.fit(X_train, Y_train)\n",
    "y_rfc_prob = rfc.predict_proba(X_test)\n",
    "\n",
    "gbt = GradientBoostingClassifier(min_samples_split = 2, max_leaf_nodes = None, learning_rate = 0.1, max_depth = 8)\n",
    "gbt.fit(X_train, Y_train)\n",
    "y_gbt_prob = gbt.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_list = [y_lr_prob, y_rfc_prob, y_gbt_prob]\n",
    "model_names = [\"Logistic\", \"RF\", \"GBT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_roc_curves(Y_test, prob_list, model_names):\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    auc_list = []\n",
    "    for y_prob in prob_list:\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test, y_prob.T[1])\n",
    "        area_under_curve = auc(fpr, tpr)\n",
    "\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "        auc_list.append(area_under_curve)\n",
    "\n",
    "    x = np.arange(0,Y_test.shape[0]+1, Y_test.shape[0])\n",
    "    y = x\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (12,5))\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.xlim(-0.01, 1.01)\n",
    "    plt.ylim(0,1.01)\n",
    "    plt.plot(fpr_list[0], tpr_list[0], 'g', label = \"{}: AUC = {:.3}\".format(model_names[0], auc_list[0]));\n",
    "    plt.plot(fpr_list[1], tpr_list[1], 'r', label = \"{}: AUC = {:.3}\".format(model_names[1], auc_list[1]));\n",
    "    plt.plot(fpr_list[2], tpr_list[2], 'b', label = \"{}: AUC = {:.3}\".format(model_names[2], auc_list[2]));\n",
    "\n",
    "    plt.plot(x,y,'--r');\n",
    "    plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_roc_curves(Y_test, prob_list, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You do\n",
    "\n",
    "Analyze the ROC curve and describe in your own words what the viz is communicating about the performance of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_curves(Y_test):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    avg_pre_list = []\n",
    "\n",
    "\n",
    "    for y_prob in prob_list:\n",
    "\n",
    "        pre, rec, pre_rec_thresholds = precision_recall_curve(Y_test, y_prob.T[1])\n",
    "        area_under_precision_recall_curve = average_precision_score(Y_test, y_prob.T[1])\n",
    "\n",
    "        precision_list.append(pre)\n",
    "        recall_list.append(rec)\n",
    "        avg_pre_list.append(area_under_precision_recall_curve)\n",
    "\n",
    "    x = np.arange(0,Y_test.shape[0]+1, Y_test.shape[0])\n",
    "    y = -1*x\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (12,5))\n",
    "    plt.title(\"Precision Recall Curve\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.xlim(-0.01, 1.01)\n",
    "    plt.ylim(0,1.01)\n",
    "    plt.plot(recall_list[0], precision_list[0], 'g', label = \"{}: AUC = {:.3}\".format(model_names[0], avg_pre_list[0]));\n",
    "    plt.plot(recall_list[1], precision_list[1], 'r', label = \"{}: AUC = {:.3}\".format(model_names[1], avg_pre_list[1]));\n",
    "    plt.plot(recall_list[2], precision_list[2], 'b', label = \"{}: AUC = {:.3}\".format(model_names[2], avg_pre_list[2]));\n",
    "\n",
    "    n_samples = Y_test.shape[0]\n",
    "\n",
    "    x = np.linspace(0.0, 1.0, n_samples)\n",
    "    y = np.linspace(1.0, 0, n_samples)\n",
    "    plt.plot(x, y, '--b', alpha = 0.3)\n",
    "    plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_precision_recall_curves(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You do\n",
    "\n",
    "Analyze the ROC curve and describe in your own words what the viz is communicating about the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this cell to markdown and write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Logistic's precision recall curve is worest than average. This isn't surprising considing the model's learning curves. We can also see that Random Forest out performs Logistic and Gradient Boosted Trees. Although, there is a narrow range, between 0.65 and 0.75, where GBT slightly out performs RF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing LR and RF on the Holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2')\n",
    "lr.fit(X_train, Y_train)\n",
    "y_lr_pred = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(Y_test, y_lr_pred), precision_score(Y_test, y_lr_pred), recall_score(Y_test, y_lr_pred)\n",
    "\n",
    "print confusion_matrix(Y_test, y_lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(min_samples_split = 6, max_leaf_nodes = None, max_depth = None)\n",
    "rfc.fit(X_train, Y_train)\n",
    "y_rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "print accuracy_score(Y_test, y_rfc_pred), precision_score(Y_test, y_rfc_pred), recall_score(Y_test, y_rfc_pred)\n",
    "\n",
    "print confusion_matrix(Y_test, y_rfc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matricies and the metrics built from it show that Random Forest performs significantly better than Logistic Regression. This was acheived with minimual data preperation: scaling values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Now that we have confirmed that Random Forest has reliable results. Let's use its feature importance method to find out which features are most predictive of churn. Use like Amazon and Uber, we can use information as actionable insight to help retain users and help aquire more users in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create array of feture names for ploting\n",
    "# call the array field_names\n",
    "\n",
    "# create array of feature importance weights from Random Forest\n",
    "# call the array feat_weights\n",
    "\n",
    "# use np.argsort to get an array of ordered indicies that can be used to \n",
    "# sort feature importance weights from largest to smallest\n",
    "# call the array indices\n",
    "\n",
    "# Create a list comprehension that collects the feature importance values for every tree in the RF ensemble\n",
    "# Hint: Use rfc.estimators_\n",
    "# call the list feat_importance_trees\n",
    "\n",
    "# take the standard deviation of this list, this resulting list is the std value for each feature \n",
    "# call the array std\n",
    "\n",
    "# create an int variable call n_features that is the numer of features in the feature set\n",
    "\n",
    "# I'm giving you this one =) \n",
    "tick_marks_list = list(range(len(field_names)))\n",
    "\n",
    "# create an array of sorted feature names\n",
    "# call the array sorted_feat_names\n",
    "\n",
    "# create an array of soreted std \n",
    "# call the array sorted_std\n",
    "\n",
    "# create an array of sorted feature weights\n",
    "# call the array sorted_feat_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.ylabel(\"Information Gain\")\n",
    "plt.bar(list(range(n_features)), sorted_feat_weights, color=\"g\",alpha = 0.5, yerr=sorted_std, align=\"center\")\n",
    "plt.xticks(tick_marks_list, sorted_feat_names, rotation=90)\n",
    "plt.xlim([-1, len(field_names)])\n",
    "plt.ylim([0,0.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Results\n",
    "\n",
    "Recall that Random Forest randomly selects a subset of features to split nodes on, those features that lead to the greatest information gain are used to split the node. If a majority of trees choose feature VMail Message to split the first node in the tree, then that feature is ranked as being most important because it is responsible for the most amount of information gain in the ensemble. \n",
    "\n",
    "### Correlation vs. Causality\n",
    "\n",
    "![](https://imgs.xkcd.com/comics/correlation.png)\n",
    "\n",
    "The information gain criteria say **NOTHING ABOUT CAUSATION**. In practice we will never know about direct causation between features. This is true because there are always 2nd, 3rd, ..., nth latent features that influence the behavior of a feature. A [latent feature](https://en.wikipedia.org/wiki/Latent_variable) is a hidden feature that influences the outcome of another feature. \n",
    "\n",
    ">Latent variables , as opposed to observable variables, are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured).\n",
    ">--Wikipdia\n",
    "\n",
    "For this reason we can't be sure that Eve Calls and Day Charges directly cause churn. However, we can still use their importance to turn a profit! But before we can do that, we need to determine if VMail Messages and Day Mins are positively or negativately correlated with churn. For this task we can use Logistic Regression's odds ratio feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take top k feature given by RF and analyze the odds ratio from LR \n",
    "# let k = 5\n",
    "\n",
    "\n",
    "# create a new desgin matrix and call it X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2')\n",
    "lr.fit(X2, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cut off training to prevent overfitting \n",
    "lr = LogisticRegression(penalty='l2');\n",
    "lr.fit(X2, Y);\n",
    "# remove nested list\n",
    "feature_weights = np.exp(lr.coef_[0])\n",
    "\n",
    "sorted_indicies = np.argsort(feature_weights)\n",
    "\n",
    "# sort features by odds\n",
    "sorted_features = feature_weights[sorted_indicies]\n",
    "\n",
    "plt.figure(figsize = (12,4))\n",
    "plt.title('Odds Ratio of Click Through vs. Features')\n",
    "sb.barplot(sorted_features, top_k_feats[sorted_indicies], palette=\"RdBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Interpretation of Feature Weights\n",
    "\n",
    "We define the odds of Y = 1 as,\n",
    "$$odds = \\frac{P(Y = 1)}{1 - P(Y = 1)} = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}$$\n",
    "\n",
    "We interpret the $\\beta's$ in multiplicative terms with respect to the odds.\n",
    "\n",
    "E.g., the interpretation of $\\beta_1$ is, holding all the other variables/features fixed, for every increase of 1 in $X_1$, the odds of $Y = 1$ increases by a factor of $e^{\\beta_1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at Night calls and Eve Calls. Their odds ratios are just over 1.0. This means that for every increase of 1 for Night calls and Eve Calls, a costumer is sligtly more likely to churn rathe than not churn. But these results unfortably close to a 1-to-1 odds of churn/not churn. So this model makes a very weak statement about Eve Calls being a directly correlating feature with churn.\n",
    "\n",
    "Well, those are not very strong statemetns. Keep in mind that Logistic Regression is a very poor performing model with this data set. Unfortunately, this is the only model that we know of that will isolate the effect of a single predictor on the response variable. \n",
    "\n",
    "The Day Charge feature is telling us that for every increase of 1 in day charges, 4 times out of 5 times a customer will churn. We can work with these odds. \n",
    "\n",
    "### Testing for Effect\n",
    "\n",
    "The good news is that we can test this statement. We can design an experiment in which coupons are sent to cosmters with high day time charges. We would have to create a threshold, create a cost/benefit matrix, and plot a profit curve to know what portion of the population classied as positive for churn we need to send coupons to to maximize profits. \n",
    "\n",
    "The previous [churn case study](./5.1-Classification-Review-and-Case-Study-Churn-Problem.ipynb) gives an example of how to create such an experiment. After running such an experiment and collecting a statistically significant number of samples, we can then run an A/B test to determine if the coupon had any statistically significant impact in decreasing churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "<a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we covered how to: \n",
    "\n",
    "- Plot, analyze,and explain Random Forest's performance on learning curves \n",
    "- Plot, analyze,and explain Gradient Boosted Trees'performance on learning curves\n",
    "- Plot, analyze,and explain Random Forest's performance on ROC and Precision vs. Recall curves \n",
    "- Plot, analyze,and explain Gradient Boosted Trees'performance on ROC and Precision vs. Recall curves \n",
    "\n",
    "We also explored an alternative implementation of Grid Search that optimizes memory management in order speed up cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"more\"></a>\n",
    "## Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check out this supplemental notebook that dives deeper into** optimizing memory management for [Gridsearch](./Supplemental-Memory-Management.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
