{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "### Ensemble Methods: Random Forests, Boosting\n",
    "\n",
    "** Week 5 | Lesson 3.2 **\n",
    "\n",
    "---\n",
    "| TIMING  | TYPE  \n",
    "|:-:|---|---|\n",
    "| 25 min| [Review: Decision Trees](#review) |\n",
    "| 10 min| [Kaggle Winners](#hook) |\n",
    "| 45 min| [Random Forests, Boosting (Adaboost, GBTs), ](#content) |\n",
    "| 20 min| [Conclusion](#conclusion) |\n",
    "| 5 min | [Additional Resources](#more)\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "\n",
    "- Explain what Random Forest is and how it is different from Bagging of Decision trees\n",
    "- Explain what Extra Trees models are\n",
    "- Apply both techniques for supervised machine learning\n",
    "- Describe Boosting and how it differs from Bagging\n",
    "- Apply Adaboost and Gradient Boosting for supervised machine learning problems\n",
    "---\n",
    "### Student Pre-Work \n",
    "\n",
    "*Before this lesson, you should already be able to:*\n",
    "\n",
    "- Decision Trees \n",
    "- Bias, Variance Trade Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"review\"></a>\n",
    "### Review: Decision Trees, Bagging \n",
    "\n",
    "| Algorithm  | Assumptions | Class/Reg/Both | Loss | Regularization | Parameters |Hyperparameters| Metrics | Intuition | Implementation\n",
    "|:-:|---|---|\n",
    "|Decision Trees| Partition the dataset into regions | Both | Regression: Minimize Variance in splits; Classification: Gini, Entropy | None | None |<a href=http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html> Parameters in the SKlearn Documentation </a> | Relevant Metrics for Classification and Regression |<a href=https://www.youtube.com/watch?v=eKD5gxPPeY0> Decision Tree for Applied ML </a>| <a href=http://gabrielelanaro.github.io/blog/2016/03/03/decision-trees.html> Decision Tree Implementation using Numpy </a>|\n",
    "\n",
    "> **Check:** What are the main advantages of decision trees?\n",
    "\n",
    "---\n",
    "> **Check:** Describe how the bagging algorithm works. What is the difference between boostrapping and bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"hook\"> </a>\n",
    "\n",
    "### Applications: Algorithms of Champions \n",
    "<a href=http://blog.kaggle.com/2016/11/03/red-hat-business-value-competition-1st-place-winners-interview-darius-barusauskas/> Winners Solution: Predicting Customer Business Value </a>\n",
    "\n",
    "<a href=http://blog.kaggle.com/2017/01/12/santander-product-recommendation-competition-2nd-place-winners-solution-write-up-tom-van-de-wiele/> Winners Solution: Product Recommendations </a>\n",
    "\n",
    "<a href=http://blog.kaggle.com/> Blog for Looking Through Past Solutions </a>\n",
    "\n",
    "<img src=http://www.kdnuggets.com/wp-content/uploads/kaggle.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Random Forests, Extra Trees, AdaBoost, and GBTs \n",
    "<a name=\"content\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://image.slidesharecdn.com/slides-141010042109-conversion-gate01/95/understanding-random-forests-from-theory-to-practice-16-638.jpg?cb=1412914915> \n",
    "\n",
    "#### Random Forests\n",
    "Another way of looking at it...<a href=https://www.coursera.org/learn/practical-machine-learning/lecture/XKsl6/random-forests> Coursera RF </a>\n",
    "\n",
    "While DTs are powerful, DTs have some limitations. In particular, trees that are **grown very deep** tend to learn highly irregular patterns: they **overfit their training sets.** Bagging helps mitigate this problem by exposing different trees to different sub-samples of the whole training set.\n",
    "\n",
    "Random forests are a further way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "\n",
    "\n",
    "Random Forests are some of the most widespread classifiers and regression models used.  They are relatively simple to use because they require very few parameters to set and they perform pretty well.\n",
    "\n",
    "> **Check:** Describe how the bagging algorithm works:\n",
    "\n",
    "\n",
    "_Random forests_ differ from bagging decision tree in only one way: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a **random subset of the features**. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the bagging base trees, causing them to become correlated. By selecting a random subset of the features at each split, we counter this correlation between base trees, strengthening the overall model.\n",
    "\n",
    "Typically, for a classification problem with $p$ features, $\\sqrt{p}$ (rounded down) features are used in each split. For regression problems the inventors recommend $p/3$ (rounded down) with a minimum node size of 5 as the default.\n",
    "\n",
    "\n",
    "\n",
    "#### Extremely Randomized Trees\n",
    "Adding one further step of randomization yields extremely randomized trees, or _ExtraTrees_. These are trained using bagging and the random subspace method, like in an ordinary random forest, but an additional layer of randomness is introduced. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity), **for each feature under consideration, a random value is selected for the split.** This value is selected from the feature's empirical range (in the tree's training set, i.e., the bootstrap sample), in other words, the top-down splitting in the tree learner is randomized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Practice: Random Forest and ExtraTrees in Scikit Learn (20 min)\n",
    "\n",
    "Scikit Learn implements both random forest and extra trees methods as part of the `ensemble` module.\n",
    "\n",
    "Have a look at the documentation here: <a href= http://scikit-learn.org/stable/modules/ensemble.html#forest> Hyperparameters of RF </a>\n",
    "\n",
    "> **Check:** What are the main Hyperparameters do you notice? \n",
    "\n",
    "\n",
    "Let's load the [car dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/), we are familiar with it by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('car.csv')\n",
    "print df.head()\n",
    "\n",
    "print df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn now:\n",
    "\n",
    "Initialize the following models and check their performance:\n",
    "- Bagging + Decision Trees\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "You can also create a function to speed up your work...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "With bagging and random forests we tran models on separate subsets and then combine their prediction. In a sense we are parallelizing the training and then combining (like a map-reduce...).\n",
    "\n",
    "\n",
    "\n",
    "_Boosting_ is a different ensembling technique that is sequential.\n",
    "\n",
    "Boosting is an iterative procedure that adaptively changes the sampling distribution of training records at each iteration, in order to correct the errors of the previous iteration of models. The first iteration uses uniform weights (like bagging) for all samples. In subsequent iterations, the weights are adjusted to emphasize records that were misclassified in previous iterations. The final prediction is constructed by a weighted vote (where the weights for a base classifier depends on its training error).\n",
    "\n",
    "Since the base classifier's focus more and more closely on records that are difficult to classify as the sequence of iterations progresses, they are faced with progressively more difficult learning problems.\n",
    "\n",
    "Boosting takes a base _weak_ learner and tries to make it a _strong_ learner by re-training it on the misclassified samples.\n",
    "\n",
    "\n",
    "\n",
    "There are several algorithms for boosting, in particular we will mention `AdaBoost`, `GradientBoostingClassifier` that are implemented in scikit learn.\n",
    "\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "`AdaBoost` refers to a particular method of training a boosted classifier. A boost classifier is a classifier in the for\n",
    "$$\n",
    "F_T(x) = \\sum_{t=1}^T f_t(x)\n",
    "$$\n",
    "where each $f_t$ is a weak learner that takes an object $x$ as input and returns a real valued result indicating the class of the object.\n",
    "<img src=https://www.analyticsvidhya.com/wp-content/uploads/2015/11/bigd.png>\n",
    "\n",
    "\n",
    "<img src=http://i.stack.imgur.com/5b2VM.png>\n",
    "\n",
    "\n",
    "Each weak learner produces an output, hypothesis $h(x_i)$, for each sample in the training set. At each iteration $t$, a weak learner is selected and assigned a coefficient \\alpha_t such that the sum training error E_t of the resulting t-stage boost classifier is minimized.\n",
    "\n",
    "\n",
    "\n",
    "Here $F_{t-1}(x)$ is the boosted classifier that has been built up to the previous stage of training, $E(F)$ is some error function and $f_t(x) = \\alpha_t h(x)$ is the weak learner that is being considered for addition to the final classifier.\n",
    "\n",
    "At each iteration of the training process, a weight is assigned to each sample in the training set equal to the current error $E(F_{t-1}(x_i))$ on that sample. These weights can be used to inform the training of the weak learner, for instance, decision trees can be grown that favor splitting sets of samples with high weights.\n",
    "\n",
    "$$\n",
    "E_t = \\sum_i E[F_{t-1}(x_i) + \\alpha_t h(x_i)]\n",
    "$$\n",
    "\n",
    "\n",
    "- Fit an additive model (ensemble) in a forward stage-wise manner.\n",
    "- In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Trees\n",
    "\n",
    "<a href=http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html> Gradient Boosted Trees </a>\n",
    "\n",
    "<a href=http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf> A Gentle Introduction to GBTs </a>\n",
    "\n",
    "Gradient Boosting is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology.\n",
    "\n",
    "The advantages of GBRT are:\n",
    "- Natural handling of data of mixed type (= heterogeneous features)\n",
    "- Predictive power\n",
    "- Robustness to outliers in output space (via robust loss functions)\n",
    "\n",
    "The disadvantages of GBRT are:\n",
    "- Scalability, due to the sequential nature of boosting it can hardly be parallelized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to XGBoost: Content from Jason Mastery ML\n",
    "\n",
    "<a href=https://s3.amazonaws.com/MLMastery/xgboost_with_python_mini_course.pdf?__s=mudfzqhho7h4jzuswfzy> Introduction to XGBoost </a>\n",
    "\n",
    "- **Parallelization** of tree construction using all of your CPU cores during training.\n",
    "- **Distributed Computing** for training very large models using a cluster of machines.\n",
    "- **Out-of-Core Computing** for very large datasets that donâ€™t fit into memory.\n",
    "- **Cache Optimization** of data structures and algorithms to make best use of hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "labels = LabelEncoder()\n",
    "label_encoded_y = labels.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model\n",
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = [50, 100, 150, 200]\n",
    "max_depth = [2, 4, 6, 8]\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 359 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 859 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 953 out of 960 | elapsed:   35.8s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:   36.1s finished\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold,\n",
    "verbose=1)\n",
    "\n",
    "result = grid_search.fit(X, label_encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.470744 using {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 2}\n",
      "-0.691621 (0.000103) with: {'n_estimators': 50, 'learning_rate': 0.0001, 'max_depth': 2}\n",
      "-0.690097 (0.000195) with: {'n_estimators': 100, 'learning_rate': 0.0001, 'max_depth': 2}\n",
      "-0.688588 (0.000276) with: {'n_estimators': 150, 'learning_rate': 0.0001, 'max_depth': 2}\n",
      "-0.687101 (0.000342) with: {'n_estimators': 200, 'learning_rate': 0.0001, 'max_depth': 2}\n",
      "-0.691340 (0.000146) with: {'n_estimators': 50, 'learning_rate': 0.0001, 'max_depth': 4}\n",
      "-0.689552 (0.000291) with: {'n_estimators': 100, 'learning_rate': 0.0001, 'max_depth': 4}\n",
      "-0.687774 (0.000428) with: {'n_estimators': 150, 'learning_rate': 0.0001, 'max_depth': 4}\n",
      "-0.686008 (0.000568) with: {'n_estimators': 200, 'learning_rate': 0.0001, 'max_depth': 4}\n",
      "-0.691188 (0.000208) with: {'n_estimators': 50, 'learning_rate': 0.0001, 'max_depth': 6}\n",
      "-0.689255 (0.000422) with: {'n_estimators': 100, 'learning_rate': 0.0001, 'max_depth': 6}\n",
      "-0.687360 (0.000641) with: {'n_estimators': 150, 'learning_rate': 0.0001, 'max_depth': 6}\n",
      "-0.685492 (0.000856) with: {'n_estimators': 200, 'learning_rate': 0.0001, 'max_depth': 6}\n",
      "-0.691226 (0.000283) with: {'n_estimators': 50, 'learning_rate': 0.0001, 'max_depth': 8}\n",
      "-0.689334 (0.000571) with: {'n_estimators': 100, 'learning_rate': 0.0001, 'max_depth': 8}\n",
      "-0.687477 (0.000870) with: {'n_estimators': 150, 'learning_rate': 0.0001, 'max_depth': 8}\n",
      "-0.685645 (0.001166) with: {'n_estimators': 200, 'learning_rate': 0.0001, 'max_depth': 8}\n",
      "-0.678506 (0.000762) with: {'n_estimators': 50, 'learning_rate': 0.001, 'max_depth': 2}\n",
      "-0.665185 (0.001432) with: {'n_estimators': 100, 'learning_rate': 0.001, 'max_depth': 2}\n",
      "-0.652950 (0.002437) with: {'n_estimators': 150, 'learning_rate': 0.001, 'max_depth': 2}\n",
      "-0.641760 (0.003542) with: {'n_estimators': 200, 'learning_rate': 0.001, 'max_depth': 2}\n",
      "-0.675693 (0.001315) with: {'n_estimators': 50, 'learning_rate': 0.001, 'max_depth': 4}\n",
      "-0.659756 (0.002624) with: {'n_estimators': 100, 'learning_rate': 0.001, 'max_depth': 4}\n",
      "-0.645424 (0.003920) with: {'n_estimators': 150, 'learning_rate': 0.001, 'max_depth': 4}\n",
      "-0.632473 (0.005200) with: {'n_estimators': 200, 'learning_rate': 0.001, 'max_depth': 4}\n",
      "-0.674836 (0.002057) with: {'n_estimators': 50, 'learning_rate': 0.001, 'max_depth': 6}\n",
      "-0.658742 (0.003802) with: {'n_estimators': 100, 'learning_rate': 0.001, 'max_depth': 6}\n",
      "-0.644154 (0.005566) with: {'n_estimators': 150, 'learning_rate': 0.001, 'max_depth': 6}\n",
      "-0.630922 (0.007186) with: {'n_estimators': 200, 'learning_rate': 0.001, 'max_depth': 6}\n",
      "-0.675181 (0.002832) with: {'n_estimators': 50, 'learning_rate': 0.001, 'max_depth': 8}\n",
      "-0.659492 (0.004977) with: {'n_estimators': 100, 'learning_rate': 0.001, 'max_depth': 8}\n",
      "-0.645298 (0.006967) with: {'n_estimators': 150, 'learning_rate': 0.001, 'max_depth': 8}\n",
      "-0.632545 (0.008595) with: {'n_estimators': 200, 'learning_rate': 0.001, 'max_depth': 8}\n",
      "-0.590697 (0.011585) with: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 2}\n",
      "-0.544303 (0.021095) with: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 2}\n",
      "-0.518271 (0.027707) with: {'n_estimators': 150, 'learning_rate': 0.01, 'max_depth': 2}\n",
      "-0.503596 (0.033391) with: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 2}\n",
      "-0.574397 (0.011789) with: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 4}\n",
      "-0.522552 (0.023655) with: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 4}\n",
      "-0.500683 (0.031509) with: {'n_estimators': 150, 'learning_rate': 0.01, 'max_depth': 4}\n",
      "-0.487145 (0.037051) with: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 4}\n",
      "-0.573450 (0.012016) with: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 6}\n",
      "-0.519968 (0.019349) with: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 6}\n",
      "-0.497642 (0.028456) with: {'n_estimators': 150, 'learning_rate': 0.01, 'max_depth': 6}\n",
      "-0.491285 (0.036890) with: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 6}\n",
      "-0.575907 (0.014331) with: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 8}\n",
      "-0.528467 (0.018768) with: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 8}\n",
      "-0.503114 (0.025218) with: {'n_estimators': 150, 'learning_rate': 0.01, 'max_depth': 8}\n",
      "-0.497191 (0.034463) with: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 8}\n",
      "-0.477111 (0.046143) with: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2}\n",
      "-0.470744 (0.053117) with: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 2}\n",
      "-0.472093 (0.059364) with: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 2}\n",
      "-0.482542 (0.062630) with: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2}\n",
      "-0.481295 (0.054197) with: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 4}\n",
      "-0.503012 (0.067436) with: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 4}\n",
      "-0.532853 (0.077897) with: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 4}\n",
      "-0.555373 (0.085270) with: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 4}\n",
      "-0.513898 (0.059020) with: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 6}\n",
      "-0.548517 (0.073543) with: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6}\n",
      "-0.588891 (0.091284) with: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 6}\n",
      "-0.623352 (0.105371) with: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 6}\n",
      "-0.539782 (0.066689) with: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 8}\n",
      "-0.592316 (0.089694) with: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 8}\n",
      "-0.639891 (0.108228) with: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 8}\n",
      "-0.678557 (0.120470) with: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 8}\n",
      "-0.473264 (0.057502) with: {'n_estimators': 50, 'learning_rate': 0.2, 'max_depth': 2}\n",
      "-0.487205 (0.069411) with: {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 2}\n",
      "-0.503738 (0.074608) with: {'n_estimators': 150, 'learning_rate': 0.2, 'max_depth': 2}\n",
      "-0.521757 (0.075180) with: {'n_estimators': 200, 'learning_rate': 0.2, 'max_depth': 2}\n",
      "-0.515005 (0.059033) with: {'n_estimators': 50, 'learning_rate': 0.2, 'max_depth': 4}\n",
      "-0.565794 (0.085318) with: {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 4}\n",
      "-0.617611 (0.101741) with: {'n_estimators': 150, 'learning_rate': 0.2, 'max_depth': 4}\n",
      "-0.667558 (0.111419) with: {'n_estimators': 200, 'learning_rate': 0.2, 'max_depth': 4}\n",
      "-0.564275 (0.086116) with: {'n_estimators': 50, 'learning_rate': 0.2, 'max_depth': 6}\n",
      "-0.648924 (0.114876) with: {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 6}\n",
      "-0.702267 (0.135573) with: {'n_estimators': 150, 'learning_rate': 0.2, 'max_depth': 6}\n",
      "-0.749088 (0.148642) with: {'n_estimators': 200, 'learning_rate': 0.2, 'max_depth': 6}\n",
      "-0.600560 (0.091675) with: {'n_estimators': 50, 'learning_rate': 0.2, 'max_depth': 8}\n",
      "-0.692094 (0.131025) with: {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 8}\n",
      "-0.742382 (0.146512) with: {'n_estimators': 150, 'learning_rate': 0.2, 'max_depth': 8}\n",
      "-0.779675 (0.159257) with: {'n_estimators': 200, 'learning_rate': 0.2, 'max_depth': 8}\n",
      "-0.482454 (0.063274) with: {'n_estimators': 50, 'learning_rate': 0.3, 'max_depth': 2}\n",
      "-0.511434 (0.072594) with: {'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 2}\n",
      "-0.541814 (0.081014) with: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 2}\n",
      "-0.565765 (0.079993) with: {'n_estimators': 200, 'learning_rate': 0.3, 'max_depth': 2}\n",
      "-0.545997 (0.084919) with: {'n_estimators': 50, 'learning_rate': 0.3, 'max_depth': 4}\n",
      "-0.619144 (0.106156) with: {'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 4}\n",
      "-0.684434 (0.125814) with: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 4}\n",
      "-0.747874 (0.139604) with: {'n_estimators': 200, 'learning_rate': 0.3, 'max_depth': 4}\n",
      "-0.619067 (0.093618) with: {'n_estimators': 50, 'learning_rate': 0.3, 'max_depth': 6}\n",
      "-0.722110 (0.122245) with: {'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 6}\n",
      "-0.787266 (0.145175) with: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 6}\n",
      "-0.832066 (0.163851) with: {'n_estimators': 200, 'learning_rate': 0.3, 'max_depth': 6}\n",
      "-0.650066 (0.112673) with: {'n_estimators': 50, 'learning_rate': 0.3, 'max_depth': 8}\n",
      "-0.736105 (0.139994) with: {'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 8}\n",
      "-0.790495 (0.156314) with: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 8}\n",
      "-0.826463 (0.164719) with: {'n_estimators': 200, 'learning_rate': 0.3, 'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Conclusion\n",
    "<a name=\"conclusion\"></a>\n",
    "\n",
    "In this class we learned about Random Forest, Extremely randomized trees and Boosting. They are different ways to improve the performance of a weak learner.\n",
    "\n",
    "Some of these methods will perform better in some cases, some better in other cases. For example, Decision Trees are more nimble and easier to communicate, but have a tendency to overfit. On the other hand Ensemble methods perform better in more complex scenarios, but may become very complicated and harder to explain.\n",
    "Have a look [here](https://www.wise.io/resources) for a couple of examples from real world startup Wise.io.\n",
    "\n",
    "> **Check:** Can you think of what could be limitations of these methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"more\"></a>\n",
    "### Additional Resources\n",
    "\n",
    "- **[Two Cultures of Statistics](https://projecteuclid.org/euclid.ss/1009213726)**\n",
    "- [Original Random Forest Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) \n",
    "- [Random Forest on wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [Quora question on Random Forest](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n",
    "- [Scikit Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Scikit Learn Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- <a href=https://s3.amazonaws.com/MLMastery/xgboost_with_python_mini_course.pdf?__s=mudfzqhho7h4jzuswfzy> Introduction to XGBoost </a>\n",
    "\n",
    "- <a href=http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html> Gradient Boosted Trees </a>\n",
    "\n",
    "- <a href=http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf> A Gentle Introduction to GBTs </a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
