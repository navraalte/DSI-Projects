{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px\">\n",
    "\n",
    "# Classifier Evaluation and the Confusion Matrix\n",
    "\n",
    "Week 4 | 4.2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam = pd.read_csv('~/DSI-SF-5/datasets/spam/spam_words_wide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spam</th>\n",
       "      <th>getzed</th>\n",
       "      <th>86021</th>\n",
       "      <th>babies</th>\n",
       "      <th>sunoco</th>\n",
       "      <th>ultimately</th>\n",
       "      <th>thk</th>\n",
       "      <th>voted</th>\n",
       "      <th>spatula</th>\n",
       "      <th>fiend</th>\n",
       "      <th>...</th>\n",
       "      <th>itna</th>\n",
       "      <th>borin</th>\n",
       "      <th>thoughts</th>\n",
       "      <th>iccha</th>\n",
       "      <th>videochat</th>\n",
       "      <th>freefone</th>\n",
       "      <th>pist</th>\n",
       "      <th>reformat</th>\n",
       "      <th>strict</th>\n",
       "      <th>69698</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_spam  getzed  86021  babies  sunoco  ultimately  thk  voted  spatula  \\\n",
       "0        0       0      0       0       0           0    0      0        0   \n",
       "1        0       0      0       0       0           0    0      0        0   \n",
       "2        1       0      0       0       0           0    0      0        0   \n",
       "3        0       0      0       0       0           0    0      0        0   \n",
       "4        0       0      0       0       0           0    0      0        0   \n",
       "\n",
       "   fiend  ...    itna  borin  thoughts  iccha  videochat  freefone  pist  \\\n",
       "0      0  ...       0      0         0      0          0         0     0   \n",
       "1      0  ...       0      0         0      0          0         0     0   \n",
       "2      0  ...       0      0         0      0          0         0     0   \n",
       "3      0  ...       0      0         0      0          0         0     0   \n",
       "4      0  ...       0      0         0      0          0         0     0   \n",
       "\n",
       "   reformat  strict  69698  \n",
       "0         0       0      0  \n",
       "1         0       0      0  \n",
       "2         0       0      0  \n",
       "3         0       0      0  \n",
       "4         0       0      0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 1001) 747\n"
     ]
    }
   ],
   "source": [
    "print spam.shape, spam.is_spam.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The Baseline Accuracy\n",
    "\n",
    "The concept of baseline accuracy is extremely important and too often forgotten when evaluating a model. \n",
    "\n",
    "> **Baseline Accuracy**: The accuracy that can be achieved by a model simply by guessing the majority class every time.\n",
    "\n",
    "We are trained to think of \"50% accuracy\" as guessing by chance. In fact, a 50% accuracy is only guessing by chance in a very specific context: when we have equal proportion of positive and negatvie (1 and 0) labels in our dataset, and when we are predicting between two classes.\n",
    "\n",
    "In reality, your dataset is unlikely to have balanced classes, and the more unbalanced it is the higher the baseline accuracy becomes. This is important to remember because if 99% of your observations are of one class, predicting 99% of them correctly with a model is performing at chance.\n",
    "\n",
    "You can calculate baseline accuracy as: \n",
    "\n",
    "**`baseline_accuracy = majority_class_N / total_population`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the baseline accuracy for the spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8659368269921034"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_acc = 1. - spam.is_spam.mean()\n",
    "baseline_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Set up a kNN model to predict spam\n",
    "\n",
    "It's up to you what predictors you want to use and how you want to parameterize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y = spam.is_spam.values\n",
    "X = spam.iloc[:, 1:100]\n",
    "\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validate the accuracy of the model\n",
    "\n",
    "Use 10 folds. How does the performace compare to the baseline accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8781362   0.88530466  0.88351254  0.87992832  0.8781362   0.87432675\n",
      "  0.88689408  0.87589928  0.88309353  0.88669065]\n",
      "0.881192220024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "accs = cross_val_score(knn, X, y, cv=10)\n",
    "print accs\n",
    "print np.mean(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Predicted labels and predicted probabilities\n",
    "\n",
    "Sklearn classification models typically have functions to predict the labels (classes) of observations as well as the _predicted probability_ of labels, which are the probabilities that they belong to a class.\n",
    "\n",
    "The `.predict()` function will return the predicted labels for a design matrix. The `.predict_proba()` function will return the probabilities of belonging to classes (classes are in columns in ascending order).\n",
    "\n",
    "Fit the knn model and print out the predicted labels and predicted probabilities for a few points below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(X.iloc[0:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(X.iloc[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "|   |Predicted Positive | Predicted Negative |   \n",
    "|---|---|---|\n",
    "|**Actual Positive** | True Positive (TP)  | False Negative (FN)  |  \n",
    "|**Actual Negative**  | False Positive (FP)  | True Negative (TN)  | \n",
    "\n",
    "In a binary classifier, the \"true\" class is labeled with 1 and the \"false\" class is labeled with 0. \n",
    "\n",
    "> **True Positive**: A positive class observation (1) is correctly classified as positive by the model.\n",
    "\n",
    "> **False Positive**: A negative class observation (0) is incorrectly classified as positive.\n",
    "\n",
    "> **True Negative**: A negative class observation is correctly classified as negative.\n",
    "\n",
    "> **False Negative**: A positive class observation is incorrectly classified as negative.\n",
    "\n",
    "Columns of the confusion matrix sum to the predictions by class. Rows of the matrix sum to the actual values within each class.\n",
    "\n",
    "As the name suggests, the labels can be confusing. Remember that the first word (True or False) indicates whether or not the guess is correct. The second word (Positive or Negative) indicates the label of the _guess_ (not the actual label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the confusion matrix metrics for your model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = knn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 18 4807 640\n"
     ]
    }
   ],
   "source": [
    "tp = np.sum((y == 1) & (predicted == 1))\n",
    "fp = np.sum((y == 0) & (predicted == 1))\n",
    "tn = np.sum((y == 0) & (predicted == 0))\n",
    "fn = np.sum((y == 1) & (predicted == 0))\n",
    "print tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify this is the same as the numbers you get from sklearn's `confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4807,   18],\n",
       "       [ 640,  107]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Type I error and p-values\n",
    "\n",
    "In the context of hypothesis testing false positives and false negatives are often referred to as Type I and Type II error, respectively. \n",
    "\n",
    "Type I error is the incorrect rejection of the null hypothesis when in fact the null hypothesis is true. This is equivalent to a false positive in classification, in which the model labels an observation as \"true\" when in fact it is \"false\". \n",
    "\n",
    "Type I error directly corresponds to the p-value: **the p-value is the probability of incorrectly rejecting the null hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Type II error and \"power\"\n",
    "\n",
    "Type II error on the other hand directly corresponds to false negatives. A Type II error in the context of hypothesis testing would be to accept the null hypothesis when in fact the alternative hypothesis is true. \n",
    "\n",
    "Whereas Type I error corresponds to the concept of _statistical significance_, Type II error corresponds to the idea of _statistical power._ The power of a test is:\n",
    "\n",
    "### $$ \\text{power} = 1 - P(\\text{Type II error}) $$\n",
    "\n",
    "More intuitively, **power measures our ability to detect an effect that is present.**\n",
    "\n",
    "We can visualize the ideas of significance, power, and error types in a matrix the same as our confusion matrix from above:\n",
    "\n",
    "|   |Accept $H_0$ | Reject $H_0$ |   \n",
    "|---|---|---|\n",
    "|**$H_0$ is True** | P(correct) <br> _(1 - alpha)_  | P(type I error) <br> _(alpha, significance)_  |  \n",
    "|**$H_0$ is False**  | P(type II error) <br> _(beta)_  | P(correct) <br> _(1 - beta, power)_ | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "The accuracy metric can be constructed using the components of the confusion matrix. With the total population as:\n",
    "\n",
    "**`total_population = tp + fp + tn + fn`**\n",
    "\n",
    "The accuracy can be calculated as:\n",
    "\n",
    "**`accuracy = (tp + tn) / total_population`**\n",
    "\n",
    "Which is just the proportion of correct guesses, regardless of class. The `.score()` function attached to sklearn classification model objects defaults to returning the accuracy of the model's predictions given an `X` and `y`.\n",
    "\n",
    "The inverse of the accuracy is known as the **misclassification rate**, which is calculated:\n",
    "\n",
    "**`misclassification_rate = (fp + fn) / total_population`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.881909547739\n",
      "0.881909547739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "total_population = tp + fp + tn + fn\n",
    "\n",
    "print accuracy_score(y, predicted)\n",
    "print float(tp + tn) / total_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sensitivity / Recall / True Positive Rate\n",
    "\n",
    "The true positive rate is the percent of times when the label is 1 the model actually predicted 1. This is alternatively known as the **Sensitivity** or **Recall**. \n",
    "\n",
    "This is calculated as:\n",
    "\n",
    "**`sensitivity = tp / (tp + fn)`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.143239625167\n",
      "0.143239625167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print recall_score(y, predicted)\n",
    "print float(tp) / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### False Positive Rate\n",
    "\n",
    "Alternatively, the false positive rate measures the fraction of times the model predicts a 1 when the target class is actually a 0. \n",
    "\n",
    "**`fpr = fp / (tn + fp)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00373056994819\n"
     ]
    }
   ],
   "source": [
    "print float(fp) / (tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Specificity, or the True Negative Rate\n",
    "\n",
    "The true negative rate measures the fraction of times the classifier predicted the class was 0 out of all the times the class was 0. It can be considered the sister metric to Sensitivity, which measures the same thing but for positives.\n",
    "\n",
    "**`specificity = tn / (tn + fp)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996269430052\n"
     ]
    }
   ],
   "source": [
    "specificity = float(tn) / (tn + fp)\n",
    "print specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Precision, or Positive Predictive Value\n",
    "\n",
    "The precision measures the fraction of times that the classifier guessed correctly when it was predicting the true (1) class.\n",
    "\n",
    "**`precision = tp / (tp + fp)`**\n",
    "\n",
    "The idea of the classifier being _precise_ is subtly different than _accurate_. Precision is a measure of correctness only for when the classifier guesses the positive class, regardless of how many times it actually \"tries\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n",
      "0.856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print precision_score(y, predicted)\n",
    "print float(tp) / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## F1-score and the `classification_report`\n",
    "\n",
    "sklearn contains a function `classification_report` in the `metrics` submodule that helps diagnose the effectiveness of your classifier. The report focuses on the precision, recall, and a metric known as the f1-score.\n",
    "\n",
    "The f1-score is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of the precision and recall metrics. Blending the two is useful: precision measures how effectively the classifier performs when it is predicting a 1, whereas recall measures how many of the total 1 classes out of all the 1-labeled observations were predicted correctly. \n",
    "\n",
    "### $$ F_1 = 2 \\cdot \\frac{1}{\\tfrac{1}{\\mathrm{recall}} + \\tfrac{1}{\\mathrm{precision}}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$$\n",
    "\n",
    "By combining the two we have a measure of the classifiers ability to find the positive labeled observations as well as how permissive it is for identification errors on those labels.\n",
    "\n",
    "You can print out the report of these three metrics on both of the classes (or more if you have a multi-class problem) using the `classification_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      4825\n",
      "          1       0.86      0.14      0.25       747\n",
      "\n",
      "avg / total       0.88      0.88      0.84      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Table of Common Classification Terms\n",
    "\n",
    "<br><br>\n",
    "\n",
    "|  TERM | DESCRIPTION  |\n",
    "|---|---|\n",
    "|**TRUE POSITIVES** | The number of \"true\" classes correctly predicted to be true by the model. <br><br> `TP = Sum of observations predicted to be 1 that are actually 1`<br><br>The true class in a binary classifier is labeled with 1.|\n",
    "|**TRUE NEGATIVES** | The number of \"false\" classes correctly predicted to be false by the model. <br><br> `TP = Sum of observations predicted to be 0 that are actually 0`<br><br>The false class in a binary classifier is labeled with 0.|\n",
    "|**FALSE POSITIVES** | The number of \"false\" classes incorrectly predicted to be true by the model. This is the measure of **Type I error**.<br><br> `TP = Sum of observations predicted to be 1 that are actually 0`<br><br>Remember that the \"true\" and \"false\" refer to the veracity of your guess, and the \"positive\" and \"negative\" component refer to the guessed label.|\n",
    "|**FALSE NEGATIVES** | The number of \"true\" classes incorrectly predicted to be false by the model. This is the measure of **Type II error.**<br><br> `TP = Sum of observations predicted to be 0 that are actually 1`<br><br>|\n",
    "|**TOTAL POPULATION** | In the context of the confusion matrix, the sum of the cells. <br><br> `total population = tp + tn + fp + fn`<br><br>|\n",
    "|**SUPPORT** | The marginal sum of rows in the confusion matrix, or in other words the total number of observations belonging to a class regardless of prediction. <br><br>|\n",
    "|**ACCURACY** | The number of correct predictions by the model out of the total number of observations. <br><br> `accuracy = (tp + tn) / total_population`<br><br>|\n",
    "|**PRECISION** | The ability of the classifier to avoid labeling a class as a member of another class. <br><br> `Precision = True Positives / (True Positives + False Positives)`<br><br>_A precision score of 1 indicates that the classifier never mistakenly classified the current class as another class.  precision score of 0 would mean that the classifier misclassified every instance of the current class_ |\n",
    "|**RECALL/SENSITIVITY**    | The ability of the classifier to correctly identify the current class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations of the class.  0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**SPECIFICITY** | Percent of times the classifier predicted 0 out of all the times the class was 0.<br><br> `specificity = tn / (tn + fp)`<br><br>|\n",
    "|**FALSE POSITIVE RATE** | Percent of times model predicts 1 when the class is 0.<br><br> `fpr = fp / (tn + fp)`<br><br>|\n",
    "|**F1-SCORE** | The harmonic mean of the precision and recall. The harmonic mean is used here rather than the more conventional arithmetic mean because the harmonic mean is more appropriate for averaging rates. <br><br>`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)` <br><br>_The f1-score's best value is 1 and worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once._ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
