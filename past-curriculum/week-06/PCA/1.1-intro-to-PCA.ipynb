{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "### Introduction to Principal Component Analysis\n",
    "\n",
    "Week **6 ** | Lesson **1.1 **\n",
    "\n",
    "---\n",
    "| TIMING  | TYPE  \n",
    "|:-:|---|---|\n",
    "| 25 min| [Review](#review) |\n",
    "| 10 min| [**Industry Example **](#hook) |\n",
    "| 45 min| [**Content **](#content) |\n",
    "| 20 min| [Conclusion](#conclusion) |\n",
    "| 5 min | [Additional Resources](#more)\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "   - Apply PCA for dimensionality reduction \n",
    "   - Understand the geometric intuition behind PCA \n",
    "\n",
    "---\n",
    "### Student Pre-Work \n",
    "\n",
    "*Before this lesson, you should already be able to:*\n",
    "   - Understand Matrix Multiplication \n",
    "   - Read through [PCA Explained Visually](http://setosa.io/ev/principal-component-analysis/ )\n",
    "   - Watch video on [Eigenvalues and Eigenvectors](https://www.youtube.com/watch?v=PFDu9oVAE-g&t=695s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "<a name=\"review\"></a>\n",
    "### Review: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already gone through the Student Pre-Work material, do so now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "PCA is the quintessential \"dimensionality reduction\" algorithm. \n",
    "\n",
    "Dimensionality reduction is the process of combining or collapsing your existing features (columns in X) into new features that not only retain the signal in the original data in fewer variables while ideally reducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is PCA?\n",
    "\n",
    "---\n",
    "\n",
    "PCA finds the linear combinations of your current predictor variables that create new \"principal components\". The principal components explain (in order) the maximum possible amount of variance in your predictors.\n",
    "\n",
    "A more natural way of thinking about PCA is that **it transforms the coordinate system so that the axes become the  most concise, informative descriptors of our data as a whole.**\n",
    "\n",
    "Your old axes are your original variables and your new axes are the principal components from PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Process of PCA \n",
    "\n",
    "---\n",
    "\n",
    "Say we have a matrix $X$ of predictor variables. PCA will give us the ability to transform our $X$ matrix into a new matrix $Z$. \n",
    "\n",
    "First we will derive a **weighting matrix** $W$ from the correlational/covariance structure of $X$ that allows us to perform the transformation.\n",
    "\n",
    "Each successive dimension (column) in $Z$ will be rank-ordered according to variance in it's values!\n",
    "\n",
    "**There are 4 assumptions that PCA makes:**\n",
    "1. Linearity: Our data does not hold nonlinear relationships.\n",
    "2. Large variances define importance: our dimensions are constructed to maximize remaining variance.\n",
    "3. Principal components are orthogonal: each component (columns of $Z$) is completely un-correlated with the others.\n",
    "4. Mean and Variance are appropriate statistics -- data is normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**EIGENVECTORS**\n",
    "\n",
    "An eigenvector specifies a direction through the original coordinate space. The eigenvector with the highest correspoding eigenvalue is the first principal component.\n",
    "\n",
    "---\n",
    "\n",
    "**EIGENVALUES**\n",
    "\n",
    "Eigenvalues indicate the amount of variance in the direction of it's corresponding eigenvector.\n",
    "\n",
    "---\n",
    "\n",
    "**Every eigenvector has a corresponding eigenvalue!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![PCA coord transform](./assets/images/pca_coordinate_transformation.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Principal Components\"\n",
    "\n",
    "---\n",
    "\n",
    "What is a principal component? **Principal components are the vectors that define the new coordinate system for your data.** Transforming your original data columns onto the principal component axes constructs new variables that are optimized to explain as much variance as possible and to be independent (uncorrelated).\n",
    "\n",
    "Creating these variables is a well-defined mathematical process, but in essence **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![setosa pc1](./assets/images/setosa_pc1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why would we want to do PCA?\n",
    "\n",
    "---\n",
    "\n",
    "- We can reduce the number of dimensions (remove bottom number of components) and lose the least possible amount of variance information in our data.\n",
    "- Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n",
    "- The directions of largest variance should have the highest Signal to Noise ratio.\n",
    "- Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent.\n",
    "\n",
    "---\n",
    "\n",
    "[Good paper on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "[Nice site on performing PCA](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manual PCA Codealong\n",
    "\n",
    "---\n",
    "\n",
    "**MANUAL PCA STEPS:**\n",
    "\n",
    "1. Standardize data: centering is required, but full normalization is nice for visuals later.\n",
    "2. Calculate eigenvectors and eigenvalues from correlation or covariance matrix.\n",
    "3. Sort eigenvalues and choose eigenvectors that correspond to the largest eigenvalues. The number you choose is up to you, but we will take 2 for the sake of visualization here.\n",
    "4. Construct the projection weighting matrix $W$ from the eigenvectors.\n",
    "5. Transform the original dataset $X$ with $W$ to obtain the new 2-dimensional transformed matrix $Z$.\n",
    "\n",
    "---\n",
    "\n",
    "**DATA**\n",
    "\n",
    "We are going to be using a simple 75-row, 4-column dataset with demographic information. It contains:\n",
    "\n",
    "    age (limited to 20-65)\n",
    "    income\n",
    "    health (a rating on a scale of 1-100, where 100 is the best health)\n",
    "    stress (a rating on a scale of 1-100, where 100 is the most stressed)\n",
    "    \n",
    "All of the variables are continuous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "demo = pd.read_csv('./assets/datasets/simple_demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic EDA\n",
    "\n",
    "Make a seaborn regplot for each of:\n",
    "\n",
    "1. age vs. income\n",
    "2. age vs. health\n",
    "3. age vs. stress\n",
    "\n",
    "Also make a pairplot of the entire dataset.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Subset and normalize\n",
    "\n",
    "Subset data to just:\n",
    "\n",
    "    income\n",
    "    health\n",
    "    stress\n",
    "\n",
    "We will be comparing the principal components to age specifically so we are leaving them out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take subset of features: demo[['health', 'income', 'stress']]\n",
    "\n",
    "# normalize the data by taking each feature and subtracting by its mean and dividing by the standard deviation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate correlation matrix\n",
    "\n",
    "We will be using the correlation matrix to calculate the eigenvectors and eigenvalues.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the eigenvalues and eigenvectors from the correlation matrix\n",
    "\n",
    "numpy has a convenient function to calculate this:\n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate and plot the explained variance\n",
    "\n",
    "A useful measure is the **explained variance**, which is calculated from the eigenvalues. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "### $$ ExpVar_i = \\bigg(\\frac{eigenvalue_i}{\\sum_j^n{eigenvalue_j}}\\bigg) * 100$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct the Projection Matrix $W$\n",
    "\n",
    "This is simply a matrix of our top 2 eigenvectors.\n",
    "\n",
    "The eigenvectors are concatenated as columns.\n",
    "\n",
    "1. Start by ordering the eigenvectors by their corresponding eigenvalues biggest to smallest.\n",
    "- Concatenate the eigenvectors together. `np.hstack()` is useful for this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Construct the Transformed 2D Matrix $Z$\n",
    "\n",
    "To do this, we take the dot product of our 3D demographic matrix $X$ with the projection matrix $W$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot Principal Component 1 vs 2\n",
    "\n",
    "PC1 is the first column in $Z$, and PC2 is the second.\n",
    "\n",
    "Notice how they are un-correlated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plot age vs principal component 1 with regplot\n",
    "\n",
    "Look how tight the relationship is. PC1 took the shared variance out of income, health, and stress, which are intuitively directly related to increasing age. \n",
    "\n",
    "This principal component, or more specifically the column weighting matrix $W$, is essentially **capturing the latent age variance embedded in these variables.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Concatenate PC1 and PC2 to the full demographic (4D) dataset, then melt it with PC1 and PC2 and index variables\n",
    "\n",
    "1. Re-normalize so that all 4 variables are on the same scale.\n",
    "2. Remember the pandas melt code:\n",
    "\n",
    "```python\n",
    "melted_df = pd.melt(df, id_vars=['PC1','PC2'])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use lmplot to check out PC1 vs all 4 variables\n",
    "\n",
    "Make the `col` keyword argument \"variable\" and the `hue` keyword argument \"variable\" as well, assuming that's what you called them in the melt command (those are the defaults).\n",
    "\n",
    "Make `col_wrap = 2` and `size = 7` or something to make it nice.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Use lmplot to do the same for PC2\n",
    "\n",
    "Notice how PC2 captures the variance of income, which was not captured well by PC1. This makes sense, as the variance each principal component captures has to be orthogonal to the other components.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Conclusion**\n",
    "<a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we covered how to: \n",
    "- Apply PCA for dimensionality reduction \n",
    "- Understand the geometric intuition behind PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"more\"></a>\n",
    "### Additional Resources\n",
    "\n",
    "[PCA 4 dummies](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "\n",
    "[Stackoverflow making sense of PCA](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n",
    "\n",
    "[PCA and spectral theorem](http://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit)\n",
    "\n",
    "[PCA in 3 steps: eigendecomposition and SVD](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)\n",
    "\n",
    "[Tutorial on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "[PCA math and examples](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
