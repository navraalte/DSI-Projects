{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "### Introduction to Natural Language Processing\n",
    "\n",
    "**Week 6 | Lesson 3.1**\n",
    "\n",
    "---\n",
    "| TIMING  | TYPE  \n",
    "|:-:|---|---|\n",
    "| 25 min| [Principal Component Analysis, Clustering](#review) |\n",
    "| 10 min| [Bloomberg, Google NMT](#hook) |\n",
    "| 45 min| [Natural Language Processing](#content) |\n",
    "| 20 min| [Conclusion](#conclusion) |\n",
    "| 5 min | [Additional Resources](#more)\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "\n",
    "- Understand how natural language processing is driving innovations industry \n",
    "- Create a hierarchy of tasks in natural language processing \n",
    "- Identify Parts of Speech using NLTK\n",
    "- Extract features from unstructured text using `scikit-learn` and vectorization techniques like _tf-idf_\n",
    "- Apply _ti-idf_ to vectorize documents and words \n",
    "\n",
    "\n",
    "---\n",
    "### Student Pre-Work \n",
    "\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Understand PCA \n",
    "- Familiarize yourself with [nltk.download()](http://www.nltk.org/data.html) in case you need to download additional corpuses\n",
    "- Recognize basic principles of English language syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"review\"></a>\n",
    "### Review: Principal Components Analysis \n",
    "\n",
    "<a href=http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html> Introduction to PCA </a>\n",
    "\n",
    "<img src=http://www.nlpca.org/fig-pca-principal-component-analysis-m.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"hook\"> </a>\n",
    "\n",
    "### Bloomberg's Vision for 2020\n",
    "\n",
    "<img src=https://cdn.theatlantic.com/assets/media/img/mt/2014/03/Terminal/lead_large.jpg?1430158145>\n",
    "\n",
    "---\n",
    "<center> **<a href=https://www.techatbloomberg.com/post-topic/data-science/> How is Bloomberg Using Natural Language Processing to power its products? </a> ** </center>\n",
    "\n",
    "\n",
    "** <center> Imagine Bloomberg in 2020. What will NLP and data scientists have accomplished? </center> ** \n",
    "\n",
    " <center> I think there will be more natural ways to communicate with the Bloomberg Professional Service (aka the Terminal) and engage with workflows. Instead of having to remember multiple commands, a customer might be able to query different functions simply by chatting or typing a single request for exactly what information they’re seeking. There might even be a spoken interface to the terminal! </center>\n",
    "\n",
    "--- \n",
    "\n",
    "### Google's Word Lens, Machine Translation System\n",
    "--- \n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/WordLens_Screenshot_French.jpg/360px-WordLens_Screenshot_French.jpg> \n",
    "<center> <a href=https://www.youtube.com/watch?v=h2OfQdYrHRs> Word Lens </a> </center>\n",
    "---\n",
    "<center>\n",
    "\n",
    "<a href=https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html> The Great AI Awakening </a>\n",
    "\n",
    "\n",
    "<img src=https://2.bp.blogspot.com/-AmBczBtfi3Q/WDSB0M3InDI/AAAAAAAABbQ/1U_51u5ynl4FK4L0KOEllfRCq0Oauzy5wCEw/s640/image00.png> \n",
    "\n",
    "</center>\n",
    "\n",
    "<a href=https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html> Zero Shot NMT </a>\n",
    "\n",
    "<a href=https://research.googleblog.com/2016/09/a-neural-network-for-machine.html>Google's Machine Translation System </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"content\"></a>\n",
    "\n",
    "# Common NLP Problems\n",
    "\n",
    "The types of problems you can solve with natural language processing are vast.\n",
    "\n",
    "> \"I am a janitor, I wash windows at Microsoft.\"\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| **Sentiment Analysis** | Is what is written positive or negative? | \n",
    "| **Named Entity Recognition** | Classify names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. |\n",
    "| **Summarization** | Boiling down large bodies of text to paraphrased versions |\n",
    "| **Topic Modeling** | What topics does a body of text belong to? (ie: Auto tagging of news articles) |\n",
    "| **Question answering** | Given a human-language question, determine its answer. |\n",
    "| **Word disambiguation** | Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet. |\n",
    "| **Machine dialog systems (chat bots)** | Building response systems that react contextually to human input (ie: me: Siri, cook me some bacon.  Siri:  How do you like your bacon? ) | \n",
    "| ** Natural Language Generation ** | Generating natural language from knowledg base \n",
    "\n",
    "\n",
    "## Group Question (10 minutes)\n",
    "Can you think of 2-3 real world applications or examples you have encountered?\n",
    "\n",
    "\n",
    "See Also:\n",
    "\n",
    "- [News Headline Anlaysis](http://nbviewer.jupyter.org/github/AYLIEN/headline_analysis/blob/06f1223012d285412a650c201a19a1c95859dca1/main-chunks.ipynb?utm_content=buffer5d40c&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)\n",
    "- [Sentiment + Robot Classification in Movies](http://nbviewer.jupyter.org/github/cojette/ClusteringRobotsinMovie/blob/master/Classification%20of%20Robots%20in%20Movies.ipynb)\n",
    "- [Text Summarization /w Gensim](http://nbviewer.jupyter.org/github/piskvorky/gensim/blob/develop/docs/notebooks/summarization_tutorial.ipynb)\n",
    "- [Sentiment Analysis Intro](http://nbviewer.jupyter.org/github/sgsinclair/alta/blob/master/ipynb/SentimentAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study: How would you classify an email from span or not?\n",
    "\n",
    "Suppose we are building a spam/ham classifier. Input are emails, output is a binary classification.\n",
    "\n",
    "Here's an example of an input email:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n",
      "\n",
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\"\"\"\n",
    "print spam\n",
    "print\n",
    "print ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Check:** Can you think of a simple heuristic rule to catch email like this?\n",
    "- Create a list of emails who might be asssociate with spam (blacklist and whitelist)\n",
    "- Create a list of words that appear in spam emails, e.g. money, buy now, limited time \n",
    "- Poor grammar, repetition of structure \n",
    "- Number of words in all caps \n",
    "- Number of emojis \n",
    "\n",
    "\n",
    "By defining a simple rule that parses the text we have performed one of the simplest feature extraction from text: _binary word counting_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Our Machines Smarter:  Bag of words, _n-grams_\n",
    "\n",
    "\n",
    "<img src=http://radimrehurek.com/data_science_python/plot_ML_flow_chart_11.png> \n",
    "---\n",
    "\n",
    "<img src = http://i.imgur.com/sWec30N.png>\n",
    "\n",
    "<img src = http://i.imgur.com/rcolBSq.png>\n",
    "---\n",
    "The bag-of-words model is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the **bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.**\n",
    "\n",
    "**n-grams**: n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application\n",
    "\n",
    "<img src=http://i.imgur.com/f43dQvX.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello,\\ni saw your contact information on linkedin. i have carefully read through your profile and you seem to have an outstanding personality. this is one major reason why i am in contact with you. my name is mr. valery grayfer chairman of the board of directors of pjsc \"lukoil\". i am 86 years old and i was diagnosed with cancer 2 years ago. i will be going in for an operation later this week. i decided to will/donate the sum of 8,750,000.00 euros(eight million seven hundred and fifty thousand euros only etc. etc.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 7, 'of': 4, 'and': 3, 'is': 2, 'etc.': 2, 'am': 2, 'an': 2, 'have': 2, 'in': 2, 'your': 2, 'to': 2, 'years': 2, 'with': 2, 'this': 2, 'contact': 2, 'the': 2, 'major': 1, 'old': 1, 'cancer': 1, 'outstanding': 1, 'seven': 1, 'decided': 1, 'through': 1, 'carefully': 1, 'euros(eight': 1, 'seem': 1, 'saw': 1, 'information': 1, 'for': 1, 'euros': 1, 'fifty': 1, '86': 1, 'sum': 1, '\"lukoil\".': 1, 'only': 1, 'pjsc': 1, 'mr.': 1, '2': 1, 'linkedin.': 1, 'will/donate': 1, 'you': 1, 'hundred': 1, 'was': 1, 'personality.': 1, 'chairman': 1, 'profile': 1, 'you.': 1, 'hello,': 1, 'ago.': 1, 'read': 1, 'going': 1, 'thousand': 1, 'million': 1, 'grayfer': 1, 'reason': 1, 'be': 1, 'one': 1, 'why': 1, 'on': 1, 'name': 1, 'week.': 1, '8,750,000.00': 1, 'later': 1, 'board': 1, 'operation': 1, 'will': 1, 'directors': 1, 'diagnosed': 1, 'valery': 1, 'my': 1})\n",
      "\n",
      "Counter({'to': 5, 'you': 4, 'of': 4, 'the': 3, 'and': 2, 'we': 2, 'scientist': 2, 'data': 2, 'i': 2, 'further': 2, 'this': 1, 'regards.': 1, 'find': 1, 'information': 1, 'am': 1, 'an': 1, 'at': 1, 'in': 1, 'our': 1, 'message': 1, 'pleased': 1, 'best': 1, 'if': 1, 'will': 1, 'would': 1, 'with': 1, 'interviews': 1, 'please': 1, 'writing': 1, 'application': 1, 'mr.': 1, 'location': 1, 'passed': 1, 'interview': 1, 'for': 1, 'john': 1, 'date,': 1, 'be': 1, 'hello,': 1, 'x.': 1, 'invite': 1, 'that': 1, 'any': 1, 'interview.': 1, 'regards': 1, 'let': 1, 'know': 1, 'hooli': 1, 'on-site': 1, 'me': 1, 'on': 1, 'your': 1, 'like': 1, 'assistance.': 1, 'attached': 1, 'senior': 1, 'inform': 1, 'smith.': 1, 'can': 1, 'time': 1, 'position': 1, 'first': 1, 'round': 1, 'are': 1})\n"
     ]
    }
   ],
   "source": [
    "## word counting \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "print Counter(spam.lower().split())\n",
    "print\n",
    "print Counter(ham.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Scikit Learn Count Vectorizer (10 min)\n",
    "\n",
    "Scikit-learn offers a `CountVectorizer` with many configurable options:\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit([spam])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
    "            dtype='numpy.int64', encoding=u'utf-8', input=u'content',\n",
    "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "            ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "            strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "            tokenizer=None, vocabulary=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "df  = pd.DataFrame(cvec.transform([spam]).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>of</th>\n",
    "      <th>and</th>\n",
    "      <th>your</th>\n",
    "      <th>contact</th>\n",
    "      <th>is</th>\n",
    "      <th>in</th>\n",
    "      <th>have</th>\n",
    "      <th>euros</th>\n",
    "      <th>the</th>\n",
    "      <th>this</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>4</td>\n",
    "      <td>3</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Note that there are several parameters to tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contact</th>\n",
       "      <th>years</th>\n",
       "      <th>euros</th>\n",
       "      <th>million</th>\n",
       "      <th>outstanding personality</th>\n",
       "      <th>outstanding</th>\n",
       "      <th>operation later</th>\n",
       "      <th>operation</th>\n",
       "      <th>old diagnosed</th>\n",
       "      <th>old</th>\n",
       "      <th>mr valery</th>\n",
       "      <th>mr</th>\n",
       "      <th>million seven</th>\n",
       "      <th>00</th>\n",
       "      <th>personality</th>\n",
       "      <th>major</th>\n",
       "      <th>lukoil 86</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>linkedin carefully</th>\n",
       "      <th>linkedin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contact  years  euros  million  outstanding personality  outstanding  \\\n",
       "0        2      2      2        1                        1            1   \n",
       "\n",
       "   operation later  operation  old diagnosed  old  mr valery  mr  \\\n",
       "0                1          1              1    1          1   1   \n",
       "\n",
       "   million seven  00  personality  major  lukoil 86  lukoil  \\\n",
       "0              1   1            1      1          1       1   \n",
       "\n",
       "   linkedin carefully  linkedin  \n",
       "0                   1         1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## max_df, min_df, ngram_range are the main hyperparameters \n",
    "count_words = CountVectorizer(stop_words='english', ngram_range=(1, 2), max_df=2, min_df=1)\n",
    "count_words.fit([spam, ham])\n",
    "#print count_words.vocabulary_ # what is the vocbulary that is informing the features for us \n",
    "\n",
    "df_spam = pd.DataFrame(count_words.transform([spam]).todense(), columns=count_words.get_feature_names())\n",
    "df_spam.transpose().sort_values(0, ascending=False).head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scientist</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>data</th>\n",
       "      <th>interview</th>\n",
       "      <th>regards</th>\n",
       "      <th>let know</th>\n",
       "      <th>pleased inform</th>\n",
       "      <th>hello writing</th>\n",
       "      <th>like invite</th>\n",
       "      <th>hello</th>\n",
       "      <th>position</th>\n",
       "      <th>regards application</th>\n",
       "      <th>hooli pleased</th>\n",
       "      <th>round</th>\n",
       "      <th>round interviews</th>\n",
       "      <th>location</th>\n",
       "      <th>location interview</th>\n",
       "      <th>hooli</th>\n",
       "      <th>inform</th>\n",
       "      <th>scientist mr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scientist  data scientist  data  interview  regards  let know  \\\n",
       "0          2               2     2          2        2         1   \n",
       "\n",
       "   pleased inform  hello writing  like invite  hello  position  \\\n",
       "0               1              1            1      1         1   \n",
       "\n",
       "   regards application  hooli pleased  round  round interviews  location  \\\n",
       "0                    1              1      1                 1         1   \n",
       "\n",
       "   location interview  hooli  inform  scientist mr  \n",
       "0                   1      1       1             1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ham  = pd.DataFrame(count_words.transform([ham]).todense(), columns=count_words.get_feature_names())\n",
    "df_ham.transpose().sort_values(0, ascending=False).head(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Hyperprameters does CountVectorizer take?\n",
    "\n",
    "Share a few takeaways from the documentation in groups. What features stand out to you?\n",
    "\n",
    "[Count Vectorizer Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We should at least notice that you can set:\n",
    "    - max_df\n",
    "    - min_df\n",
    "    - max_features\n",
    "   \n",
    "max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    "* max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "* max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "* The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "min_df is used for removing terms that appear too infrequently. For example:\n",
    "\n",
    "* min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "* min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "* The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a hash function?\n",
    "![](https://i.ytimg.com/vi/bs7Wq0Z1uYk/maxresdefault.jpg)\n",
    "\n",
    "A hash value (or simply hash), also called a message digest, is a number generated from a string of text. The hash is substantially smaller than the text itself, and is generated by a formula in such a way that it is extremely unlikely that some other text will produce the same hash value.\n",
    "\n",
    "<a name=\"guided_practice\"></a>\n",
    "## Scikit-Learn Hashing Vectorizer (10 min)\n",
    "\n",
    "### Hashing Vectorizer\n",
    "\n",
    "As you have seen we can set the `CountVectorizer` dictionary to have a fixed size, only keeping words of certain frequencies, however, we still have to compute a dictionary and hold the dictionary in memory. This could be a problem when we have a large corpus or in streaming applications where we don't know which words we will encounter in the future.\n",
    "\n",
    "These problems can be solved using the `HashingVectorizer`, which converts a collection of text documents to a matrix of occurrences, calculated with the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing). Each word is mapped to a feature with the use of a [hash function](https://en.wikipedia.org/wiki/Hash_function) that converts it to a hash. If we encounter that word again in the text, it will be converted to the same hash, allowing us to count word occurence without retaining a dictionary in memory. This is very convenient!\n",
    "\n",
    "The main drawback of the this trick is that it's not possible to compute the inverse transform, and thus we lose information on what words the important features correspond to. The hash function employed is the signed 32-bit version of Murmurhash3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> \n",
    ">\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    hvec = HashingVectorizer()\n",
    "    hvec.fit([spam])\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "#\n",
    "# .todense() is just returning it as an array.. toarray() would be a nicer name for this in sklearn\n",
    "#####\n",
    "df  = pd.DataFrame(hvec.transform([spam]).todense())  \n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>479532</th>\n",
    "      <th>144749</th>\n",
    "      <th>174171</th>\n",
    "      <th>832412</th>\n",
    "      <th>828689</th>\n",
    "      <th>994433</th>\n",
    "      <th>1005907</th>\n",
    "      <th>170062</th>\n",
    "      <th>675997</th>\n",
    "      <th>959146</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>0.338062</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.169031</td>\n",
    "      <td>0.084515</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hvec = HashingVectorizer(stop_words='english')\n",
    "hvec.fit([spam])\n",
    "hvec.transform([spam]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Check:** What characteristics should text feature extraction from text satisfy?\n",
    "\n",
    "> **Check **: What are the advantages and disadvantages of the approaches described? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _tf-idf_ Vectorization Scheme ### \n",
    "\n",
    "<a name=\"guided-practice_2\"></a>\n",
    "## Term frequency - Inverse document Frequency (10 mins-ish)\n",
    "\n",
    "Another vectorization scheme is tf-idf score. This tells us which words are most discriminating between documents. Words that occur a lot in one document but doesn't occur in many documents will tell you something special about the document.\n",
    "\n",
    "- This weight is a statistical measure used to evaluate how important a word is to a document in a collection (aka corpus)\n",
    "- The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "Let's see how it is calculated.\n",
    "\n",
    "Term frequency tf is the frequency of a certain term in a document:\n",
    "\n",
    "<img src=http://i.imgur.com/K8UrQTr.png>\n",
    "\n",
    "Inverse document frequency is a measure of how much information gain is associated with the word. \n",
    "\n",
    "<img src=http://i.imgur.com/uwZUWA4.png> \n",
    "\n",
    "Term frequency - Inverse Document Frequency is calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n",
    "\n",
    "\n",
    "<Br><br>\n",
    "![](https://snag.gy/rBNLtd.jpg)\n",
    "\n",
    "This enhances terms that are highly specific of a particular document, while suppressing terms that are common to most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>euros</th>\n",
       "      <th>contact</th>\n",
       "      <th>personality</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>major</th>\n",
       "      <th>million</th>\n",
       "      <th>old</th>\n",
       "      <th>operation</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>location</th>\n",
       "      <th>let</th>\n",
       "      <th>know</th>\n",
       "      <th>john</th>\n",
       "      <th>invite</th>\n",
       "      <th>interviews</th>\n",
       "      <th>interview</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.155195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         years     euros   contact  personality  linkedin    lukoil     major  \\\n",
       "spam  0.290133  0.290133  0.290133     0.145067  0.145067  0.145067  0.145067   \n",
       "ham   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       million       old  operation    ...         date   message  location  \\\n",
       "spam  0.145067  0.145067   0.145067    ...     0.000000  0.000000  0.000000   \n",
       "ham   0.000000  0.000000   0.000000    ...     0.155195  0.155195  0.155195   \n",
       "\n",
       "           let      know      john    invite  interviews  interview      like  \n",
       "spam  0.000000  0.000000  0.000000  0.000000    0.000000    0.00000  0.000000  \n",
       "ham   0.155195  0.155195  0.155195  0.155195    0.155195    0.31039  0.155195  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(stop_words='english', min_df=1, max_df=2)\n",
    "tvec.fit([spam, ham])\n",
    "\n",
    "\n",
    "df  = pd.DataFrame(tvec.transform([spam, ham]).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "\n",
    "df.sort_values('spam', axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regards</th>\n",
       "      <th>interview</th>\n",
       "      <th>data</th>\n",
       "      <th>scientist</th>\n",
       "      <th>location</th>\n",
       "      <th>position</th>\n",
       "      <th>let</th>\n",
       "      <th>know</th>\n",
       "      <th>senior</th>\n",
       "      <th>invite</th>\n",
       "      <th>...</th>\n",
       "      <th>euros</th>\n",
       "      <th>going</th>\n",
       "      <th>grayfer</th>\n",
       "      <th>later</th>\n",
       "      <th>000</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>major</th>\n",
       "      <th>million</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.290133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      regards  interview     data  scientist  location  position       let  \\\n",
       "spam  0.00000    0.00000  0.00000    0.00000  0.000000  0.000000  0.000000   \n",
       "ham   0.31039    0.31039  0.31039    0.31039  0.155195  0.155195  0.155195   \n",
       "\n",
       "          know    senior    invite    ...        euros     going   grayfer  \\\n",
       "spam  0.000000  0.000000  0.000000    ...     0.290133  0.145067  0.145067   \n",
       "ham   0.155195  0.155195  0.155195    ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "         later       000  linkedin    lukoil     major   million     years  \n",
       "spam  0.145067  0.145067  0.145067  0.145067  0.145067  0.145067  0.290133  \n",
       "ham   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('ham', axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to NLP with `nltk` \n",
    "\n",
    "Bag of word approaches like the one outlined before completely ignores the structure of a sentence. BOW's merely assess presence of specific words or word combinations.\n",
    "\n",
    "Besides, the same word can have multiple meanings in different contexts. Consider for example the following two sentences:\n",
    "\n",
    "- There's wood floating in the **sea**\n",
    "- Mike's in a **sea** of trouble with the move\n",
    "\n",
    "In the first case the word \"sea\" indicates a large body of water, while in the second case it indicates \"a lot of\".\n",
    "\n",
    "How do we teach a computer to disambiguate? Here are some additional techniques that may come to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation\n",
    "\n",
    "_Segmentation_ is a technique to **identify sentences** within a body of text. Language is not a continuous uninterrupted stream of words: punctuation serves as a guide to group together words that convey meaning when contiguous.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def simple_sentencer(text):\n",
    "    '''take a string called `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "    \n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "simple_sentencer(easy_text)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "`# Result:`\n",
    "\n",
    "    ['I went to the zoo today.',\n",
    "     ' What do you think of that?',\n",
    "     ' I bet you hate it!']\n",
    "\n",
    "The sentencer above doesn't work perfectly. In the lab you will learn how to improve it. On the other hand, the NLTK library offers an easy to use sentencer.\n",
    "\n",
    "### There's an easier way to do the same thing!\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sent_detector = PunktSentenceTokenizer()\n",
    "sent_detector.sentences_from_text(easy_text)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ['I went to the zoo today.',\n",
    "     'What do you think of that?',\n",
    "     'I bet you hate it!',\n",
    "     \"Or maybe you don't\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo_2\"></a>\n",
    "## Demo: Advanced NLP with NLTK (15 mins)\n",
    "\n",
    "_Normalization_ is when slightly different version of a word exist. For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\n",
    "\n",
    "### What are other common cases of text that could need normalization?\n",
    "\n",
    "- Person titles (Mr. MR. DR etc.)\n",
    "- Dates (10/03, March 10 etc.)\n",
    "- Numbers\n",
    "- Plurals\n",
    "- Verb conjugations\n",
    "- Slang\n",
    "- SMS abbreviations\n",
    "\n",
    "## Stemming\n",
    "\n",
    "It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called _Stemming_.\n",
    "\n",
    "- Science, Scientist => Scien\n",
    "- Swimming, Swimmer, Swim => Swim\n",
    "\n",
    "As we did above we could define a Stemmer based on rules:\n",
    "\n",
    "\n",
    "```python\n",
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "    \n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-2])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag\n",
    "\n",
    "stem(['Science', 'Scientist'])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ['Scien', 'Scien']\n",
    "    \n",
    "Luckily for us, NLTK contains several robust stemmers.\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('Swimmed')\n",
    "print stemmer.stem('Swimming')\n",
    "```\n",
    "\n",
    "    Swim\n",
    "    Swim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 mins) Reading Exercise\n",
    "\n",
    "There are other stemmers available in NLTK. Let's take a look at [this article](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html).  \n",
    "\n",
    "You have 5 minutes to read. What did you learn?\n",
    "\n",
    "> notes:\n",
    "> First Stemmer is Lovins in 1968\n",
    "> Porter Stemmer written in 1980 and became de-facto in 2000, many versions, some buggy\n",
    "> Snowball Stemmer is a Framework to build stemmers, written by Porter too\n",
    "> See [here](http://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg) for more info on stemmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Some words are very common and provide no information on the text content.\n",
    "\n",
    "#### Can you give some examples?\n",
    "\n",
    "\n",
    "We should remove these _stop words_. Note that each language has different stop words.\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "sentence = \"this is a foo bar sentence\"\n",
    "print [i for i in sentence.split() if i not in stop]\n",
    "```\n",
    "\n",
    "    ['foo', 'bar', 'sentence']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech\n",
    "\n",
    "Each word has a specific role in a sentence (Verb, Noun etc.) Parts-of-speech tagging (POS) is a feature extraction technique that attaches a tag to each word in the sentence, in order to provide a more precise context for further analysis. This is often a resource intensive process, but it can sometimes improve the accuracy or our models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pos_tag(tok.tokenize(\"today is a great day to learn nlp\"))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    [('today', 'NN'),\n",
    "     ('is', 'VBZ'),\n",
    "     ('a', 'DT'),\n",
    "     ('great', 'JJ'),\n",
    "     ('day', 'NN'),\n",
    "     ('to', 'TO'),\n",
    "     ('learn', 'VB'),\n",
    "     ('nlp', 'NN')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Conclusion\n",
    "<a name=\"conclusion\"></a>\n",
    "\n",
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 mins)\n",
    "\n",
    "In this lesson we learned about Natural Language Processing and about two very powerful toolkits:\n",
    "- Scikit Learn Feature Extraction Text\n",
    "- Natural Language Tool Kit\n",
    "\n",
    "\n",
    "> **Check:** Discussion: what are some real world applications of these techniques?\n",
    "\n",
    "- Spam Detection for one\n",
    "- Preprocessing for larger NLP problems\n",
    "- Job market analysis\n",
    "- Is someone date-able or not? \"I\" in relation to signifier\n",
    "- Crude topic analyis\n",
    "- Build a keyword extractation heuristic and pipe it into a marketing analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"more\"></a>\n",
    "### Additional Resources\n",
    "\n",
    "\n",
    "- [Count Vectorizer Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [Choosing a Stemmer](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html)\n",
    "- [Feature Hashing](https://en.wikipedia.org/wiki/Feature_hashing)\n",
    "- [Term Frequency Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- [TFIDF Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
