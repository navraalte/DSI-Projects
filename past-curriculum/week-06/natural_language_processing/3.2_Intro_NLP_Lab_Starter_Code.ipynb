{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Lab\n",
    "\n",
    "In this lab we will further explore Scikit's and NLTK's capabilities to process text. We will use the 20 Newsgroup dataset, which is provided by Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "> sklearn.datasets.base.Bunch\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "> Dict\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?\n",
    "> A blurb of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'DESCR', 'filenames', 'target_names', 'data', 'target']\n"
     ]
    }
   ],
   "source": [
    "print data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_train) ## fancier dictonary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "> Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_chars = [\"\\n\", \"\\t\", \"_\"] \n",
    "\n",
    "def remove_bad_chars(document):\n",
    "    for char in bad_chars: \n",
    "        document = document.replace(char, '')\n",
    "    return document \n",
    "\n",
    "clean_data = lambda data_dict: map(remove_bad_chars, data_dict['data'])\n",
    "clean_data_train = clean_data(data_train)\n",
    "clean_data_test = clean_data(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n"
     ]
    }
   ],
   "source": [
    "print data_train['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=0.005,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "doc2vec = CountVectorizer(ngram_range=(1, 2), stop_words='english', min_df=0.005, max_df=0.95)\n",
    "doc2vec.fit(clean_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min DF:0.0001, Max DF: 0.75, Vocabulary Size:205421\n",
      "Min DF:0.001, Max DF: 0.75, Vocabulary Size:11235\n",
      "Min DF:0.005, Max DF: 0.75, Vocabulary Size:2514\n",
      "Min DF:0.01, Max DF: 0.75, Vocabulary Size:1176\n",
      "Min DF:0.05, Max DF: 0.75, Vocabulary Size:95\n",
      "Min DF:0.1, Max DF: 0.75, Vocabulary Size:15\n",
      "Min DF:0.0001, Max DF: 0.8, Vocabulary Size:205421\n",
      "Min DF:0.001, Max DF: 0.8, Vocabulary Size:11235\n",
      "Min DF:0.005, Max DF: 0.8, Vocabulary Size:2514\n",
      "Min DF:0.01, Max DF: 0.8, Vocabulary Size:1176\n",
      "Min DF:0.05, Max DF: 0.8, Vocabulary Size:95\n",
      "Min DF:0.1, Max DF: 0.8, Vocabulary Size:15\n",
      "Min DF:0.0001, Max DF: 0.85, Vocabulary Size:205421\n",
      "Min DF:0.001, Max DF: 0.85, Vocabulary Size:11235\n",
      "Min DF:0.005, Max DF: 0.85, Vocabulary Size:2514\n",
      "Min DF:0.01, Max DF: 0.85, Vocabulary Size:1176\n",
      "Min DF:0.05, Max DF: 0.85, Vocabulary Size:95\n",
      "Min DF:0.1, Max DF: 0.85, Vocabulary Size:15\n",
      "Min DF:0.0001, Max DF: 0.9, Vocabulary Size:205421\n",
      "Min DF:0.001, Max DF: 0.9, Vocabulary Size:11235\n",
      "Min DF:0.005, Max DF: 0.9, Vocabulary Size:2514\n",
      "Min DF:0.01, Max DF: 0.9, Vocabulary Size:1176\n",
      "Min DF:0.05, Max DF: 0.9, Vocabulary Size:95\n",
      "Min DF:0.1, Max DF: 0.9, Vocabulary Size:15\n",
      "Min DF:0.0001, Max DF: 0.95, Vocabulary Size:205421\n",
      "Min DF:0.001, Max DF: 0.95, Vocabulary Size:11235\n",
      "Min DF:0.005, Max DF: 0.95, Vocabulary Size:2514\n",
      "Min DF:0.01, Max DF: 0.95, Vocabulary Size:1176\n",
      "Min DF:0.05, Max DF: 0.95, Vocabulary Size:95\n",
      "Min DF:0.1, Max DF: 0.95, Vocabulary Size:15\n"
     ]
    }
   ],
   "source": [
    "for max_freq in [0.75, 0.8, 0.85, 0.9, .95]:\n",
    "    for min_freq in [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1]:\n",
    "        doc2vec_tmp = CountVectorizer(ngram_range=(1, 2), stop_words='english', min_df=min_freq, max_df=max_freq)\n",
    "        doc2vec_tmp.fit(clean_data_train)\n",
    "        print \"Min DF:{}, Max DF: {}, Vocabulary Size:{}\".format(min_freq, max_freq, len(doc2vec_tmp.vocabulary_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc2vec.transform(clean_data_test + clean_data_train).todense(), columns=doc2vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space            1355\n",
       "don              1213\n",
       "god              1207\n",
       "like             1137\n",
       "image            1135\n",
       "people           1121\n",
       "just             1052\n",
       "know              978\n",
       "does              953\n",
       "edu               950\n",
       "think             914\n",
       "time              857\n",
       "graphics          828\n",
       "use               773\n",
       "jpeg              725\n",
       "data              720\n",
       "good              695\n",
       "say               669\n",
       "way               638\n",
       "file              622\n",
       "available         605\n",
       "jesus             593\n",
       "program           587\n",
       "images            576\n",
       "make              567\n",
       "new               565\n",
       "point             548\n",
       "software          528\n",
       "nasa              527\n",
       "believe           522\n",
       "                 ... \n",
       "apologize          16\n",
       "los                16\n",
       "93 04              16\n",
       "accepting          16\n",
       "successfully       16\n",
       "exception          15\n",
       "honor              15\n",
       "extract            15\n",
       "01 14              15\n",
       "04 01              15\n",
       "fellow             15\n",
       "ordinary           15\n",
       "attempting         15\n",
       "theworld           15\n",
       "managed            15\n",
       "typically          15\n",
       "modified date      15\n",
       "bearing            15\n",
       "deep space         15\n",
       "date 93            15\n",
       "14 39              14\n",
       "handling           14\n",
       "proud              14\n",
       "fora               14\n",
       "time time          14\n",
       "relates            14\n",
       "30 days            14\n",
       "opposition         14\n",
       "condition          14\n",
       "survival           13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>01 14</th>\n",
       "      <th>04</th>\n",
       "      <th>04 01</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>years ago</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  01  01 14  04  04 01  10  100  1000  11 ...   yeah  year  years  \\\n",
       "0   0    0   0      0   0      0   0    0     0   0 ...      0     0      0   \n",
       "1   0    0   0      0   0      0   0    0     0   0 ...      0     0      0   \n",
       "2   0    0   0      0   0      0   0    0     0   0 ...      0     0      0   \n",
       "3   0    0   0      0   0      0   0    0     0   0 ...      0     0      0   \n",
       "4   0    0   0      0   0      0   0    0     0   0 ...      0     0      0   \n",
       "\n",
       "   years ago  yes  yesterday  york  young  zero  zip  \n",
       "0          0    0          0     0      0     0    0  \n",
       "1          0    0          0     0      0     0    0  \n",
       "2          0    0          0     0      0     0    0  \n",
       "3          0    0          0     0      0     0    0  \n",
       "4          0    0          0     0      0     0    0  \n",
       "\n",
       "[5 rows x 2514 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['target'] = np.hstack((data_train.target, data_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_target_sums = df.groupby('target').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alt.atheism :\n",
      "god          356\n",
      "space        295\n",
      "don          239\n",
      "image        235\n",
      "think        232\n",
      "like         232\n",
      "jpeg         230\n",
      "people       213\n",
      "does         210\n",
      "know         203\n",
      "just         202\n",
      "data         193\n",
      "use          170\n",
      "time         168\n",
      "jesus        168\n",
      "nasa         163\n",
      "available    162\n",
      "say          155\n",
      "lord         154\n",
      "edu          151\n",
      "Name: 0, dtype: int64\n",
      "---\n",
      "1 comp.graphics :\n",
      "space        463\n",
      "edu          360\n",
      "image        350\n",
      "don          350\n",
      "graphics     344\n",
      "people       327\n",
      "like         320\n",
      "god          307\n",
      "just         299\n",
      "know         278\n",
      "time         277\n",
      "does         249\n",
      "data         244\n",
      "jpeg         242\n",
      "good         240\n",
      "use          239\n",
      "available    237\n",
      "file         230\n",
      "think        228\n",
      "ftp          198\n",
      "Name: 1, dtype: int64\n",
      "---\n",
      "2 sci.space :\n",
      "image     457\n",
      "don       401\n",
      "like      363\n",
      "god       351\n",
      "people    345\n",
      "does      326\n",
      "just      321\n",
      "know      296\n",
      "space     296\n",
      "think     263\n",
      "edu       237\n",
      "time      234\n",
      "jpeg      231\n",
      "use       229\n",
      "way       205\n",
      "say       194\n",
      "earth     181\n",
      "file      179\n",
      "images    179\n",
      "make      179\n",
      "Name: 2, dtype: int64\n",
      "---\n",
      "3 talk.religion.misc :\n",
      "space       301\n",
      "people      236\n",
      "just        230\n",
      "don         223\n",
      "like        222\n",
      "jesus       205\n",
      "edu         202\n",
      "know        201\n",
      "god         193\n",
      "think       191\n",
      "graphics    178\n",
      "time        178\n",
      "does        168\n",
      "say         146\n",
      "launch      143\n",
      "good        137\n",
      "use         135\n",
      "said        129\n",
      "way         125\n",
      "pub         121\n",
      "Name: 3, dtype: int64\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for label in df_target_sums.index:\n",
    "    print label, data_train.target_names[label], \":\"\n",
    "    print df_target_sums.loc[label].sort_values(ascending=False)[:20]\n",
    "    print '---'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def fit_logit(data, min_df, ngram_range=(1, 2)):\n",
    "    ## fit count vectorizer\n",
    "    doc2vec = CountVectorizer(ngram_range=ngram_range, stop_words='english', min_df=min_df, max_df=0.95)\n",
    "    docs_train, target_train, docs_test, target_test = data\n",
    "    X_train = doc2vec.fit_transform(docs_train)\n",
    "    X_test = doc2vec.transform(docs_test)\n",
    "    \n",
    "    ## fit the logistic regression with ridge model \n",
    "    logit = LogisticRegressionCV()\n",
    "    logit.fit(X_train, target_train)\n",
    "    \n",
    "    ## print metrics \n",
    "    test_predictions = logit.predict(X_test)\n",
    "    print classification_report(target_test, test_predictions, target_names = data_train.target_names)\n",
    "    print logit.score(X_test, target_test)\n",
    "    return logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## input data \n",
    "\n",
    "data = clean_data_train, data_train.target, clean_data_test, data_test.target\n",
    "min_freqs = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "ngram_ranges = [(1, 1), (1, 2), (1, 3)]\n",
    "for ngram_range in ngram_ranges:\n",
    "    for freq in min_freqs:\n",
    "        print 'Minimum freq hp:{}, ngram range: {}'.format(freq, ngram_range)\n",
    "        fit_logit(data, freq, ngram_range=ngram_range)\n",
    "# Best Hyperparameters \n",
    "#         Minimum freq hp:0.0001, ngram range: (1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.62      0.56      0.59       319\n",
      "     comp.graphics       0.84      0.86      0.85       389\n",
      "         sci.space       0.74      0.83      0.79       394\n",
      "talk.religion.misc       0.59      0.53      0.55       251\n",
      "\n",
      "       avg / total       0.71      0.72      0.72      1353\n",
      "\n",
      "0.720620842572\n"
     ]
    }
   ],
   "source": [
    "best_logit_count_vect = fit_logit(data, 0.001, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashing and TF-IDF\n",
    "\n",
    "Let's see if Hashing or TF-IDF improves the accuracy.\n",
    "\n",
    "1. Initialize a HashingVectorizer and repeat the test with no restriction on the number of features\n",
    "- does the score improve with respect to the count vectorizer?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model\n",
    "- Initialize a TF-IDF Vectorizer and repeat the analysis above\n",
    "- can you improve on your best score above?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def fit_logit(data, min_df, ngram_range=(1, 2), vectorizer_type='tfidf'):\n",
    "    ## fit count vectorizer\n",
    "    docs_train, target_train, docs_test, target_test = data\n",
    "    if vectorizer_type == 'tfidf':\n",
    "        doc2vec = TfidfVectorizer(ngram_range=ngram_range, stop_words='english', min_df=min_df, max_df=0.95)\n",
    "    else:\n",
    "        doc2vec = CountVectorizer(ngram_range=ngram_range, stop_words='english', min_df=min_df, max_df=0.95)\n",
    "\n",
    "    X_train = doc2vec.fit_transform(docs_train)\n",
    "    X_test = doc2vec.transform(docs_test)\n",
    "    \n",
    "    ## fit the logistic regression with ridge model \n",
    "    logit = LogisticRegressionCV()\n",
    "    logit.fit(X_train, target_train)\n",
    "    \n",
    "    ## print metrics \n",
    "    test_predictions = logit.predict(X_test)\n",
    "    print classification_report(target_test, test_predictions, target_names = data_train.target_names)\n",
    "    print logit.score(X_test, target_test)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## attributes, instance variable input: data, min_df, ngram_Range, vectorizer_type \n",
    "## attributes: scores, logistic regression cv, doc2vec \n",
    "## method: classification report, fitting/predict/score the logistic regression,\n",
    "class LogisticRegresionText(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data,\n",
    "                 min_df = 0.001, \n",
    "                 max_df = 0.95,\n",
    "                 ngram_range = (1, 2), \n",
    "                 vecotrizer_type = 'tfidf'):\n",
    "        \n",
    "        self.data = data \n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vectorizer_type = vecotrizer_type    \n",
    "        self.docs_train, self.target_train, self.docs_test, self.target_test = data\n",
    "    \n",
    "    def vectorize_docs(self):\n",
    "        if self.vectorizer_type == 'tfidf':\n",
    "            self.doc2vec = TfidfVectorizer(ngram_range=self.ngram_range, \n",
    "                                           min_df=self.min_df, \n",
    "                                           max_df=self.max_df,\n",
    "                                           stop_words='english')\n",
    "        else:\n",
    "            self.doc2vec = CountVectorizer(ngram_range=self.ngram_range, \n",
    "                                           min_df=self.min_df, \n",
    "                                           max_df=self.max_df,\n",
    "                                           stop_words='english')\n",
    "        \n",
    "        self.X_train = self.doc2vec.fit_transform(self.docs_train)\n",
    "        self.X_test = self.doc2vec.transform(self.docs_test)\n",
    "        \n",
    "    def fit_logit(self):\n",
    "        self.vectorize_docs()\n",
    "        self.logit = LogisticRegressionCV()\n",
    "        self.logit.fit(self.X_train, self.target_train)\n",
    "    \n",
    "    def predict_logit(self):\n",
    "        self.fit_logit()\n",
    "        self.test_predictions = self.logit.predict(self.X_test)\n",
    "        self.accuracy =  self.logit.score(self.X_test, self.target_test)\n",
    "        print classification_report(self.target_test, \n",
    "                                    self.test_predictions, \n",
    "                                    target_names = data_train.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.66      0.58      0.62       319\n",
      "     comp.graphics       0.82      0.90      0.86       389\n",
      "         sci.space       0.81      0.82      0.81       394\n",
      "talk.religion.misc       0.61      0.60      0.61       251\n",
      "\n",
      "       avg / total       0.74      0.75      0.74      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_logit = LogisticRegresionText(data)\n",
    "sample_logit.predict_logit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'00 pounds',\n",
       " u'000',\n",
       " u'000 000',\n",
       " u'000 feet',\n",
       " u'000 years',\n",
       " u'01',\n",
       " u'01 14',\n",
       " u'02',\n",
       " u'03',\n",
       " u'04',\n",
       " u'04 01',\n",
       " u'04 17computer',\n",
       " u'04 21',\n",
       " u'05',\n",
       " u'06',\n",
       " u'0674',\n",
       " u'07',\n",
       " u'08',\n",
       " u'09',\n",
       " u'10',\n",
       " u'10 00',\n",
       " u'10 000',\n",
       " u'10 10',\n",
       " u'10 15',\n",
       " u'10 20',\n",
       " u'10 30',\n",
       " u'10 clicks',\n",
       " u'10 minutes',\n",
       " u'10 years',\n",
       " u'100',\n",
       " u'100 000',\n",
       " u'100 years',\n",
       " u'1000',\n",
       " u'101',\n",
       " u'101010',\n",
       " u'101010 binary',\n",
       " u'102',\n",
       " u'102 18',\n",
       " u'1024x768',\n",
       " u'1030',\n",
       " u'104',\n",
       " u'105',\n",
       " u'107',\n",
       " u'109',\n",
       " u'11',\n",
       " u'11 nineteenth',\n",
       " u'110',\n",
       " u'111',\n",
       " u'112',\n",
       " u'113',\n",
       " u'115',\n",
       " u'1150',\n",
       " u'12',\n",
       " u'12 13',\n",
       " u'120',\n",
       " u'1200',\n",
       " u'1200 2400',\n",
       " u'121',\n",
       " u'125',\n",
       " u'128',\n",
       " u'128 102',\n",
       " u'128 149',\n",
       " u'128 214',\n",
       " u'129',\n",
       " u'129 92',\n",
       " u'13',\n",
       " u'13 38',\n",
       " u'130',\n",
       " u'130 11',\n",
       " u'130 167',\n",
       " u'131',\n",
       " u'133',\n",
       " u'1333',\n",
       " u'134',\n",
       " u'135',\n",
       " u'136',\n",
       " u'137',\n",
       " u'138',\n",
       " u'13h',\n",
       " u'14',\n",
       " u'14 1993',\n",
       " u'14 39',\n",
       " u'140',\n",
       " u'1400',\n",
       " u'141',\n",
       " u'142',\n",
       " u'144',\n",
       " u'145',\n",
       " u'146',\n",
       " u'147',\n",
       " u'149',\n",
       " u'15',\n",
       " u'15 000',\n",
       " u'15 16',\n",
       " u'15 1993',\n",
       " u'15 20',\n",
       " u'15 bit',\n",
       " u'15 years',\n",
       " u'150',\n",
       " u'150 cd',\n",
       " u'1500',\n",
       " u'151',\n",
       " u'152',\n",
       " u'155',\n",
       " u'16',\n",
       " u'16 32',\n",
       " u'16 bit',\n",
       " u'16 color',\n",
       " u'160',\n",
       " u'1600',\n",
       " u'163',\n",
       " u'167',\n",
       " u'169',\n",
       " u'17',\n",
       " u'17 april',\n",
       " u'17 life',\n",
       " u'171',\n",
       " u'172',\n",
       " u'172 27',\n",
       " u'178',\n",
       " u'17computer',\n",
       " u'17computer graphics',\n",
       " u'18',\n",
       " u'18 172',\n",
       " u'180',\n",
       " u'1800',\n",
       " u'18084tm',\n",
       " u'18084tm ibm',\n",
       " u'189',\n",
       " u'19',\n",
       " u'192',\n",
       " u'192 35',\n",
       " u'194',\n",
       " u'1950',\n",
       " u'1958',\n",
       " u'1960',\n",
       " u'1960s',\n",
       " u'1963',\n",
       " u'1964',\n",
       " u'1965',\n",
       " u'1966',\n",
       " u'1968',\n",
       " u'1969',\n",
       " u'1970',\n",
       " u'1971',\n",
       " u'1972',\n",
       " u'1973',\n",
       " u'1975',\n",
       " u'1976',\n",
       " u'1977',\n",
       " u'1978',\n",
       " u'1979',\n",
       " u'1980',\n",
       " u'1981',\n",
       " u'1982',\n",
       " u'1983',\n",
       " u'1984',\n",
       " u'1985',\n",
       " u'1986',\n",
       " u'1987',\n",
       " u'1988',\n",
       " u'1989',\n",
       " u'1989 pp',\n",
       " u'1990',\n",
       " u'1991',\n",
       " u'1992',\n",
       " u'1992 1993',\n",
       " u'1993',\n",
       " u'1993 04',\n",
       " u'1993version',\n",
       " u'1994',\n",
       " u'1995',\n",
       " u'1996',\n",
       " u'1997',\n",
       " u'1999',\n",
       " u'1b',\n",
       " u'1d',\n",
       " u'1g',\n",
       " u'1m',\n",
       " u'1mb',\n",
       " u'1st',\n",
       " u'20',\n",
       " u'20 000',\n",
       " u'20 20',\n",
       " u'20 30',\n",
       " u'20 50',\n",
       " u'20 52',\n",
       " u'20 surface',\n",
       " u'20 years',\n",
       " u'200',\n",
       " u'200 km',\n",
       " u'2000',\n",
       " u'2000 years',\n",
       " u'2001',\n",
       " u'20024',\n",
       " u'201',\n",
       " u'202',\n",
       " u'202 358',\n",
       " u'205',\n",
       " u'20771',\n",
       " u'209',\n",
       " u'20k',\n",
       " u'20th',\n",
       " u'20th century',\n",
       " u'21',\n",
       " u'21 22',\n",
       " u'210',\n",
       " u'21000',\n",
       " u'212',\n",
       " u'213',\n",
       " u'214',\n",
       " u'214 100',\n",
       " u'215',\n",
       " u'216',\n",
       " u'216 433',\n",
       " u'217',\n",
       " u'2178',\n",
       " u'2178 wk',\n",
       " u'22',\n",
       " u'22 1993',\n",
       " u'220',\n",
       " u'225',\n",
       " u'227',\n",
       " u'229',\n",
       " u'23',\n",
       " u'23 1993',\n",
       " u'230',\n",
       " u'231',\n",
       " u'24',\n",
       " u'24 10',\n",
       " u'24 bit',\n",
       " u'24 bits',\n",
       " u'24 hours',\n",
       " u'240',\n",
       " u'2400',\n",
       " u'246',\n",
       " u'246 17',\n",
       " u'24bit',\n",
       " u'24th',\n",
       " u'24x',\n",
       " u'25',\n",
       " u'25 92',\n",
       " u'250',\n",
       " u'253',\n",
       " u'254',\n",
       " u'255',\n",
       " u'256',\n",
       " u'256 000',\n",
       " u'256 color',\n",
       " u'256 colors',\n",
       " u'258',\n",
       " u'26',\n",
       " u'264',\n",
       " u'264 hours',\n",
       " u'265',\n",
       " u'267',\n",
       " u'27',\n",
       " u'270',\n",
       " u'272',\n",
       " u'273',\n",
       " u'275',\n",
       " u'277',\n",
       " u'28',\n",
       " u'28 30',\n",
       " u'280',\n",
       " u'282',\n",
       " u'284',\n",
       " u'286',\n",
       " u'2888',\n",
       " u'2888 dublin',\n",
       " u'29',\n",
       " u'2d',\n",
       " u'2d 3d',\n",
       " u'2mb',\n",
       " u'2nd',\n",
       " u'30',\n",
       " u'30 1993',\n",
       " u'30 days',\n",
       " u'30 minutes',\n",
       " u'300',\n",
       " u'300 1200',\n",
       " u'3000',\n",
       " u'301',\n",
       " u'301 286',\n",
       " u'303',\n",
       " u'31',\n",
       " u'310',\n",
       " u'312',\n",
       " u'32',\n",
       " u'32 bit',\n",
       " u'320',\n",
       " u'320x200',\n",
       " u'320x200x256',\n",
       " u'320x240',\n",
       " u'325',\n",
       " u'33',\n",
       " u'330',\n",
       " u'336',\n",
       " u'336 9591',\n",
       " u'34',\n",
       " u'34 35',\n",
       " u'344',\n",
       " u'345',\n",
       " u'35',\n",
       " u'35 246',\n",
       " u'350',\n",
       " u'354',\n",
       " u'354 1333',\n",
       " u'355',\n",
       " u'355 2178',\n",
       " u'3553',\n",
       " u'358',\n",
       " u'36',\n",
       " u'360',\n",
       " u'37',\n",
       " u'373',\n",
       " u'38',\n",
       " u'380',\n",
       " u'386',\n",
       " u'386 486',\n",
       " u'39',\n",
       " u'390',\n",
       " u'3d',\n",
       " u'3d animation',\n",
       " u'3d data',\n",
       " u'3d graphics',\n",
       " u'3d modeling',\n",
       " u'3d objects',\n",
       " u'3d studio',\n",
       " u'3do',\n",
       " u'3ds',\n",
       " u'3rd',\n",
       " u'3rd debate',\n",
       " u'40',\n",
       " u'400',\n",
       " u'4000',\n",
       " u'405',\n",
       " u'407',\n",
       " u'408',\n",
       " u'408 428',\n",
       " u'41',\n",
       " u'410',\n",
       " u'415',\n",
       " u'416',\n",
       " u'42',\n",
       " u'42 101010',\n",
       " u'427',\n",
       " u'427 0674',\n",
       " u'428',\n",
       " u'428 3553',\n",
       " u'43',\n",
       " u'430',\n",
       " u'433',\n",
       " u'435',\n",
       " u'4368',\n",
       " u'44',\n",
       " u'44135',\n",
       " u'444',\n",
       " u'45',\n",
       " u'458',\n",
       " u'46',\n",
       " u'4650',\n",
       " u'47',\n",
       " u'471',\n",
       " u'48',\n",
       " u'480',\n",
       " u'4800',\n",
       " u'483',\n",
       " u'483 4368',\n",
       " u'486',\n",
       " u'49',\n",
       " u'494',\n",
       " u'495',\n",
       " u'4d',\n",
       " u'4mb',\n",
       " u'4th',\n",
       " u'50',\n",
       " u'50 000',\n",
       " u'50 50',\n",
       " u'500',\n",
       " u'500 000',\n",
       " u'5000',\n",
       " u'503',\n",
       " u'508',\n",
       " u'51',\n",
       " u'512',\n",
       " u'513',\n",
       " u'513 427',\n",
       " u'515',\n",
       " u'517',\n",
       " u'517 355',\n",
       " u'52',\n",
       " u'53',\n",
       " u'530',\n",
       " u'54',\n",
       " u'55',\n",
       " u'550',\n",
       " u'56',\n",
       " u'560',\n",
       " u'57',\n",
       " u'58',\n",
       " u'59',\n",
       " u'60',\n",
       " u'60 80',\n",
       " u'600',\n",
       " u'6000',\n",
       " u'601',\n",
       " u'602',\n",
       " u'604',\n",
       " u'605',\n",
       " u'61',\n",
       " u'617',\n",
       " u'619',\n",
       " u'62',\n",
       " u'63',\n",
       " u'64',\n",
       " u'640',\n",
       " u'640x480',\n",
       " u'65',\n",
       " u'66',\n",
       " u'666',\n",
       " u'67',\n",
       " u'68',\n",
       " u'68070',\n",
       " u'69',\n",
       " u'6th',\n",
       " u'70',\n",
       " u'700',\n",
       " u'703',\n",
       " u'708',\n",
       " u'70s',\n",
       " u'71',\n",
       " u'713',\n",
       " u'713 483',\n",
       " u'72',\n",
       " u'725',\n",
       " u'73',\n",
       " u'735',\n",
       " u'75',\n",
       " u'75 00',\n",
       " u'750',\n",
       " u'76',\n",
       " u'768',\n",
       " u'77',\n",
       " u'77058',\n",
       " u'78',\n",
       " u'79',\n",
       " u'790',\n",
       " u'7900',\n",
       " u'7th',\n",
       " u'7th day',\n",
       " u'80',\n",
       " u'800',\n",
       " u'800 km',\n",
       " u'8000',\n",
       " u'800x600',\n",
       " u'805',\n",
       " u'81',\n",
       " u'810',\n",
       " u'818',\n",
       " u'818 354',\n",
       " u'82',\n",
       " u'83',\n",
       " u'84',\n",
       " u'85',\n",
       " u'86',\n",
       " u'862',\n",
       " u'867',\n",
       " u'87',\n",
       " u'88',\n",
       " u'89',\n",
       " u'8900',\n",
       " u'895',\n",
       " u'8mm',\n",
       " u'90',\n",
       " u'900',\n",
       " u'9000',\n",
       " u'91',\n",
       " u'91109',\n",
       " u'91109 818',\n",
       " u'916',\n",
       " u'92',\n",
       " u'92 66',\n",
       " u'93',\n",
       " u'93 04',\n",
       " u'93103',\n",
       " u'94035',\n",
       " u'95',\n",
       " u'950',\n",
       " u'9591',\n",
       " u'9591 hm',\n",
       " u'96',\n",
       " u'9600',\n",
       " u'9600 bps',\n",
       " u'97',\n",
       " u'98',\n",
       " u'99',\n",
       " u'aas',\n",
       " u'ab',\n",
       " u'abandon',\n",
       " u'abandoned',\n",
       " u'abc',\n",
       " u'abilities',\n",
       " u'ability',\n",
       " u'abit',\n",
       " u'able',\n",
       " u'able read',\n",
       " u'able tell',\n",
       " u'able use',\n",
       " u'able work',\n",
       " u'ableto',\n",
       " u'aboard',\n",
       " u'abolish',\n",
       " u'abort',\n",
       " u'abortion',\n",
       " u'abouta',\n",
       " u'aboutthe',\n",
       " u'abraham',\n",
       " u'abruptly',\n",
       " u'absence',\n",
       " u'absence belief',\n",
       " u'absolute',\n",
       " u'absolute moral',\n",
       " u'absolute morality',\n",
       " u'absolute truth',\n",
       " u'absolutely',\n",
       " u'absorption',\n",
       " u'abstract',\n",
       " u'abstracts',\n",
       " u'absurd',\n",
       " u'abuse',\n",
       " u'ac',\n",
       " u'ac nz',\n",
       " u'ac uk',\n",
       " u'acad3',\n",
       " u'acad3 alaska',\n",
       " u'academic',\n",
       " u'academy',\n",
       " u'acceleration',\n",
       " u'accelerations',\n",
       " u'accelerator',\n",
       " u'accept',\n",
       " u'acceptable',\n",
       " u'acceptance',\n",
       " u'accepted',\n",
       " u'accepted number',\n",
       " u'accepting',\n",
       " u'accepts',\n",
       " u'access',\n",
       " u'access digex',\n",
       " u'access visa',\n",
       " u'accessed',\n",
       " u'accessed 24',\n",
       " u'accessible',\n",
       " u'accident',\n",
       " u'accidental',\n",
       " u'accidentally',\n",
       " u'accidents',\n",
       " u'accompanied',\n",
       " u'accompanying',\n",
       " u'accomplish',\n",
       " u'accomplished',\n",
       " u'accordance',\n",
       " u'according',\n",
       " u'according jim',\n",
       " u'account',\n",
       " u'accounting',\n",
       " u'accounts',\n",
       " u'accredited',\n",
       " u'accumulate',\n",
       " u'accuracy',\n",
       " u'accurate',\n",
       " u'accurately',\n",
       " u'accusation',\n",
       " u'accusations',\n",
       " u'accuse',\n",
       " u'accused',\n",
       " u'accusing',\n",
       " u'ace',\n",
       " u'achieve',\n",
       " u'achieved',\n",
       " u'achristian',\n",
       " u'acid',\n",
       " u'acknowledge',\n",
       " u'acknowledged',\n",
       " u'acknowledgement',\n",
       " u'acknowledges',\n",
       " u'acm',\n",
       " u'acm siggraph',\n",
       " u'acquainted',\n",
       " u'acquire',\n",
       " u'acronym',\n",
       " u'acsu',\n",
       " u'acsu buffalo',\n",
       " u'act',\n",
       " u'acting',\n",
       " u'action',\n",
       " u'action does',\n",
       " u'actions',\n",
       " u'active',\n",
       " u'actively',\n",
       " u'activist',\n",
       " u'activities',\n",
       " u'activity',\n",
       " u'activity far',\n",
       " u'acts',\n",
       " u'actual',\n",
       " u'actually',\n",
       " u'actually said',\n",
       " u'ad',\n",
       " u'ad astra',\n",
       " u'adam',\n",
       " u'adam eve',\n",
       " u'adams',\n",
       " u'adams nsmca',\n",
       " u'adaptation',\n",
       " u'adapted',\n",
       " u'adapter',\n",
       " u'adapting',\n",
       " u'add',\n",
       " u'add airmail',\n",
       " u'add support',\n",
       " u'added',\n",
       " u'adding',\n",
       " u'addition',\n",
       " u'additional',\n",
       " u'additionally',\n",
       " u'additions',\n",
       " u'address',\n",
       " u'address answer',\n",
       " u'address issue',\n",
       " u'address mark',\n",
       " u'address password',\n",
       " u'address request',\n",
       " u'addressed',\n",
       " u'addressed order',\n",
       " u'addresses',\n",
       " u'addressing',\n",
       " u'adds',\n",
       " u'adequate',\n",
       " u'adequately',\n",
       " u'adjustment',\n",
       " u'admin',\n",
       " u'administration',\n",
       " u'administrative',\n",
       " u'administrator',\n",
       " u'admiration',\n",
       " u'admission',\n",
       " u'admit',\n",
       " u'admitted',\n",
       " u'admittedly',\n",
       " u'adobe',\n",
       " u'adobe photoshop',\n",
       " u'adopt',\n",
       " u'adopted',\n",
       " u'ads',\n",
       " u'adult',\n",
       " u'adults',\n",
       " u'advance',\n",
       " u'advance help',\n",
       " u'advanced',\n",
       " u'advanced space',\n",
       " u'advancement',\n",
       " u'advances',\n",
       " u'advantage',\n",
       " u'advantages',\n",
       " u'advertisement',\n",
       " u'advertisers',\n",
       " u'advertising',\n",
       " u'advertising space',\n",
       " u'advice',\n",
       " u'advised',\n",
       " u'advisory',\n",
       " u'advisory committee',\n",
       " u'advocate',\n",
       " u'advocated',\n",
       " u'advocates',\n",
       " u'aero',\n",
       " u'aerobraking',\n",
       " u'aeronautics',\n",
       " u'aeronautics space',\n",
       " u'aerospace',\n",
       " u'af',\n",
       " u'af mil',\n",
       " u'afb',\n",
       " u'afew',\n",
       " u'affairs',\n",
       " u'affect',\n",
       " u'affected',\n",
       " u'affecting',\n",
       " u'affiliation',\n",
       " u'affirmation',\n",
       " u'affirmed',\n",
       " u'afford',\n",
       " u'affordable',\n",
       " u'afit',\n",
       " u'afit af',\n",
       " u'afraid',\n",
       " u'africa',\n",
       " u'african',\n",
       " u'african americans',\n",
       " u'afternoon',\n",
       " u'age',\n",
       " u'agencies',\n",
       " u'agency',\n",
       " u'agency esa',\n",
       " u'agenda',\n",
       " u'agent',\n",
       " u'agents',\n",
       " u'ages',\n",
       " u'agnostic',\n",
       " u'agnosticism',\n",
       " u'agnostics',\n",
       " u'ago',\n",
       " u'agood',\n",
       " u'agree',\n",
       " u'agree really',\n",
       " u'agree robert',\n",
       " u'agreed',\n",
       " u'agreement',\n",
       " u'agreements',\n",
       " u'agrees',\n",
       " u'ah',\n",
       " u'ahead',\n",
       " u'ahmediye',\n",
       " u'ahmediye risalesi',\n",
       " u'aiaa',\n",
       " u'aid',\n",
       " u'aided',\n",
       " u'aids',\n",
       " u'aiken',\n",
       " u'aim',\n",
       " u'aim stanford',\n",
       " u'aimed',\n",
       " u'ain',\n",
       " u'air',\n",
       " u'air force',\n",
       " u'air space',\n",
       " u'aircraft',\n",
       " u'airmail',\n",
       " u'airmail access',\n",
       " u'airplane',\n",
       " u'airport',\n",
       " u'airports',\n",
       " u'aka',\n",
       " u'akin',\n",
       " u'al',\n",
       " u'ala',\n",
       " u'alabama',\n",
       " u'alan',\n",
       " u'alas',\n",
       " u'alaska',\n",
       " u'alaska edu',\n",
       " u'albedo',\n",
       " u'albeit',\n",
       " u'albert',\n",
       " u'albert einstein',\n",
       " u'alchemy',\n",
       " u'alert',\n",
       " u'alexander',\n",
       " u'alexandria',\n",
       " u'alexia',\n",
       " u'alexia lis',\n",
       " u'algorithm',\n",
       " u'algorithms',\n",
       " u'algorithms just',\n",
       " u'ali',\n",
       " u'aliasing',\n",
       " u'alien',\n",
       " u'alignment',\n",
       " u'alike',\n",
       " u'alittle',\n",
       " u'alive',\n",
       " u'alive moon',\n",
       " u'allah',\n",
       " u'allah allah',\n",
       " u'allegations',\n",
       " u'alleged',\n",
       " u'allegiance',\n",
       " u'allegory',\n",
       " u'allen',\n",
       " u'allies',\n",
       " u'allocated',\n",
       " u'allof',\n",
       " u'allow',\n",
       " u'allowed',\n",
       " u'allowing',\n",
       " u'allows',\n",
       " u'allows users',\n",
       " u'allthe',\n",
       " u'alongthe',\n",
       " u'alot',\n",
       " u'aloud',\n",
       " u'alpha',\n",
       " u'als',\n",
       " u'alt',\n",
       " u'alt atheism',\n",
       " u'alt binaries',\n",
       " u'alt fan',\n",
       " u'altar',\n",
       " u'altar boy',\n",
       " u'alter',\n",
       " u'altered',\n",
       " u'alternate',\n",
       " u'alternative',\n",
       " u'alternatives',\n",
       " u'altitude',\n",
       " u'altogether',\n",
       " u'aluminum',\n",
       " u'ama',\n",
       " u'amateur',\n",
       " u'amateur radio',\n",
       " u'amazing',\n",
       " u'ambiguous',\n",
       " u'ambitious',\n",
       " u'america',\n",
       " u'american',\n",
       " u'american culture',\n",
       " u'american institute',\n",
       " u'americans',\n",
       " u'ames',\n",
       " u'ames arc',\n",
       " u'ames archive',\n",
       " u'ames dryden',\n",
       " u'ames research',\n",
       " u'ames space',\n",
       " u'amiga',\n",
       " u'amigas',\n",
       " u'amnot',\n",
       " u'amorc',\n",
       " u'amounts',\n",
       " u'ample',\n",
       " u'amsat',\n",
       " u'amusing',\n",
       " u'analog',\n",
       " u'analogous',\n",
       " u'analogy',\n",
       " u'analyses',\n",
       " u'analysis',\n",
       " u'analysis software',\n",
       " u'analyst',\n",
       " u'analytical',\n",
       " u'analyze',\n",
       " u'ancestors',\n",
       " u'ancestry',\n",
       " u'anchor',\n",
       " u'ancient',\n",
       " u'anda',\n",
       " u'andall',\n",
       " u'anderson',\n",
       " u'anderson replied',\n",
       " u'andevil',\n",
       " u'andhave',\n",
       " u'andhis',\n",
       " u'andi',\n",
       " u'andis',\n",
       " u'andit',\n",
       " u'andnot',\n",
       " u'andreas',\n",
       " u'andrew',\n",
       " u'andrew cmu',\n",
       " u'andshould',\n",
       " u'andtechnology',\n",
       " u'andthat',\n",
       " u'andthe',\n",
       " u'andtheir',\n",
       " u'andto',\n",
       " u'andwill',\n",
       " u'andwould',\n",
       " u'andy',\n",
       " u'andyou',\n",
       " u'anew',\n",
       " u'angel',\n",
       " u'angeles',\n",
       " u'angeles ca',\n",
       " u'angels',\n",
       " u'angels freewill',\n",
       " u'angle',\n",
       " u'angles',\n",
       " u'angular',\n",
       " u'animal',\n",
       " u'animal kingdom',\n",
       " u'animals',\n",
       " u'animated',\n",
       " u'animation',\n",
       " u'animations',\n",
       " u'animator',\n",
       " u'announce',\n",
       " u'announce reward',\n",
       " u'announced',\n",
       " u'announcement',\n",
       " u'announcements',\n",
       " u'annoy',\n",
       " u'annoyed',\n",
       " u'annual',\n",
       " u'anobjective',\n",
       " u'anon',\n",
       " u'anonymous',\n",
       " u'anonymous ftp',\n",
       " u'ansi',\n",
       " u'answer',\n",
       " u'answer point',\n",
       " u'answer question',\n",
       " u'answer questions',\n",
       " u'answer think',\n",
       " u'answer yes',\n",
       " u'answered',\n",
       " u'answered question',\n",
       " u'answering',\n",
       " u'answers',\n",
       " u'answers don',\n",
       " u'answers questions',\n",
       " u'antenna',\n",
       " u'anthony',\n",
       " u'anti',\n",
       " u'anti aliasing',\n",
       " u'anti semitism',\n",
       " u'anti social',\n",
       " u'antiquity',\n",
       " u'antranslations',\n",
       " u'antranslations arabic',\n",
       " u'ants',\n",
       " u'anybody',\n",
       " u'anybody got',\n",
       " u'anybody help',\n",
       " u'anybody know',\n",
       " u'anymore',\n",
       " u'anyways',\n",
       " u'ap',\n",
       " u'ap mchp',\n",
       " u'apart',\n",
       " u'aparticular',\n",
       " u'aperson',\n",
       " u'aperture',\n",
       " u'api',\n",
       " u'apollo',\n",
       " u'apologies',\n",
       " u'apologize',\n",
       " u'apology',\n",
       " u'apostle',\n",
       " u'apostles',\n",
       " u'app',\n",
       " u'apparent',\n",
       " u'apparently',\n",
       " u'appeal',\n",
       " u'appealing',\n",
       " u'appeals',\n",
       " u'appear',\n",
       " u'appearance',\n",
       " u'appeared',\n",
       " u'appearing',\n",
       " u'appears',\n",
       " u'appendix',\n",
       " u'apple',\n",
       " u'apple com',\n",
       " u'applicable',\n",
       " u'application',\n",
       " u'applications',\n",
       " u'applied',\n",
       " u'applies',\n",
       " u'apply',\n",
       " u'applying',\n",
       " u'appointed',\n",
       " u'appreciate',\n",
       " u'appreciated',\n",
       " u'appreciated thanks',\n",
       " u'approach',\n",
       " u'approached',\n",
       " u'approaches',\n",
       " u'appropriate',\n",
       " u'approval',\n",
       " u'approve',\n",
       " u'approved',\n",
       " u'approximate',\n",
       " u'approximately',\n",
       " u'approximating',\n",
       " u'apr',\n",
       " u'apr 16',\n",
       " u'apr 1993',\n",
       " u'apr 93',\n",
       " u'april',\n",
       " u'april 15',\n",
       " u'april 1993',\n",
       " u'april 1993version',\n",
       " u'april 21',\n",
       " u'april 23',\n",
       " u'april 30',\n",
       " u'arabia',\n",
       " u'arabic',\n",
       " u'arabic turkish',\n",
       " u'arbitrarily',\n",
       " u'arbitrary',\n",
       " u'arc',\n",
       " u'arc nasa',\n",
       " u'archer',\n",
       " u'archie',\n",
       " u'archimedes',\n",
       " u'architecture',\n",
       " u'archive',\n",
       " ...]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_logit.doc2vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.66      0.58      0.62       319\n",
      "     comp.graphics       0.82      0.90      0.86       389\n",
      "         sci.space       0.81      0.82      0.81       394\n",
      "talk.religion.misc       0.61      0.60      0.61       251\n",
      "\n",
      "       avg / total       0.74      0.75      0.74      1353\n",
      "\n",
      "0.745011086475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_logit(data, min_df=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classifier comparison\n",
    "\n",
    "Of all the vectorizers tested above, choose one that has a reasonable performance with a manageable number of features and compare the performance of these models:\n",
    "\n",
    "- KNN\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machine\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "In order to speed up the calculation it's better to vectorize the data only once and then compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other classifiers\n",
    "\n",
    "Adapt the code from [this example](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py) to compare across all the classifiers suggested and to display the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: NLTK\n",
    "\n",
    "NLTK is a vast library. Can you find some interesting bits to share with classmates?\n",
    "Start here: http://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
