{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Use the `fakefriends.csv` data to figure out the Average Number of Friends by Age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Use the `1800.csv` to find the minimum and maximum temperatures by location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Use the `Book.txt` file to build a word counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "From the `ml-100/u.data`, find the most popular movies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Use the Marvel Superheroes dataset to find the most popular superhero based on co-occurrences with other superheroes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 \n",
    "\n",
    "Let's download Bay Area Bike Share's trip data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! wget https://s3.amazonaws.com/babs-open-data/babs_open_data_year_1.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! unzip babs_open_data_year_1.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -f *.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of the downloaded data\n",
    "! du -h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = sc.textFile(\"201408_babs_open_data/201408_trip_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Check:** What kind of object is `trips`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Split CSV Lines\n",
    "\n",
    "In Spark, we can build complex pipelines that only get executed when we ask to collect them.\n",
    "\n",
    "\n",
    "\n",
    "In other words, we can define the pipeline with all its steps, and only when we call collect will the data flow through it. In order to get familiar with this new workflow, we will start with small steps to build our pipeline.\n",
    "\n",
    "First step:\n",
    "- Apply a map to trips that splits each line at commas and save that to a an RDD\n",
    "\n",
    "**Hint:** if you want to check that you're doing things right, you can collect the result and display the first few lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: filter for Caltrain stationÂ¶\n",
    "\n",
    "In Spark we can also create filters using the `filter` method. Let's select station number 70 by filtering on the 5th column, we will do all the following analysis just on this station, which corresponds to the most popular starting point. Save this to a variable called `station_70.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: trips by day - hour (mapper)\n",
    "Let's analyse the trips by the hour. We can do this by performing a map reduce job in Spark. First we will need to emit tuples with a count of 1 for each (date, hour) key, and then we will sum the counts by key.\n",
    "\n",
    "- Emit tuple of ((date, hour), 1), applying a map to `station_70` that extracts the relevant data from each line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:  trips by day - hour (reducer)\n",
    "\n",
    "Use the `reduceByKey` method to obtain the number of trips per (day, hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: trips by hour (mapper)\n",
    "\n",
    "Let's further group the trips by hour. We'll do this with a second Map Reduce job.\n",
    "First we will discard the day and emit tuples of (hour, count). You can achieve this with a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: trips by hour (reducer)\n",
    "Then let's calculate the average number of trips by hour using the `combineByKey` method.\n",
    "\n",
    "You can find a suggestion on how to do it [here](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: collect\n",
    "We can finally collect our result and sort them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
