{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "### Introduction to Big Data\n",
    "\n",
    "Week 8 | Lesson 3.1\n",
    "\n",
    "---\n",
    "| TIMING  | TYPE  \n",
    "|:-:|---|---|\n",
    "| 25 min| [Review: SQL](#review) |\n",
    "| 90 min| [Introduction to Big Data](#content) |\n",
    "| 20 min| [Conclusion](#conclusion) |\n",
    "| 5 min | [Additional Resources](#more)\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "\n",
    "- Recognize Big Data Problems \n",
    "- Explain how the map reduce algorithm works \n",
    "- Perform a map-reduce on a single node using Python\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Student Pre-Work \n",
    "\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Write SQL Queries \n",
    "- Understand the fundamentals of CS Architecture  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 20), ('F', 30), ('M', 20)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(['M', 'F', 'M'], [20, 30, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Fundamentals of SQL\n",
    "---\n",
    "<a name=\"review\"></a>\n",
    "**Exercise:** What is the algorithmic complexity of the \n",
    "following SQL queries? \n",
    "\n",
    "```     \n",
    "SELECT * from consumers \n",
    "\n",
    "SELECT * from consumers c \n",
    "    INNER JOIN deliveries d \n",
    "    ON d.consumer_id = c.id \n",
    "\n",
    "SELECT LEFT(sub.date, 2) AS cleaned_month,\n",
    "       sub.day_of_week,\n",
    "       AVG(sub.incidents) AS average_incidents\n",
    " FROM (\n",
    "        SELECT day_of_week,\n",
    "               date,\n",
    "               COUNT(incidnt_num) AS incidents\n",
    "         FROM tutorial.sf_crime_incidents_2014_01\n",
    "         GROUP BY 1,2\n",
    "       ) sub\n",
    " GROUP BY 1,2\n",
    " ORDER BY 1,2\n",
    " \n",
    "```     \n",
    "*Hint: You can assume all tables are indexed. Think about the computational complexity of a `SELECT`, `INNER JOIN`, `GroupBy`, and `ORDER BY` operations. Try to write out the Python code that might match with the query.*\n",
    "\n",
    "** Exercise:** Does it impact the perfomance of a query whether a table is indexed or not?\n",
    "\n",
    "*Hint: Make an argument based on computational complexity.*\n",
    "\n",
    "** Exercise:** What are the tradeoffs associated with having nested inner joins in your query vs. creating a new table from operations of frequently used join operations?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Installation \n",
    "---\n",
    "**Step 1: Download the JDK: <a href=http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html> Oracle Website </a>\n",
    "**\n",
    "\n",
    "Spark runs on the Java Virtual Machine, or JVM for short, which comes in the Java SE Development Kit (JDK for short). We recommend installing Java SE Development Kit version 7 or higher, which you can download from Oracle’s website.\n",
    "\n",
    "**Step 2: Download the Pre-Built Version of Spark: <a href=http://spark.apache.org/downloads.html> Pre-built Version of Spark </a>**\n",
    "\n",
    "Click the link that appears in Step 4 to download Spark as a .TGZ file to your computer. Open your command line application and navigate to the folder you downloaded it to. Unzip the file and move the resulting folder into your home directory. Windows does not have a built in utility that can unzip tgz files - we recommend downloading and using 7-Zip. Once you have unzipped the file, move the resulting folder into your home directory.\n",
    "\n",
    "**Step 3: PySpark Shell**\n",
    "\n",
    "In the last mission, you learned that PySpark is a Python library that allows us to interact with Spark objects. The source code for the PySpark library is located in the `python/pyspark` directory, but the executable version of the library is located in `bin/pyspark`. To test whether your installation built Spark properly, run the command `bin/pyspark` to start up the PySpark shell. The output should be similar to Spark Shell (very similar to IPython Shell but with Spark logo). \n",
    "\n",
    "**Step 4: Set the Shell Enviornment.**\n",
    "\n",
    "Move your Spark Directory: \n",
    "\n",
    "    sudo mv ~/Downloads/spark-2.1.0-bin-hadoop2.7 /usr/local/spark\n",
    "\n",
    "If you're using the default Terminal application, open the file `~/.bash_profile`. If you're using ZSH instead, your configuration file will be in `~/.zshrc.`\n",
    "\n",
    "    # spark path\n",
    "    SPARK_HOME=\"/usr/local/spark\"\n",
    "    PATH=\"/usr/local/spark/bin:$PATH\"\n",
    "    \n",
    "    export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n",
    "    \n",
    "    # use whatever version is used for your python/lib/py4j- ...\n",
    "    export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"hook\"> </a>\n",
    "\n",
    "### Big Data: Introduction\n",
    "\n",
    "*** \n",
    "<center>\n",
    "**\"Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it...\" - Dan Ariely, Duke Professor, Best-Selling Author**\n",
    "</center>\n",
    "***\n",
    "\n",
    "> **Exercise:** What do you consider \"Big\" Datasets?  \n",
    "\n",
    "> **Exercise:** What challenges exist when processing \"Big\" Datasets?\n",
    "\n",
    "***\n",
    "Computers have many limitations: \n",
    "- **Memory Limitations:** Dataset won't fit into memory (RAM) but can be stored on your computer. E.g. If you have a 6 gigabyte dataset, and 4 gigabytes of memory, there's no way you can load your data into Pandas and process it without using a workaround.\n",
    "\n",
    "- **Size of Hard Drive:** Datasets bigger than what can be fit into memory. Large Scale weather modeling often fits this category. \n",
    "\n",
    "- ** CPU Bound **: Ability of your CPU to execute quickly \n",
    "\n",
    "- ** I/O **: I/O-bound program will be dependent on external resources, like files on disk and network services to execute quickly. The faster these external resources can be accessed, the faster your program will run.\n",
    "\n",
    "<img src=https://s3.amazonaws.com/dq-content/168/CPU+and+I_O+bounds.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"content\"></a>\n",
    "\n",
    "***\n",
    "Big data is the term used when the **data exceeds what can be stored on a typical computer.** According to Wikipedia, Big data \"size\" is a constantly moving target, as of 2012 ranging from a few dozen terabytes to many petabytes of data.\n",
    "\n",
    "> *Reading:* [Information Overload](https://www.chemheritage.org/distillations/magazine/information-overload)\n",
    "\n",
    "We need a big data analytics when the data grows quickly and we need to uncover hidden patterns, unknown correlations, and other useful information. There are three main features in big data (the 3 \"V\"s):\n",
    "\n",
    "- **Volume**: Large amounts of data (typical \n",
    "- **Variety**: Different types of structured, unstructured, and multi-structured data\n",
    "- **Velocity**: Needs to be analyzed quickly\n",
    "\n",
    "<img src=https://phemi.com/wp-content/uploads/2013/04/small-big-data.png>\n",
    "\n",
    "Vrushank: 4th V (unofficial big data tenet):\n",
    "\n",
    "- **Value**: It's important to assess the value of predictions to business value.  Understanding the underpinnings of cost vs benefit is even more essential in the context of big data.  It's easy to misundersatnd the 3 V's without looking at the bigger picture, connecting the value of the business cases involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two approaches to Big Data: HPC and Cloud.\n",
    "> **Independent Research (15 Mins):**\n",
    "1. **Supercomputers**: Where are the top supercomputers in teh world? How much does it take to build one? Should the US join race to build the best supercomputers? \n",
    "2. **Cloud Computing**: Who are the biggest providers of cloud services? What are the different offerings they have? How much do they cost?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High performance Computing\n",
    "Supercomputers are very expensive, very powerful calculators used by researchers to solve complicated math problems.\n",
    "\n",
    "<img src=http://curiouspost.com/wp-content/uploads/2015/09/supercomputers.jpg height=500 width=500>\n",
    "\n",
    "> pros:\n",
    "- can perform very complex calculation\n",
    "- centrally controlled\n",
    "- useful for research and defense complicated math problems\n",
    "\n",
    "> cons:\n",
    "- expensive\n",
    "- difficult to maintain (self-manged or managed hosting both incur operations overhead)\n",
    "- scalability is bounded (pre-bigdata era:  this would be medium data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud computing\n",
    "Instead of one huge machine, what if we got a bunch of (commodity) machines?\n",
    "\n",
    "![commodity hardware](https://snag.gy/fNYgt0.jpg)<center>*Actual AWS Datacenter*</center>\n",
    "\n",
    "> pros:\n",
    "- Relatively cheaper\n",
    "- Easier to maintain (as a user of the cloud system)\n",
    "- Scalability is unbounded (just add more nodes to the cluster)\n",
    "- Variety of turn-key solutions available through cloud providers\n",
    "\n",
    "> cons:\n",
    "- Complex infrastructure \n",
    "- SME required to leverage lower level resources within infrastructure\n",
    "- Mainly tailored for parallelizable problems\n",
    "- Relatively small cpu power at the lowest level\n",
    "- More I/O between machines\n",
    "\n",
    "The term Big Data refers to the latter case, where commodity hardware with unlimited scalability is used to solve highly parallelizable problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Having a number of commodity machines at your disposal allows you to make use of: \n",
    "\n",
    "- **Parallelism:** The foundation of Big Data processing, is the idea that a problem can be computed by multiple machines together.  This allows many resources to be used in \"parallel\".\n",
    "\n",
    "![](https://snag.gy/MknIN6.jpg)\n",
    "\n",
    "    - Running multiple instances to process data\n",
    "    - Data can be subset and solved iteratively \n",
    "    - Sub-solutions can be solved independently\n",
    "    \n",
    "    \n",
    "- **Divide and Conquer:**Divide and conquer strategy is a fundamental algorithmic technique for solving a given task, whose steps include:\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/xh2mJA.jpg\">\n",
    "\n",
    "    - Split task into subtasks\n",
    "    - Solve these subtasks independently\n",
    "    - Recombine the subtask results into a final result\n",
    "\n",
    "The defining characteristic of a problem that is suitable for the divide and conquer approach is that it can be broken down into independent subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> **Check for Understanding:** \n",
    "1. What does the **`map`** function in Python do?\n",
    "\n",
    " `map(fn, iterable)`. fn is applied to each element in the iterable. \n",
    " \n",
    "> 1. What about **`reduce`** from the `functools` library? \n",
    " \n",
    " `reduce(agg, iterable)` Subsequently applies the `agg` function to elements in the list.  \n",
    " \n",
    "**Review:** Let's Implement the least common multiple function. \n",
    "    \n",
    "    def gcd(a, b):\n",
    "        \"\"\"\n",
    "        implement the gcd fn here\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        while b != 0: \n",
    "        \n",
    "            \n",
    "    def lcm(*args):    \n",
    "        \"\"\"\n",
    "        implement lcm \n",
    "            \n",
    "        \"\"\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gcd(a, b):\n",
    "        \"\"\"\n",
    "        implement the gcd fn here\n",
    "        \n",
    "        \"\"\"\n",
    "        while b: \n",
    "            a, b = b, a%b\n",
    "        return a \n",
    "        \n",
    "            \n",
    "def lcm(*args):    \n",
    "    \"\"\"\n",
    "    implement lcm \n",
    "    \"\"\"\n",
    "    return reduce(lambda x, y: x*y / gcd(x, y), args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Map-Reduce\n",
    "\n",
    "<img src=\"https://snag.gy/XBgCOs.jpg\">\n",
    "\n",
    "The term **Map Reduce** indicate a two-phase divide and conquer algorithm initially invented and publicized by Google in 2004. It involves splitting a problem into subtasks and processing these subtasks in parallel and it consists of two phases:\n",
    "\n",
    "1. the **mapper** phase\n",
    "- the **reducer** phase\n",
    "\n",
    "In the **mapper phase**, data is split into chunks and the same computation is performed on each chunk, while in the **reducer phase**, data is aggregated back to produce a final result.\n",
    "\n",
    "Map-reduce uses a functional programming paradigm.  The data processing primitives are mappers and reducers, as we’ve seen.\n",
    "\n",
    "- **mappers** – filter & transform data\n",
    "- **reducers** – aggregate results\n",
    "\n",
    "The functional paradigm is good at describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins).\n",
    "\n",
    "\n",
    "<img src= http://blog.sqlauthority.com/i/b/mapreduce.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Value pairs\n",
    "\n",
    "<img src=\"https://snag.gy/k2FCar.jpg\">\n",
    "\n",
    "Data is passed through the various phases of a **map-reduce pipeline** as key-value pairs.\n",
    "\n",
    "> ** Exercise: ** What python data structures could be used to implement a key value pair?\n",
    "- **Dictionary**\n",
    "- **Tuple** of 2 elements\n",
    "- **List** of 2 elements\n",
    "- Named **tuple**\n",
    "\n",
    "To understand map reduce one needs to always keep in mind that data is flowing through a pipeline as key-value pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "### Guided Practice: Word Count on paper (20 min)\n",
    "\n",
    "Let's perform a simple map-reduce in the class, let's find the 10 most common words in the paragraph below.\n",
    "\n",
    "    1:  MapReduce is a programming model for large-scale distributed data processing.\n",
    "    3:  It is inspired by the map function and the reduce function of the functional\n",
    "    4:  programming languages such as Lisp, Haskell, or Python. One of the most\n",
    "    5:  important features of MapReduce is that it allows us to hide the low-level\n",
    "    6:  implementation such as message passing or synchronization from users and\n",
    "    7:  allows to split a problem into many partitions. This is a great way to make\n",
    "    8:  trivial parallelization of data processing without any need for\n",
    "    9:  communication between the partitions.\n",
    "    10: MapReduce became main stream because of Apache Hadoop, which is an open\n",
    "    11: source framework that was derived from Google's MapReduce paper.\n",
    "    12: MapReduce allows us to process massive amounts of data in a distributed\n",
    "    13: cluster. In fact, there are many implementations of the MapReduce\n",
    "    14: programming model. Some of them are shown in the following list. It is\n",
    "    15: important to say that MapReduce is not an algorithm; it is just a part\n",
    "    16: of a high-performance infrastructure that provides a lightweight\n",
    "    17: way to run a program in a lot of parallel machines.\n",
    "    18:                from: Practical Data Analysis, Hector Cuesta, 2013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this as follows:\n",
    "- Students will perform the mapper function\n",
    "- Instructor will perform the reducer function\n",
    "\n",
    "Each student will be assigned 1 line of text, and you have to produce a list of key value pairs `(word, 1)` and hand those to the instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Combiners\n",
    "\n",
    "Combiners are intermediate reducers that are performed at node level in a multi node architecture.\n",
    "\n",
    "![](https://snag.gy/lFYfoC.jpg)\n",
    "\n",
    "When data is really large we will distribute it to several mappers running on different machines. Sending a long list of `(word, 1)` pairs to the reducer node is not efficient. We can first aggregate at mapper node level and send the result of the aggregation to the reducer. This is possible because aggregations are associative.\n",
    "\n",
    "Let's repeat the exercise we did before, with a small change.\n",
    "\n",
    "1.Let's divide the class in 3 groups, in each group one student will be the combiner, the others will be mappers.\n",
    "- Let's split the text in 3 parts and each group gets one part\n",
    "- Mapper students produce the same list of `(word, 1)` for each line they receive and hand the list to the combiner\n",
    "- Combiner students sort the lists and sum the counts for words that appear in each list\n",
    "- Finally combiner students hand their list of counts to the instructor who will combine the intermediate sums and produce the final result\n",
    "\n",
    "**Check:** What changed?\n",
    "> \n",
    "\n",
    "Congratulations! you have just performed a map-reduce sum.\n",
    "\n",
    "**Check:** Can you think of other aggregation tasks that can be parallelized in this way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## MapReduce in Python (20 min)\n",
    "\n",
    "Now that we performed map-reduce in person, let's do it in python. Below you can find the code for a simple mapper and reducer that perform the word count.\n",
    "\n",
    "Let's look at them in detail. Here is what the workflow look like: \n",
    "\n",
    "<img src=http://www.mikepluta.com/wp-content/uploads/MapReduce-Data-Flow-of-Word-Count.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mapper.py\n",
    "## input to the mapper will be lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## reducer.py\n",
    "## input to the reducer will be the shuffled \n",
    "## version of what's in the workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the desired output, you will run the follow command at the command line. \n",
    "```\n",
    "bash cat <input-file> | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent practice (15 min)\n",
    "\n",
    "Now that you have a basic word count set up in python, try doing some of the following:\n",
    "- Read the <a href=https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf> Map Reduce Paper </a>\n",
    "- process a much larger text file (you can download it from internet)\n",
    "> for example a page from wikipedia or a blog article. If you're really ambitious you can take books from project gutemberg.\n",
    "- try to see how the execution time scales with file size\n",
    "- read [this article](http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html) for some very powerful shell tricks.  Learning to use the shell will save you tons of time munging data on your filesystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 min)\n",
    "In this class we have learned about Big Data and map-reduce. This is an algorithm that works really well for aggregations on very large datasets.\n",
    "\n",
    "**Check:** now that you know how it works can you think of a more specific business application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"more\"></a>\n",
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Top 500 Supercomputers](http://www.top500.org/lists/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
