{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Image Compression Using PCA and SVD\n",
    "\n",
    "_Author: Kiefer Katovich (SF) _\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, we will write PCA and SVD from scratch and use them to compress and reconstruct images.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand grayscale image processing using NumPy.\n",
    "- Understand the mathematics of PCA and SVD.\n",
    "- Manually implement PCA and SVD.\n",
    "- Build an image compression algorithm.\n",
    "\n",
    "Below is an overview of the math behind principal component analysis (PCA) and a method to calculate it which is very common in many algorithms, the singular value decomposition (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [1. Load Dataset](#1.-Load-Dataset)\n",
    "- [2. Understand the Dataset](#2.-Understand-the-Dataset)\n",
    "- [3. Covariance of a Matrix](#3.-Covariance-of-a-Matrix)\n",
    "- [4. An Ideal Data Representation](#4.-An-Ideal-Data-Representation) (the math of PCA)\n",
    "- [5. Manually Coding PCA](#5.-Manually-Coding-PCA)\n",
    "- [6. Singular Value Decomposition (SVD) to do PCA](#svd)\n",
    "- [7. Manually Coding SVD](#7.-Manually-Coding-SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Faces Dataset\n",
    "---\n",
    "\n",
    "Let's load in the faces dataset from sklearn and pull out one of the faces.\n",
    "\n",
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make sure you always understand *why* the shape is what it is\n",
    "#   - There are 400 images of faces. (i.e. rows of data)\n",
    "#   - Each image is 64x64 pixels, for a total of 4096 pixels. (i.e. features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Understand the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.54132229,  0.58677685,  0.6404959 , ...,  0.09504132,\n",
       "        0.11157025,  0.11157025], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep in mind:\n",
    "#   (4096,) -- A 1-element tuple.\n",
    "#   (4096)  -- An integer.\n",
    "\n",
    "face10 = faces[10]\n",
    "print(face10.shape)\n",
    "\n",
    "# This is a grayscale image, so each pixel is from 0 (black) to 1 (white)\n",
    "face10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set up a couple of functions, one to process an image (reshape it to a 2D matrix) and the other to actually plot the image matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_image_matrix(face_image):\n",
    "    \"\"\" \n",
    "    Plots a grayscale 2-D ndarray `face_image`. \n",
    "        Pixel values can have any scale.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    \n",
    "    # max value (in magnitude)\n",
    "    max_pixel_value = max(face_image.max(), -face_image.min())\n",
    "    \n",
    "    ax.imshow(face_image, cmap=plt.cm.gray,\n",
    "              interpolation='nearest', \n",
    "              vmin=-max_pixel_value,\n",
    "              vmax=max_pixel_value)\n",
    "    \n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    \"\"\" \n",
    "    Standardize a 1-D 4096-element ndarray `image` to end up with:\n",
    "      - A new 2-D 64x64-element ndarray, with\n",
    "      - Pixel values between (-0.5, 0.5).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Min-Max scaling to end up with values between (-0.5, 0.5)\n",
    "    image = image - image.min()\n",
    "    image = (image / image.max()) - 0.5\n",
    "    \n",
    "    # In this case, converts the 4096-element `image` to 64x64\n",
    "    image = image.reshape((64, 64))       # The first parameter is a tuple\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "face10 = process_image(face10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Always make sure you understand how the image is stored. Given the image below and the image matrix above:\n",
    "- The top rows are light colored.\n",
    "          So, the first few rows of numbers are > 0.0\n",
    "- The final rows start dark and end dark. \n",
    "          So, the last few rows of numbers start and end < 0.0\n",
    "- If you print out the full final row, what would you expect the middle numbers to look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# process and plot face10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's pull out a matrix of pixels -- just the eye of face #40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# process and plot image 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This gets a 2D matrix - columns 5 up to 30 and pixel rows 10 up to 15.\n",
    "#    Knowing this, what would you predict eye.shape to be? Verify your answer.\n",
    "\n",
    "eye = process_image(faces[50])[5:30, 10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### 3. Correlation and Covariance of a Matrix\n",
    "\n",
    "In this section, we'll be treating each row of the eye image as an example. Each column will be a feature. So, the correlation between the columns will be how often all five columns change their pixel values together. In the eye image, notice the five columns are approximately the same across each row; so, we would expect them to have high correlation.\n",
    "\n",
    "#### 3.A. Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to intutively describe for the below correlation matrix:\n",
    "- Why is a correlation matrix diagonal? What do the 1s mean?\n",
    "- Why is this particular correlation matrix 5x5? What does each row/col correspond to in this case?\n",
    "- What does it mean that all correlations are highly positive, in terms of the pixels?\n",
    "- For PCA, would it be good or bad for each column of pixels to be highly correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.89679602,  0.84243288,  0.73569311,  0.81209014],\n",
       "       [ 0.89679602,  1.        ,  0.98238476,  0.7519262 ,  0.73714626],\n",
       "       [ 0.84243288,  0.98238476,  1.        ,  0.78569793,  0.73761145],\n",
       "       [ 0.73569311,  0.7519262 ,  0.78569793,  1.        ,  0.93985714],\n",
       "       [ 0.81209014,  0.73714626,  0.73761145,  0.93985714,  1.        ]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(eye, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's interpret this correlation matrix:\n",
    "\n",
    "- **`rowvar=False`, so the columns are treated as the variables/features.**\n",
    "  - Hence, we can consider each row to be a data point with five features -- in this case, pixels.\n",
    "\n",
    "\n",
    "- **What does it mean that these five pixels are highly correlated with each other?**\n",
    "  - When column A's pixel is lit, is column B's pixel also lit? When column A's pixel is off, is column B's pixel also off? If so, then A and B would have a positive correlation.\n",
    "  - Looking at the eye image, all five columns are either all on or all off (generally speaking). Hence, the five columns should have positive correlations.\n",
    "\n",
    "\n",
    "- **Verify your understanding:** The lowest correlation is the first column vs the fourth column (0.736). Looking at the image, would you expect these two to have the lowest correlation? If so, then you have evidence these results are correct!\n",
    "\n",
    "\n",
    "- **How does this relate to PCA?** A high correlation between columns means that our data representation is \"wasteful\". In the eye matrix, all five pixel columns nearly contain the same information. We could clearly just copy the first column five times and have a very similar image. Once PCA is performed, no two separate columns will be correlated (i.e. our correlation matrix will be the identity matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B. Covariance\n",
    "\n",
    "Now that you understand the correlation matrix, let's use these intuitions to look at the covariance matrix to gain a similar understanding! Also, try to intuitively understand how the correlation and covariance matrices are related -- it is the [standardized covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix#Correlation_matrix). (The correlation is restricted to the range [-1, 1].)\n",
    "\n",
    "The [covariance of a matrix $X$](https://en.wikipedia.org/wiki/Covariance_matrix) is defined\n",
    "\n",
    "### $$ Cov_X = \\frac{X^T X}{n-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Center the eye image - this is necessary for manual covariance calculation.\n",
    "\n",
    "# numpy defaults to assuming that the rows are the \"variables\" (and columns are observations).\n",
    "# So, setting rowvar=False is essential if we want to think about the columns \n",
    "# as the variables and calculate the covariance matrix accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# You can see that the manual calculation is the same as the numpy calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We use np.allclose() because you should never compare floats directly\n",
    "#    (For example, `0.1 == 0.4 - 0.3` is False!)\n",
    "\n",
    "# They are basically the same numbers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### 4. An Ideal Data Representation\n",
    "\n",
    "An \"ideal\" covariance matrix for data would have large numbers (variances) along the diagonal because this would indicate a large amount of signal in the data. It would also have zero values in the off-diagonal elements because these values indicate redundancy across our variables. \n",
    "\n",
    "Currently, our covariance matrix is not diagonal. You can see this clearly in that there are many non-zero elements outside of the diagonal. This means that our variables (columns of pixels) are correlated with each other. One way of thinking about this, and the reason that PCA seeks to decorrelate the data, is that correlation between our variables indicates that the variables are redundant - we don't need as many variables to represent the same amount of variance in the data.\n",
    "\n",
    "With PCA we seek a new representation of our data such that the columns, or variables, are uncorrelated. We also want the new variables ordered in importance, so that the variables maximize information in the data represented by variance in order. \n",
    "\n",
    "---\n",
    "\n",
    "#### Defining the new representation of our data, $Y$\n",
    "\n",
    "But how do we do this? We will derive a transformation on our data so that the transformed data has the \"ideal\" covariance matrix described before. Our new representation of the information will have no correlation between variables and the variables will be ordered by importance.\n",
    "\n",
    "We can define new matrices $Y$ and $P$ such that:\n",
    "\n",
    "#### $$ Y = XP $$\n",
    "\n",
    "$Y$ will be our new representation of the data, where the covariance of matrix $Y$ is diagonal (meaning the columns of Y are uncorrelated).\n",
    "\n",
    "#### $$ Cov_Y = \\frac{Y^{T}Y}{n-1} \\text{ is a diagonal matrix} $$\n",
    "\n",
    "PCA assumes that the matrix $P$ will be \"orthonormal\", meaning that the columns of $P$ are orthogonal as well as normalized. $P$ is the matrix that will perform the rotation of the $X$ matrix into the new axis space for $Y$. $P$ will be orthonormal because of how PCA re-characterizes the data:\n",
    "\n",
    "1. Find a $p_1$ direction that maximizes the variance in $X$.\n",
    "2. Find a $p_2$ that maximizes variance in $X$ _such that $p_2$ is orthogonal to $p_1$_.\n",
    "3. Find a $p_3$ that maximizes variance in $X$ _such that $p_3$ is orthogonal to $p_1$ and $p_2$._\n",
    "4. etc. for all $p$ columns.\n",
    "\n",
    "We can rewrite the covariance of $Y$ as a function of $P$ and the covariance of $X$:\n",
    "\n",
    "### $$ Cov_Y = \\frac{Y^{T}Y}{n-1} \\\\\n",
    "Cov_Y = \\frac{(XP)^{T}(XP)}{n-1} \\\\\n",
    "Cov_Y = \\frac{P^T X^T X P}{n-1} \\\\\n",
    "Cov_Y = P^T\\left(\\frac{X^TX}{n-1}\\right)P \\\\\n",
    "Cov_Y = P^TCov_X P$$\n",
    "\n",
    "So where do we go from here? Luckily, any symmetric matrix, such as a covariance matrix, has the property that it's eigenvectors are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### If a matrix is orthogonally diagonalizable it is also symmetric\n",
    "\n",
    "Say we have matrix $A$ and its transpose $A^T$. If a matrix is symmetric, it means that it is equal to its transpose, $A = A^T$. Say we also have a diagonal matrix $D$ and a matrix $E$ such that:\n",
    "\n",
    "### $$ A = EDE^T $$\n",
    "\n",
    "We can write out the transpose of $A$, $A^T$ as:\n",
    "\n",
    "### $$ A^T = (EDE^T)^T $$\n",
    "\n",
    "The transpose of dot products of matrices is equivalent to the product of their transpose in reverse order. For example, $(AB)^T = B^T A^T$.\n",
    "\n",
    "### $$ A^T = E^{TT}D^TE^T \\\\\n",
    "A^T = ED^TE^T$$\n",
    "\n",
    "We have already defined $D$ to be a diagonal matrix, so by definition $D^T = D$. With this you can see that $A^T$ is in fact equal to $A$:\n",
    "\n",
    "### $$ A^T = EDE^T  = A$$\n",
    "\n",
    "And so if $A$ is equal to its transpose, it is orthogonally diagonalizeable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### The diagonalization of a symmetric matrix is done using its eigenvectors\n",
    "\n",
    "Again, $A$ is our symmetric matrix such that $A = A^T$. Now we define the matrix $E$ to be the matrix of eigenvectors of $A$. \n",
    "\n",
    "Recall that the eigenvectors of a matrix are the vectors that do not change direction when multiplied by the matrix, only scaled according to their corresponding eigenvalues. An eigenvector $e_i$ and its corresponding eigenvalue $\\lambda_i$ are described:\n",
    "\n",
    "#### $$ Ae_i = \\lambda_i e_i $$\n",
    "\n",
    "Likewise, the determinant of a matrix $A$ minus its eigenvalues times the identity matrix $I$ will equal zero:\n",
    "\n",
    "#### $$ det(A - \\lambda I) = 0 $$\n",
    "\n",
    "You can think of the determinant as a \"signed volume\" of the space defined by $A$, and the eigenvalues represent the scaling of the matrix $A$ relative to the unit volume.\n",
    "\n",
    "We can write out the equation:\n",
    "\n",
    "#### $$ AE = ED $$\n",
    "\n",
    "Because this is equivalent to the definition of the eigenvectors and eigenvalues above. $A$ multiplied by the matrix of eigenvectors is equivalent to the matrix of eigenvectors multiplied by a diagonal matrix that represent the eigenvalues $\\lambda$.\n",
    "\n",
    "Remember that if $A$ is symmetric, $A = A^T$. Write out two different equations with two different eigenvalue and eigenvector pairs:\n",
    "\n",
    "### $$ A e_1 = \\lambda_1 e_1 \\\\\n",
    "A e_2 = \\lambda_2 e_2 $$\n",
    "\n",
    "Pre-multiply the first equation by the transpose of $e_2$ and the second equation by the transpose of $e_1$:\n",
    "\n",
    "### $$ e_2^T A e_1 = \\lambda_1 e_2^T  e_1 \\\\\n",
    "e_1^T A e_2 = \\lambda_2 e_1^T  e_2 $$\n",
    "\n",
    "We know that $A$ is symmetric, so (with just the first equation) we can plug in $A^T$:\n",
    "\n",
    "### $$ e_2^T A^T e_1 = \\lambda_1 e_2^T e_1 $$\n",
    "\n",
    "Using the rule that $(AB)^T = B^T A^T$:\n",
    "\n",
    "### $$ (A e_2)^T e_1 =  \\lambda_1 e_2^T e_1 $$\n",
    "\n",
    "Substituting in the eigenvalue and eigenvector representation for $A e_2$:\n",
    "\n",
    "### $$ (\\lambda_2 e_2)^T e_1 = \\lambda_1 e_2^T e_1 \\\\\n",
    "\\lambda_2 e_2^T e_1 = \\lambda_1 e_2^T e_1$$\n",
    "\n",
    "And so,\n",
    "\n",
    "### $$ (\\lambda_2 - \\lambda_1)(e_2^T e_1) = 0 $$\n",
    "\n",
    "And therefore $e_2^T e_1$ must equal 0 (the eigenvalues are unique), and this holds for any of the eigenvectors, not just the first two. The eigenvectors of the symmetric matrix are therefore orthogonal.\n",
    "\n",
    "Let's go back to the equation we had at the beginning,\n",
    "\n",
    "### $$ AE = ED $$\n",
    "\n",
    "We can rewrite this as:\n",
    "\n",
    "### $$ A = EDE^{-1} $$\n",
    "\n",
    "A property of an orthogonal matrix is that its inverse is also its transpose: $E^T = E^{-1}$. As we have just shown, our matrix $E$ of eigenvectors is in fact orthogonal for this symmetric matrix $A$. Using this fact we can finally rewrite the equation as:\n",
    "\n",
    "### $$ A = EDE^T $$\n",
    "\n",
    "And we know that a symmetric matrix is diagonalized by its eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### Diagonalizing the covariance of matrix $Y$ is done using the eigenvalues of the covariance of $X$\n",
    "\n",
    "Now lets go all the way back to our equation for the covariance of our desired $Y$ matrix, which we are going to make diagonal:\n",
    "\n",
    "### $$ Cov_Y = P^TCov_X P$$\n",
    "\n",
    "Remember, we want $Cov_Y$ to be a diagonal matrix. How should we do this? Our desired outcome is:\n",
    "\n",
    "### $$ Cov_Y = D$$\n",
    "\n",
    "And so we need to set the right-hand side of the equation to equal $D$:\n",
    "\n",
    "### $$ P^TCov_X P = D $$\n",
    "\n",
    "If we set the columns of $P$ to be the eigenvectors of the covariance matrix of $X$, then $P$ is equivalent to the $E$ in our formula for the symmetric matrix $A$ before.\n",
    "\n",
    "We can plug this in to the formula in place of $Cov_X$:\n",
    "\n",
    "### $$ P^T (PDP^T) P = D \\\\\n",
    "(P^TP)D(P^TP) = D $$\n",
    "\n",
    "Because the eigenvectors $P$ are orthogonal, $P^T = P^{-1}$ and so:\n",
    "\n",
    "### $$ (P^{-1}P) D (P^{-1}P) = D \\\\\n",
    "D = D $$\n",
    "\n",
    "And therefore we know that the covariance matrix of $Y$ is a diagonal matrix if we set $P$ to be the eigenvalues of the covariance matrix of $X$:\n",
    "\n",
    "### $$ Cov_Y = D $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Manually Coding PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can do this in code to prove it to ourselves. Below I will do this process for the `eye_cent` matrix as `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# calculate the covariance matrix using the centerd eye image\n",
    "X = eye_cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot the cov matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use numpy's linalg.eigh to get the eigenvalues and vectors for a symmetric matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sort the eigenvectors by largest to smallest eigenvalue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform the dot product of X with the eigenvectors to get Y:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compute the covariance for Y to verify that the columns of Y are indeed uncorrelated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The image of the covariance matrix shows that the off-diagonal elements are\n",
    "# all essentially zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# You can also see that the rightmost column of Y contains the most variance, though these\n",
    "# new variables no longer look like an eye, of course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Be able to verify it here with the actual eye\n",
    "#   Does it make sense that the leftmost would have the most variance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"svd\"></a>\n",
    "### 6. Singular Value Decomposition (SVD) to do PCA\n",
    "\n",
    "Singular value decomposition (SVD) is the most common method for performing PCA. It is similar to the eigendecomposition of the covariance matrix done in the previous section. \n",
    "\n",
    "Say we again have matrix $X$ with variables in columns, as well as the square matrix defined by $X^TX$.\n",
    "\n",
    "If $v_i$ is an orthonormal eigenvector of $X^TX$ and $\\lambda_i$ its corresponding eigenvalue, we have as before:\n",
    "\n",
    "### $$ X^TX v_i = \\lambda_i v_i $$\n",
    "\n",
    "If you instead multiply the matrix $X$ by the eigenvectors of its covariance matrix you have the following relation:\n",
    "\n",
    "### $$ X v_i = \\sigma_i u_i $$\n",
    "\n",
    "Where $\\sigma_i = \\sqrt{\\lambda_i}$ and $u_i = \\frac{1}{\\sigma_i}X v_i$.\n",
    "\n",
    "The $U$ and $V$ matrices of vectors are both orthonormal and define a set of new bases for $X$ (a new rotated set of axes).\n",
    "\n",
    "You can represent the \"singular values\" $\\sigma$ with the diagonal matrix $\\Sigma$, where:\n",
    "\n",
    "### $$ \\Sigma = \\begin{pmatrix}\\sigma_{1} \\\\\n",
    "& \\sigma_{2} \\\\\n",
    "& & \\ddots \\\\\n",
    "& & & \\sigma_{n}\\end{pmatrix} $$\n",
    "\n",
    "The SVD is then written as:\n",
    "\n",
    "### $$ XV = U \\Sigma \\\\\n",
    "X = U \\Sigma V^{-1} \\\\\n",
    "X = U \\Sigma V^T $$\n",
    "\n",
    "\n",
    "Graphically we can show this in both the single eigenvector case and the full matrix of eigenvector case. For the equation with one eigenvector below:\n",
    "\n",
    "### $$ X v_i = \\sigma_i u_i $$\n",
    "\n",
    "looks like:\n",
    "\n",
    "### $$ \\begin{pmatrix}x_{1,1} \\\\\n",
    "& \\ddots \\\\\n",
    "& & x_{n,m}\\end{pmatrix} \\cdot \\begin{pmatrix} v_{1} \\\\ \\ddots \\\\ v_{m} \\end{pmatrix} = \\sigma \\cdot \\begin{pmatrix} u_{1} \\\\ \\ddots \\\\ u_{n} \\end{pmatrix}$$\n",
    "\n",
    "\n",
    "And likewise the equation with full matrices $V$ and $U$,\n",
    "\n",
    "### $$ XV = U \\Sigma $$\n",
    "\n",
    "looks like:\n",
    "\n",
    "### $$ \\begin{pmatrix}x_{1,1} \\\\\n",
    "& \\ddots \\\\\n",
    "& & x_{n,m}\\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix} v_{1,1} \\\\ \n",
    "& \\ddots \\\\ \n",
    "& & v_{m,m} \\end{pmatrix} = \n",
    "\\begin{pmatrix} u_{1,1} \\\\ \n",
    "& \\ddots \\\\ \n",
    "& & u_{n,n} \\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix} \\sigma_{1,1} \\\\ \n",
    "& \\ddots \\\\ \n",
    "& & \\sigma_{n,m} \\end{pmatrix} $$\n",
    "\n",
    "Keep in mind that the matrices $V$ and $U$ will typically have to be \"filled in\" with extra orthogonal vectors to compensate for the fact that $X$ is not a square matrix.\n",
    "\n",
    "So how does one use SVD to perform PCA? As it turns out, the matrix $V$ is equivalent to the $P$ that we calculated before. It is the matrix that transforms the information contained in $X$ to a new basis in which the columns are orthogonal to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Manually Coding SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform the SVD on X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print out V, the transpose of the VT that comes out of np.inalg.svd:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These are the same as the ordered eigenvectors that we calculated before\n",
    "# (some of the signs can be flipped due to slight differences in calculation,\n",
    "# but the magnitudes are equivalent which is the important part.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the benefits of using the SVD formulation is that it makes it fairly straightforward to reconstruct a version of our original matrix $X$ using only some of the principal components. The first set of principal components often accounts for a large amount of the variance in the data, and so you can get back most of the original dataset with a fraction of the dimensionality.\n",
    "\n",
    "Let's first do the SVD and visualize the matrices for our `eye_cent` image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The original image of the eye: (centered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# perform an SVD on the eye_cent data. The argument full_matrices=False only \n",
    "# returns the relevant parts of U and VT (remember that SVD will fill in orthogonal vectors\n",
    "# if X is not square).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot the U matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot_image_matrix for S, the singular values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can reconstruct the original image exactly using the formula $X = U\\Sigma V^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# calculate X using the above SVD formula\n",
    "\n",
    "# plot_image_matrix the reconstructed X matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "However, we can do the same with only two of the singular value dimensions. Because the singular values are ordered by the variance they account for, we can retain a lot of the original image just using a fraction of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of principal components\n",
    "pc = 2 \n",
    "\n",
    "# modify the above code for the SVD formula and use 'pc' to generalize it\n",
    "# so that you can specify an arbitrary number of principle component \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is even more apparent with a full face image. Below is the full face #50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAI8CAYAAAD1D3GaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13322110>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 286,
       "width": 286
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "face = process_image(faces[50])\n",
    "plot_image_matrix(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next I do SVD on the face image and look at the singular value matrix to get a feel for how many components are actually important (account for reasonable variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# do SVD on face\n",
    "\n",
    "# transform S into a diag matrix\n",
    "\n",
    "# plot_image_matrix S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looks like the first 5 or 6 have a decent proportion of all the variance across the pixel columns. We can look at what the face reconstruction looks like just using those first 5 singular values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pc = 5\n",
    "# reconstruct the image faces[50] by using only the first principal components\n",
    "\n",
    "# plot the reconstructed image\n",
    "\n",
    "# plot the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see how reconstruction is affected by different PCs, try adjusting `pc`. What is the smallest value of `pc` such that the image has minimal loss?\n",
    "\n",
    "Congratulations, you just wrote an image compressor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
