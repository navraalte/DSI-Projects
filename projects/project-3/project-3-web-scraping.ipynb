{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3:  Web Scraping\n",
    "### Finding Underpriced RVs on Craigslist\n",
    "\n",
    "![](https://snag.gy/WrdUMx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will be practicing our web scraping skills.  You can use Scrapy or Python requests in order to complete this project.  It may be helpful to write some prototype code in this notebook to test your assumptions, then move it into a Python file that can be run from the command line.\n",
    "\n",
    "> In order to run code from the command line, instead of the notebook, you just need to save your code to a file (with a .py extension), and run it using the Python interpreter:<br><br>\n",
    "> `python my_file.py`\n",
    "\n",
    "You will be building a process to scrape a single category of search results on Craigslist, that can easily be applied to other categories by changing the search terms.  The main goal is to be able to target and scrape a single page given a set of parameters.\n",
    "\n",
    "**If you use Scrapy, provide your code in a folder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your libraries for scrapy / requests / pandas / numpy / etc\n",
    "Setup whichever libraries you need. Review past material for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>List of United States cities by population - Wiki'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPARE REQUIRED LIBRARIES\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "# data modules\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# make sure charts appear in the notebook:\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")\n",
    "HTML = response.text  \n",
    "HTML[0:150]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.  Scrape for the largest US cities (non-exhaustive list)\n",
    "Search, research, and scrape Wikipedia for a list of the largest US cities.  There are a few sources but find one that is in a nice table.  We don't want all cities, just signifficant cities.  Examine your source.  Look for what can be differentiable.\n",
    "\n",
    "- Use requests\n",
    "- Build XPath query(ies)\n",
    "- Extract to a list\n",
    "- Clean your list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = Selector(text = HTML).xpath(\"//td[2]/text()|  //td[2]/a/text()\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0',\n",
       " u'1',\n",
       " u'10',\n",
       " u'10',\n",
       " u'12',\n",
       " u'192',\n",
       " u'2',\n",
       " u'22',\n",
       " u'3',\n",
       " u'38',\n",
       " u'4',\n",
       " u'5',\n",
       " u'51',\n",
       " u'54',\n",
       " u'6',\n",
       " u'7',\n",
       " u'73',\n",
       " u'8',\n",
       " u'9',\n",
       " u'Abilene',\n",
       " u'Akron',\n",
       " u'Alexandria',\n",
       " u'Allentown',\n",
       " u'Amarillo',\n",
       " u'Anaheim',\n",
       " u'Ann Arbor',\n",
       " u'Antioch',\n",
       " u'Arlington',\n",
       " u'Arvada',\n",
       " u'Athens',\n",
       " u'Augusta',\n",
       " u'Aurora',\n",
       " u'Aurora',\n",
       " u'Bakersfield',\n",
       " u'Bayam\\xf3n',\n",
       " u'Beaumont',\n",
       " u'Bellevue',\n",
       " u'Berkeley',\n",
       " u'Boulder',\n",
       " u'Broken Arrow',\n",
       " u'Brownsville',\n",
       " u'Buffalo',\n",
       " u'Burbank',\n",
       " u'Caguas',\n",
       " u'California',\n",
       " u'Cambridge',\n",
       " u'Cape Coral',\n",
       " u'Carlsbad',\n",
       " u'Carolina',\n",
       " u'Carrollton',\n",
       " u'Cary',\n",
       " u'Cedar Rapids',\n",
       " u'Centennial',\n",
       " u'Chandler',\n",
       " u'Chattanooga',\n",
       " u'Chesapeake',\n",
       " u'Chula Vista',\n",
       " u'Cincinnati',\n",
       " u'Clarksville',\n",
       " u'Clearwater',\n",
       " u'Cleveland',\n",
       " u'Clinton',\n",
       " u'Clovis',\n",
       " u'College Station',\n",
       " u'Colorado Springs',\n",
       " u'Columbia',\n",
       " u'Columbus',\n",
       " u'Concord',\n",
       " u'Coral Springs',\n",
       " u'Corona',\n",
       " u'Corpus Christi',\n",
       " u'Costa Mesa',\n",
       " u'Dallas',\n",
       " u'Daly City',\n",
       " u'Davenport',\n",
       " u'Davie',\n",
       " u'Dayton',\n",
       " u'Delaware',\n",
       " u'Denton',\n",
       " u'Downey',\n",
       " u'Durham',\n",
       " u'Edison',\n",
       " u'El Cajon',\n",
       " u'El Monte',\n",
       " u'El Paso',\n",
       " u'Elgin',\n",
       " u'Elizabeth',\n",
       " u'Elk Grove',\n",
       " u'Escondido',\n",
       " u'Eugene',\n",
       " u'Evansville',\n",
       " u'Everett',\n",
       " u'Fairfield',\n",
       " u'Fayetteville',\n",
       " u'Florida',\n",
       " u'Fontana',\n",
       " u'Fort Collins',\n",
       " u'Fort Lauderdale',\n",
       " u'Fort Wayne',\n",
       " u'Fort Worth',\n",
       " u'Fremont',\n",
       " u'Fresno',\n",
       " u'Frisco',\n",
       " u'Fullerton',\n",
       " u'Gainesville',\n",
       " u'Garden Grove',\n",
       " u'Garland',\n",
       " u'Gilbert',\n",
       " u'Glendale',\n",
       " u'Glendale',\n",
       " u'Grand Prairie',\n",
       " u'Grand Rapids',\n",
       " u'Greeley',\n",
       " u'Green Bay',\n",
       " u'Greensboro',\n",
       " u'Gresham',\n",
       " u'Hampton',\n",
       " u'Hayward',\n",
       " u'Henderson',\n",
       " u'Hialeah',\n",
       " u'High Point',\n",
       " u'Hillsboro',\n",
       " u'Hollywood',\n",
       " u'Huntington Beach',\n",
       " u'Huntsville',\n",
       " u'Independence',\n",
       " u'Indiana',\n",
       " u'Indiana',\n",
       " u'Inglewood',\n",
       " u'Irvine',\n",
       " u'Irving',\n",
       " u'Jersey City',\n",
       " u'Joliet',\n",
       " u'Jurupa Valley',\n",
       " u'Kansas City',\n",
       " u'Kent',\n",
       " u'Killeen',\n",
       " u'Knoxville',\n",
       " u'Lafayette',\n",
       " u'Lakeland',\n",
       " u'Lakewood',\n",
       " u'Lakewood',\n",
       " u'Lancaster',\n",
       " u'Laredo',\n",
       " u'Las Cruces',\n",
       " u'League City',\n",
       " u'Lewisville',\n",
       " u'Lexington',\n",
       " u'Long Beach',\n",
       " u'Louisiana',\n",
       " u'Lowell',\n",
       " u'Lubbock',\n",
       " u'Macon',\n",
       " u'Massachusetts',\n",
       " u'Massachusetts',\n",
       " u'Massachusetts',\n",
       " u'Massachusetts',\n",
       " u'McAllen',\n",
       " u'McKinney',\n",
       " u'Memphis',\n",
       " u'Mesa',\n",
       " u'Mesquite',\n",
       " u'Miami',\n",
       " u'Miami Gardens',\n",
       " u'Michigan',\n",
       " u'Michigan',\n",
       " u'Michigan',\n",
       " u'Midland',\n",
       " u'Minnesota',\n",
       " u'Miramar',\n",
       " u'Missouri',\n",
       " u'Mobile',\n",
       " u'Modesto',\n",
       " u'Moreno Valley',\n",
       " u'Murfreesboro',\n",
       " u'Murrieta',\n",
       " u'Naperville',\n",
       " u'Nevada',\n",
       " u'Nevada',\n",
       " u'Nevada',\n",
       " u'Nevada',\n",
       " u'New Haven',\n",
       " u'New Jersey',\n",
       " u'New Jersey',\n",
       " u'New York',\n",
       " u'New York',\n",
       " u'New York',\n",
       " u'New York',\n",
       " u'Newport News',\n",
       " u'Norfolk',\n",
       " u'Norman',\n",
       " u'North Charleston',\n",
       " u'North Las Vegas',\n",
       " u'Norwalk',\n",
       " u'Oakland',\n",
       " u'Oceanside',\n",
       " u'Odessa',\n",
       " u'Ohio',\n",
       " u'Ohio',\n",
       " u'Ohio',\n",
       " u'Olathe',\n",
       " u'Ontario',\n",
       " u'Orange',\n",
       " u'Orlando',\n",
       " u'Overland Park',\n",
       " u'Oxnard',\n",
       " u'Palm Bay',\n",
       " u'Palmdale',\n",
       " u'Pasadena',\n",
       " u'Pasadena',\n",
       " u'Paterson',\n",
       " u'Pearland',\n",
       " u'Pembroke Pines',\n",
       " u'Pennsylvania',\n",
       " u'Pennsylvania',\n",
       " u'Pennsylvania',\n",
       " u'Pennsylvania',\n",
       " u'Peoria',\n",
       " u'Peoria',\n",
       " u'Pittsburgh',\n",
       " u'Plano',\n",
       " u'Pomona',\n",
       " u'Pompano Beach',\n",
       " u'Ponce',\n",
       " u'Port St. Lucie',\n",
       " u'Provo',\n",
       " u'Pueblo',\n",
       " u'Rancho Cucamonga',\n",
       " u'Reno',\n",
       " u'Renton',\n",
       " u'Rialto',\n",
       " u'Richardson',\n",
       " u'Richmond',\n",
       " u'Riverside',\n",
       " u'Rochester',\n",
       " u'Rochester',\n",
       " u'Rockford',\n",
       " u'Roseville',\n",
       " u'Round Rock',\n",
       " u'Salinas',\n",
       " u'San Angelo',\n",
       " u'San Antonio',\n",
       " u'San Bernardino',\n",
       " u'San Diego',\n",
       " u'San Francisco',\n",
       " u'San Jose',\n",
       " u'San Mateo',\n",
       " u'Sandy Springs',\n",
       " u'Santa Ana',\n",
       " u'Santa Clara',\n",
       " u'Santa Clarita',\n",
       " u'Santa Maria',\n",
       " u'Santa Rosa',\n",
       " u'Savannah',\n",
       " u'Scottsdale',\n",
       " u'Shreveport',\n",
       " u'Simi Valley',\n",
       " u'South Bend',\n",
       " u'Spokane',\n",
       " u'Springfield',\n",
       " u'Springfield',\n",
       " u'St. Louis',\n",
       " u'St. Petersburg',\n",
       " u'Stamford',\n",
       " u'Sterling Heights',\n",
       " u'Stockton',\n",
       " u'Sunnyvale',\n",
       " u'Surprise',\n",
       " u'Syracuse',\n",
       " u'Tacoma',\n",
       " u'Tampa',\n",
       " u'Temecula',\n",
       " u'Tempe',\n",
       " u'Thornton',\n",
       " u'Thousand Oaks',\n",
       " u'Toledo',\n",
       " u'Torrance',\n",
       " u'Tucson',\n",
       " u'Tulsa',\n",
       " u'Tyler',\n",
       " u'Vallejo',\n",
       " u'Vancouver',\n",
       " u'Ventura',\n",
       " u'Victorville',\n",
       " u'Virginia',\n",
       " u'Virginia',\n",
       " u'Virginia',\n",
       " u'Visalia',\n",
       " u'Vista',\n",
       " u'Waco',\n",
       " u'Warren',\n",
       " u'Waterbury',\n",
       " u'West Covina',\n",
       " u'West Jordan',\n",
       " u'West Palm Beach',\n",
       " u'West Valley City',\n",
       " u'Westminster',\n",
       " u'Wichita Falls',\n",
       " u'Wilmington',\n",
       " u'Winston\\u2013Salem',\n",
       " u'Wisconsin',\n",
       " u'Woodbridge',\n",
       " u'Worcester',\n",
       " u'Yonkers']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RETAIN PROPERLY FORMED CITIES WITH FILTERING FUNCTION\n",
    "cities =[item.replace(u'â€™',\"\") for item in cities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Antonio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>San Diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dallas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>San Jose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0    San Antonio\n",
       "1      San Diego\n",
       "2         Dallas\n",
       "3       San Jose\n",
       "4  San Francisco"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cities).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Only retain cities with properly formed ASCII\n",
    "\n",
    "Optionally, filter out any cities with impropper ASCII characters.  A smaller list will be easier to look at.  However you may not need to filter these if you spend more time scraping a more concise city list.  This list should help you narrow down the list of regional Craigslist sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Write a function to capture current pricing information via Craigslist in one city.\n",
    "Choose a city from your scraped data, then go to the cooresponding city section on Craigslist, searching for \"rv\" in the auto section.  Write a method that pulls out the prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\ufeff<!DOCTYPE html>\\n\\n<html class=\"no-js\"><head>\\n    <title>las vegas recreational vehicles  - craigslist</title>\\n\\n    <meta name=\"description\" content=\"l'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = requests.get(\"https://lasvegas.craigslist.org/search/rva\")\n",
    "CL = response2.text  \n",
    "CL[0:150]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prices = Selector(text = CL).xpath(\"//span[@class='result-meta']/span[@class='result-price']/text() \").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "details = Selector(text = CL).xpath(\"//a[@class='result-title hdrlnk']/text()\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(prices).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008 Coachmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998 Monaco Windsor 38' diesel Pusher trade fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra clean travel trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001 FourWinds Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dometic model 320 with spray</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                      2008 Coachmen\n",
       "1  1998 Monaco Windsor 38' diesel Pusher trade fo...\n",
       "2                         Extra clean travel trailer\n",
       "3                           2001 FourWinds Hurricane\n",
       "4                       Dometic model 320 with spray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(details).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 106 entries, 0 to 105\n",
      "Data columns (total 1 columns):\n",
      "0    106 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 920.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert from unicode\n",
    "def clean_text(row):\n",
    "    \n",
    "    return [r.decode('unicode_escape').encode('ascii', 'ignore') for r in row]\n",
    "\n",
    "df[0] = df.apply(clean_text)\n",
    "\n",
    "type(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$28500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$18900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0   $7500\n",
       "1  $28500\n",
       "2   $6900\n",
       "3  $18900\n",
       "4    $210"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Dollar sign, convert to int \n",
    "df[0] = df[0].str.replace(',', '')\n",
    "df[0] = df[0].str.replace('$', '')\n",
    "df[0] = df[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7500\n",
       "1    28500\n",
       "2     6900\n",
       "3    18900\n",
       "4      210\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011999"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18981.122641509435"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Create a mapping of cities to cooresponding regional Craigslist URLs\n",
    "\n",
    "Major US cities on Craigslist typically have their own cooresponding section (ie: SFBay Area, NYC, Boston, Miami, Seattle, etc).  Later, you will use these to query search results for various metropolitian regions listed on Craigslist.  Between the major metropolitan Craigslist sites, the only thing that will differ is the URL's that correspond to them.\n",
    "\n",
    "The point of the \"mapping\":  Create a data structure that allows you to iterate with both the name of the city from Wikipedia, with the cooresponding variable that that will allow you to construct each craigslist URL for each region.\n",
    "\n",
    "> For San Francsico (the Bay Area metropolitan area), the url for the RV search result is:\n",
    "> http://sfbay.craigslist.org/search/sss?query=rv\n",
    ">\n",
    "> The convention is http://[region].craigslist.org/search/sss?query=rf\n",
    "> Replacing [region] with the cooresponding city name will allow you to quickly iterate through each regional Craigslist site, and scrape the prices from the search results.  Keep this in mind while you build this \"mapping\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miami',\n",
       " 'saltlakecity',\n",
       " 'anchorage',\n",
       " 'cleveland',\n",
       " 'baltimore',\n",
       " 'houston',\n",
       " 'newyork',\n",
       " 'phoenix',\n",
       " 'denver',\n",
       " 'detroit',\n",
       " 'seattle',\n",
       " 'portland',\n",
       " 'sfbay',\n",
       " 'boston']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_cities= {\"New_York\": \"newyork\", \"Alaska\": \"anchorage\", \"Arizona\": \"phoenix\",\n",
    "               \"Californa\": \"sfbay\", \"Texas\": \"houston\", \"Colorado\": \"denver\" , \n",
    "               \"Utah\": \"saltlakecity\", \"Maryland\": \"baltimore\", \"Massachusetts\": \"boston\",\n",
    "               \"Washington\": \"seattle\",\"Ohio\": \"cleveland\", \"Flordia\": \"miami\",\n",
    "               \"Michigan\": \"detroit\", \"Oregon\":\"portland\" }\n",
    "\n",
    "large_cities = major_cities.values()\n",
    "large_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchorage': 'https://anchorage.craigslist.org/search/cta?query=rv',\n",
       " 'baltimore': 'https://baltimore.craigslist.org/search/cta?query=rv',\n",
       " 'boston': 'https://boston.craigslist.org/search/cta?query=rv',\n",
       " 'cleveland': 'https://cleveland.craigslist.org/search/cta?query=rv',\n",
       " 'denver': 'https://denver.craigslist.org/search/cta?query=rv',\n",
       " 'detroit': 'https://detroit.craigslist.org/search/cta?query=rv',\n",
       " 'houston': 'https://houston.craigslist.org/search/cta?query=rv',\n",
       " 'miami': 'https://miami.craigslist.org/search/cta?query=rv',\n",
       " 'newyork': 'https://newyork.craigslist.org/search/cta?query=rv',\n",
       " 'phoenix': 'https://phoenix.craigslist.org/search/cta?query=rv',\n",
       " 'portland': 'https://portland.craigslist.org/search/cta?query=rv',\n",
       " 'saltlakecity': 'https://saltlakecity.craigslist.org/search/cta?query=rv',\n",
       " 'seattle': 'https://seattle.craigslist.org/search/cta?query=rv',\n",
       " 'sfbay': 'https://sfbay.craigslist.org/search/cta?query=rv'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_dict = {reigon: \"https://\"+reigon+\".craigslist.org/search/cta?query=rv\" \n",
    "            for reigon in large_cities }\n",
    "url_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Define a function to caculate mean and median price per city.\n",
    "\n",
    "Now that you've created a list of cities you want to scrape, adapt your solution for grabbing data in one region site, to grab data for all regional sites that you collected, then calculate the mean and median price of RV results from each city.\n",
    "\n",
    "> Look at the URLs from a few different regions (ie: portland, phoenix, sfbay), and find what they have in common.  Determine the area in the URL string that needs to change the least, and figure out how to replace only that portion of the URL in order to iterate through each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def city_rv_stats(city, url_dict): \n",
    "    url = url_dict[city]\n",
    "    response = requests.get(url)\n",
    "    HTML = response.text\n",
    "    \n",
    "    # gen list of prices \n",
    "    prices = Selector(text=HTML).xpath(\"//span/span[@class='result-price']/text()\").extract()\n",
    "    for i, amt in enumerate(prices):\n",
    "        if \"$\" in amt:\n",
    "            amt = amt.replace(\"$\",\"\")\n",
    "        prices[i] = int(amt)\n",
    "        \n",
    "    # remove outliers\n",
    "    for i, amt in enumerate(prices):\n",
    "        if (amt < 250) or (amt > 300000):\n",
    "            del prices[i]\n",
    "    \n",
    "    mean = np.mean(prices)\n",
    "    median = int(np.median(prices)) \n",
    "    std = np.std(prices)\n",
    "    \n",
    "    # Control outliers by STd.\n",
    "    for i, amt in enumerate(prices):\n",
    "        if np.abs(amt - mean) > 3*std:\n",
    "            del prices[i]\n",
    "            # update these metrics\n",
    "            std = np.std(prices)\n",
    "            mean = np.mean(prices)\n",
    "            \n",
    "    # return the price dict\n",
    "    return {\"city\":city, \"mean\":int(mean), \"std\": int(std),\n",
    "            \"max\": np.max(prices), \"median\":median, \"min\": np.min(prices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'sfbay',\n",
       " 'max': 59763,\n",
       " 'mean': 14700,\n",
       " 'median': 10000,\n",
       " 'min': 500,\n",
       " 'std': 13294}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "ny = city_rv_stats(\"sfbay\", url_dict)\n",
    "ny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Run your scraping process, and save your results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"/Users/NVR/Desktop/dsi-sf-7-materials-nvr/projects/project-3/craigslist/crawlcl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_market = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>detail</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$24500</td>\n",
       "      <td>Remodeled 25' Mallard Sprinter RV</td>\n",
       "      <td>las vegas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$4000</td>\n",
       "      <td>Pop up camper for sale</td>\n",
       "      <td>bakersfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$26500</td>\n",
       "      <td>Trade for Class A Motor-home With Slides</td>\n",
       "      <td>flagstaff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$15000</td>\n",
       "      <td>2004 FLEETWOOD PIONEER &amp; 2006 CHEVY SILVERADO</td>\n",
       "      <td>imperial co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$300000</td>\n",
       "      <td>2008 Country Coach Affinity 45'</td>\n",
       "      <td>inland empire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price                                         detail           area\n",
       "0   $24500              Remodeled 25' Mallard Sprinter RV      las vegas\n",
       "1    $4000                         Pop up camper for sale    bakersfield\n",
       "2   $26500       Trade for Class A Motor-home With Slides      flagstaff\n",
       "3   $15000  2004 FLEETWOOD PIONEER & 2006 CHEVY SILVERADO    imperial co\n",
       "4  $300000                2008 Country Coach Affinity 45'  inland empire"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv_market.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_market['price'] = rv_market['price'].str.replace(',', '')\n",
    "rv_market['price'] = rv_market['price'].str.replace('$', '')\n",
    "rv_market['price'] = rv_market['price'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_market = rv_market[rv_market['price'] > 1000]  #remove low values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_market = rv_market[rv_market['price'] < 300000] #remove high values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Do an analysis of the RV market.\n",
    "\n",
    "Go head we'll wait.  Anything notable about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_county_rv_sales = pd.DataFrame(rv_market.groupby(['area'])['price'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NVR/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>skagit</th>\n",
       "      <td>1453535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>victoria, BC</th>\n",
       "      <td>1394773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moses lake</th>\n",
       "      <td>1297787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri-cities, WA</th>\n",
       "      <td>1287679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salem</th>\n",
       "      <td>1197890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  price\n",
       "area                   \n",
       "skagit          1453535\n",
       "victoria, BC    1394773\n",
       "moses lake      1297787\n",
       "tri-cities, WA  1287679\n",
       "salem           1197890"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_county_rv_sales.sort([\"price\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NVR/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fredericksburg</th>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norfolk</th>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syracuse</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washington, DC</th>\n",
       "      <td>6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>richmond, VA</th>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                price\n",
       "area                 \n",
       "fredericksburg   3000\n",
       "norfolk          4500\n",
       "syracuse         5000\n",
       "washington, DC   6400\n",
       "richmond, VA     6500"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_county_rv_sales=most_county_rv_sales.sort([\"price\"], ascending=True).head()\n",
    "least_county_rv_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_median_price_rv_sales = pd.DataFrame(rv_market.groupby(['area'])['price'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NVR/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>provo</th>\n",
       "      <td>69497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twin falls</th>\n",
       "      <td>58495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ogden</th>\n",
       "      <td>57997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elko</th>\n",
       "      <td>41997.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harrisburg</th>\n",
       "      <td>41875.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price\n",
       "area               \n",
       "provo       69497.0\n",
       "twin falls  58495.0\n",
       "ogden       57997.0\n",
       "elko        41997.5\n",
       "harrisburg  41875.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_median_price_rv_sales.sort([\"price\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NVR/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>salt lake</th>\n",
       "      <td>2650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fredericksburg</th>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washington, DC</th>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plattsburgh</th>\n",
       "      <td>3253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baltimore</th>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 price\n",
       "area                  \n",
       "salt lake       2650.0\n",
       "fredericksburg  3000.0\n",
       "washington, DC  3200.0\n",
       "plattsburgh     3253.0\n",
       "baltimore       4000.0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_median_price_rv_sales =high_median_price_rv_sales.sort([\"price\"], ascending=True).head()\n",
    "low_median_price_rv_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NVR/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>twin falls</th>\n",
       "      <td>93622.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provo</th>\n",
       "      <td>91211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elko</th>\n",
       "      <td>76973.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frederick</th>\n",
       "      <td>65833.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klamath falls</th>\n",
       "      <td>63248.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      price\n",
       "area                       \n",
       "twin falls     93622.500000\n",
       "provo          91211.000000\n",
       "elko           76973.750000\n",
       "frederick      65833.333333\n",
       "klamath falls  63248.666667"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_avg_rv_price=pd.DataFrame(rv_market.groupby(['area'])['price'].mean()) \n",
    "high_avg_rv_price.sort([\"price\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NVR/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fredericksburg</th>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washington, DC</th>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baltimore</th>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outer banks</th>\n",
       "      <td>4325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norfolk</th>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 price\n",
       "area                  \n",
       "fredericksburg  3000.0\n",
       "washington, DC  3200.0\n",
       "baltimore       3600.0\n",
       "outer banks     4325.0\n",
       "norfolk         4500.0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_avg_rv_price = high_avg_rv_price=pd.DataFrame(rv_market.groupby(['area'])['price'].mean()) \n",
    "low_avg_rv_price.sort([\"price\"], ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#According to the data, it seems that Kileen-Temple, in Texas is the healthiest used RV market in the United States.\n",
    "#The RV sales in Kileen-Temple are almost double that of the following market, Skagit Washington. \n",
    "#The poorest RV markets in the US include Fredricksburg, Miss, and Syracuse NY. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### 5.1 Does it makes sense to buy RVs in one region and sell them in another?\n",
    "\n",
    "Assuming the cost of shipping or driving from one regional market to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#After observing the median prices of all of the highest and lowest performing markets, it is clear \n",
    "#that arbitrage can be effectivley executed by transporting RVS. However, we do not have information \n",
    "#about the overall quality of the RV, which could be a factor to why some markets have lower priced RV. I.e.,\n",
    "#the RVs for sale are either very old are very crappy.\n",
    "#\n",
    "#One particularly interesting case of arbitrage to be executed is buying in Salt Lake City, Utah, and Driving to\n",
    "#Ogden, Utah. These two cities are located only about 45 minutes apart, but the spread in the prices of RVs is Drastic.\n",
    "#In Ogden, the median price of RVs are 57997.0, whereas in Salt Lake City the price just caps 3000. \n",
    "#Unless Salt lake is not Salt lake city\n",
    "#\n",
    "#It is also interesting to Note that the further west you go, the more prices rise of RVs. This may have to do with the \n",
    "# fact that much of the RV tourinsm occurs on the West Coast, and particularly in Texas up through the great plains.\n",
    "#The east coast however is in many ways undesirable for most RV travelers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Can you pull out the \"make\" from the markup and include that in your analyis?\n",
    "How reliable is this data and does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sales = pd.DataFrame(rv_market.groupby(['detail'])['price'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detail</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>! gorgeous arctic fox fifth wheel!!</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!! BEST DEAL ON 2016 AIRSTREAM FLYING CLOUD 23D!!</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!! RARE 2005 CHINOOK GLACIER CLASS C GORGEOUS!!</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!!!!!!!!!!!!!!! ALUMINUM TRAILER POLISHING !!!!!!!!!!!!!!!!!!!!</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!!BEST DEAL ON NEW  CLASS C DIESEL MERCEDES</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    price\n",
       "detail                                                   \n",
       "! gorgeous arctic fox fifth wheel!!                     1\n",
       "!! BEST DEAL ON 2016 AIRSTREAM FLYING CLOUD 23D!!       1\n",
       "!! RARE 2005 CHINOOK GLACIER CLASS C GORGEOUS!!         1\n",
       "!!!!!!!!!!!!!!! ALUMINUM TRAILER POLISHING !!!!...      1\n",
       "!!BEST DEAL ON NEW  CLASS C DIESEL MERCEDES             1"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This data is unreliable becuase the variation in naming styles is to vast to capture the commonly occuring \n",
    "# Types. A solution could be to implement a \"Model Dictionary\" and then to sort by the revelent makes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Are there any other variables you could pull out of the markup to help describe your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similarly to the issue noted above, other useful stats like \"Feet\", \"Year\" and \"Miles\" all are unstandardized in the \n",
    "# descriptions. To extract these we would want to make a dictionary or list including all of hte possible combinations of \n",
    "# the variables listed above, and then search each listings details to see if it includes our keywords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Move your project into scrapy (if you haven't used Scrapy yet)\n",
    "\n",
    ">Start a project by using the command `scrapy startproject [projectname]`\n",
    "> - Update your settings.py (review our past example)\n",
    "> - Update your items.py\n",
    "> - Create a spiders file in your `[project_name]/[project_name]/spiders` directory\n",
    "\n",
    "You can update your spider class with the complete list of craigslist \"start urls\" to effectively scrape all of the regions.  Start with one to test.\n",
    "\n",
    "Updating your parse method with the method you chose should require minimal changes.  It will require you to update your parse method to use the response parameter, and an item model (defined in items.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.  Chose another area of Craigslist to scrape.\n",
    "\n",
    "**Choose an area having more than a single page of results, then scrape multiple regions, multiple pages of search results and or details pages.**\n",
    "\n",
    "This is the true exercise of being able to understand how to succesffuly plan, develop, and employ a broader scraping strategy.  Even though this seems like a challenging task, a few tweeks of your current code can make this very managable if you've pieced together all the touch points.  If you are still confused as to some of the milestones within this process, this is an excellent opportunity to round out your understanding, or help you build a list of questions to fill in your gaps.\n",
    "\n",
    "_Use Scrapy!  Provide your code in this project directory when you submit this project._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
